{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Notes!","text":"<p>Hi, I'm Vyshnavlal, a DevOps Engineer and System Administrator who loves Linux and learning new things. This site is a collection of my notes, guides, and documentation on topics I explore in my work and projects.  </p> <p>Feel free to look around\u2014maybe you\u2019ll find something helpful!  </p>"},{"location":"argocd/","title":"Index","text":""},{"location":"argocd/#what-is-argocd","title":"What is ArgoCD?","text":"<p>ArgoCD is a Kubernetes-native continuous deployment (CD) tool. Unlike external CD tools that only enable push-based deployments, Argo CD can pull updated code from Git repositories and deploy it directly to Kubernetes resources. It enables developers to manage both infrastructure configuration and application updates in one system.</p> <p>There are many features of ArgoCD :</p> <ol> <li>Whole Kubernetes configuration defined as code in GIT Repository.</li> <li>Web user interface and command-line interface (CLI).</li> <li>It has easy rollback mechanism through which we can rollback our changes to any previous state.</li> <li>It has cluster disaster recovery feature which helps to create the exact cluster if cluster dies because all codes are present in GIT.</li> <li>More security: Grant access to ArgoCD only.</li> <li>Keep your cluster in sync with git.</li> </ol>"},{"location":"argocd/#workflow-before-argocd","title":"Workflow Before ArgoCD","text":"<p>Before the existence of ArgoCD, the pipeline seems something like the image above. A common ci-cd tool is used for both the Continuous Integration and Continuous Deployment.</p> <ul> <li>once the code is being pushed to version-control let's say Github, the tool triggers the other jobs</li> <li>it runs the test-cases, build images, push the image to respective container registry</li> <li>then changes the deployment manifests as per the new image build, and then deploy the new manifest using kubectl commands</li> </ul> <p>All these jobs/tasks are being performed by a ci-cd tool. This is how traditional ci-cd workflow works before the existence of ArgoCD. However there are some challenges in this workflow.</p> <ul> <li>We need to install and configure tools like kubectl, helm, etc in the ci-cd tools</li> <li>We will have to provide access to k8s cluster and cloud providers if using managed services like eks to the respective ci-cd tool</li> <li>It may lead to security loopholes as credentials are provided to external tools</li> <li>There's no visibility of deployment metrics, your ci-cd tools doesn't know the status of deployed application once they apply the manifests</li> </ul>"},{"location":"argocd/#workflow-after-argocd","title":"Workflow After ArgoCD","text":"<p>After the ArgoCD was launched in the market, the complete workflow has been changed. As you can see in the image, the Continuous Integration and Continuous Deployment has been separated now. ArgoCD uses declarative GitOps based deployment which states that the best practices is to have two different git repositories, one for application source code and another for deployment manifest files.  - once the code has been pushed to version, the CI pipeline trigger the job - starts to run test cases, build images, push images to respective container registry and then update the manifest according and push manifests to its respective git repository - once the updated manifests is pushed to its respective repository be it Github, Gitlab, etc as shown in the image, ArgoCD installed within the cluster automatically checks for the updates and apply within the cluster.</p> <p>This is how, the respective ci-cd tool is limited to CI and ArgoCD handles the CD part and thus separating both the operations.</p>"},{"location":"argocd/#core-concepts","title":"Core concepts","text":"<ul> <li>Application: A group of Kubernetes resources as defined by a manifest.</li> <li>Application source - tools: ArgoCD supports the below tools as source<ul> <li>Helm charts</li> <li>Kustomize application</li> <li>Directory of Yaml files</li> <li>Jsonnet</li> </ul> </li> <li>Project: Projects provide a logical grouping of applications. Useful when ArgoCD is used by multiple teams.<ul> <li>Allow only specific sources \"trusted git repos\"</li> <li>Allow apps to be deployed into specific clusters and namespaces</li> <li>Allow specific resources to be deployed \"deployments, statefulset...etc\"</li> <li>Specifying project is mandatory, Argo CD creates a default project that you can use.</li> </ul> </li> <li>Target/Desired state: The desired state of an application, as represented by files in a Git repository.</li> <li>Live/Actual state: The live state of that application. What pods etc are deployed.</li> <li>Sync: The process of making desired state = actual state</li> <li>Refresh (Compare): Compares the latest code in Git with the live state. Figure out what is different. ArgoCD automatically refreshes every 3 minutes.</li> </ul>"},{"location":"argocd/applications/","title":"Applications","text":""},{"location":"argocd/applications/#argocd-application","title":"ArgoCD Application","text":"<ul> <li>Application is Kubernetes resource object representing a deployed application instance in an environment.</li> <li>It is defined by two key pieces of information:</li> <li>Source: reference to the desired state in Git (repository, revision, path)</li> <li>Destination: reference to the target cluster and namespace</li> <li>Applications can be created using below options:</li> <li>Declarative \"Yaml\" (recommended)</li> <li>Web UI</li> <li>CLI</li> </ul>"},{"location":"argocd/applications/#how-argocd-detects-tools","title":"How ArgoCD detects tools","text":"<ul> <li>If you don't specify a tool explicitly then it's detected as follows:</li> <li>Helm charts: if there is a file as Chart.yaml</li> <li>Kustomize: if there is a kustomization.yaml, kustomization.yml, or Kustomization</li> <li> <p>Otherwise it is assumed to be a plain Yaml directory application.</p> </li> <li> <p>Note: by default in Argo CD, helm release name is equal to app name unless we specify it explicitly. </p> </li> </ul>"},{"location":"argocd/applications/#multiple-source-for-an-application","title":"Multiple source for an Application","text":"<ul> <li>Before ArgoCD 2.6, we can only specify a single source per application. It can either git repo or helm repo.</li> <li>ArgoCD implemented the feature, multiple sources per application in version 2.6</li> <li>ArgoCD application kind has a \"sources\" field where we can specify multiple sources.</li> <li>When you use \"sources\" field, ArgoCD will ignore \"source\" field.</li> <li>If multiple sources produce the same resource (same group, kind, name, and namespace), the last source to produce the resource will take precedence.</li> </ul>"},{"location":"argocd/applications/#use-cases","title":"Use cases","text":"<ol> <li>Combine related resources that exist in different repos into one application    eg: We can install redis chart as source1 and redis prometheus exporter chart from source2. deploy these into one application rather than two.</li> <li>Remote helm chart with git-hosted values    eg: Suppose we want to deploy nginx ingress controller using helm chart and take some values from a git repo.</li> </ol>"},{"location":"argocd/architecture/","title":"Architecture","text":""},{"location":"argocd/architecture/#argocd-architecture","title":"ArgoCD Architecture","text":"<p>ArgoCD consists of 3 main components and they are running as pods in K8s cluster.</p> <ul> <li>ArgoCD server (API + Web server)</li> <li>ArgoCD Repo server</li> <li>ArgoCD Application controller</li> </ul>"},{"location":"argocd/architecture/#api-web-server","title":"API - Web server","text":"<p>It's a gRPC/REST server which exposes the API consumed by the Web UI, CLI and manages the following. The Web UI, CLI, and other gRPC/REST APIs are communicating with this server.</p> <ul> <li>Application management (Create, Update, Delete)</li> <li>Application operations (Sync, Rollback)</li> <li>Repos and clusters management</li> <li>Authentication</li> </ul>"},{"location":"argocd/architecture/#repo-server","title":"Repo server","text":"<p>Its an internal service responsible for cloning the remote git repos and generate the needed k8s manifests.</p>"},{"location":"argocd/architecture/#application-controller","title":"Application controller","text":"<p>Its a Kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state. - Communicates with Repo server to get the generated manifests - Communicate with K8s API to get the actual cluster state - Deploy apps manifests to destination clusters - Detects OutofSync Apps and take corrective actions if needed - Invoking user-defined hooks for lifecycle events (PreSync, Sync, PostSync)</p>"},{"location":"argocd/architecture/#additional-components","title":"Additional components","text":"<ul> <li>Redis: used for caching</li> <li>Dex: identity service to integrate with external identity providers, eg: GitHub</li> <li>ApplicationSet Controller: It automates the generation of Argo CD applications</li> </ul>"},{"location":"argocd/installation/","title":"Installation","text":""},{"location":"argocd/installation/#prerequisite","title":"Prerequisite","text":"<p>A running k8s cluster - Minikube - Docker desktop - Kind - Rancher desktop - Full cluster</p>"},{"location":"argocd/installation/#installation-options","title":"Installation options","text":"<ul> <li>Non High availablity setup</li> <li>Suitable for evaluation or dev/test environments</li> <li>High availablity setup</li> <li>Recommended for production</li> <li>You need atleast 3 worker nodes</li> <li>Light installation \"Core\"</li> <li>Suitable if ArgoCD is used by administrators only. UI and API server is not installed for end users.</li> <li>By default its installed as Non-HA</li> </ul>"},{"location":"argocd/installation/#privilege-options","title":"Privilege options","text":"<p>ArgoCD provides two options for in-cluster privileges: - Cluster-admin privileges: Use this if you want to deploy into the same cluster where ArgoCD runs in. - Namespace level privileges: Use this if you are planning to use ArgoCD without deploying in the same cluster where ArgoCD runs in.</p> <p>You need to create a namespace for argocd <code>kubectl create ns argocd</code> and then choose one of the below options :</p> <ol> <li>Non-HA:</li> </ol> <p>a. cluster-admin privileges: https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</p> <p>b. namespace level privileges: https://github.com/argoproj/argo-cd/raw/stable/manifests/namespace-install.yaml 2. HA:</p> <p>a. cluster-admin privileges: https://github.com/argoproj/argo-cd/raw/stable/manifests/ha/install.yaml</p> <p>b. namespace level privileges: https://github.com/argoproj/argo-cd/raw/stable/manifests/ha/namespace-install.yaml 3. Light installation \"Core\"</p> <p>https://github.com/argoproj/argo-cd/raw/stable/manifests/core-install.yaml</p> <ol> <li>Helm chart:</li> </ol> <p>https://github.com/argoproj/argo-helm/tree/main/charts/argo-cd</p> <p>Initial admin password is stored as a secret in argocd namespace:</p> <p><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo</code></p>"},{"location":"argocd/installation/#accessing-argocd-server-api-ui","title":"Accessing ArgoCD server (API + UI)","text":"<p>By default ArgoCD server is not exposed with external endpoint. But we can expose it by using:</p> <ul> <li>Service: LoadBalancer - Change the ArgoCD service type to LoadBalancer and this will create LB if we're using a managed service like EKS, AKS etc</li> <li>Ingress: Use your preferred ingress controller - Create an Ingress resource that point to argocd-server service.</li> <li>Port-forward: Use this to access locally on your machine - <code>kubectl port-forward svc/argocd-server -n argocd 8080:443</code></li> </ul>"},{"location":"argocd/installation/#installing-argocd-cli","title":"Installing ArgoCD CLI","text":"<ul> <li>CLI is useful when you need to interact with ArgoCD in CI pipelines</li> <li>You manage everything using CLI</li> <li>Manage applications</li> <li>Manage repos</li> <li>Manage clusters</li> <li>Admin tasks</li> <li>Manage projects</li> <li>And more</li> <li>You need to log into ArgoCD server before using any commands</li> </ul> <p><code>argocd login &lt;ARGOCD_SERVER&gt;</code> - Follow the guide in this link https://argo-cd.readthedocs.io/en/stable/cli_installation/</p>"},{"location":"argocd/projects/","title":"Projects","text":""},{"location":"argocd/projects/#why-projects","title":"Why Projects?","text":"<ul> <li>Application grouping</li> <li>Provides logical grouping of applications</li> <li>Access restrictions</li> <li>Useful when ArgoCD is used by multiple teams<ul> <li>Allow only specific sources \"trusted git repos\"</li> <li>Allow apps to be deployed into specific clusters and namespaces</li> <li>Allow specific resources to be deployed, \"deployments, statefulsets...etc\"</li> </ul> </li> <li>Project Roles feature</li> <li>Enables you to set a role with set of policies \"permissions\" to grant access to a project's applications</li> <li>You can use it to grant CI system a specific access to project applications. It must be associated with JWT</li> <li>You can use it to grant oidc groups a specific access to project applications</li> <li>Default project</li> <li> <p>ArgoCD creates a default project once you install it</p> </li> <li> <p>You can create project in three ways; Using declarative method, web ui, and cli.</p> </li> </ul>"},{"location":"argocd/projects/#project-roles","title":"Project roles","text":"<ul> <li>Project roles is not useful without generating a JWT</li> <li>Generated tokens are not stored in ArgoCD</li> <li>To create a token using CLI   <code>argocd proj role create-token PROJECT ROLE-NAME</code></li> <li>A user can leverage tokens in the cli by either passing them in using the --auth-token flag or setting the ARGOCD_AUTH_TOKEN env variable   <code>argocd cluster list --auth-token token-value</code></li> </ul>"},{"location":"argocd/repositories/","title":"Repositories","text":""},{"location":"argocd/repositories/#private-git-repos","title":"Private Git Repos","text":"<ul> <li>Public repos can be used directly in application.</li> <li>Private repos needs to be registered in ArgoCD with proper authentication before using it in applications.</li> <li>ArgoCD support connecting to private repos using below ways:</li> <li>HTTPs: using username and password or access token.</li> <li>SSH: using ssh private key.</li> <li>GitHub / GitHub Enterprise : GitHub App credentials.</li> <li>Private repos credentials are stored in normal k8s secrets.</li> <li>You can register repos using declarative approach, cli and web UI.</li> </ul>"},{"location":"argocd/repositories/#git-repo-using-https","title":"Git Repo using HTTPs","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo\n  password: my-password\n  username: my-username\n</code></pre>"},{"location":"argocd/repositories/#git-repo-using-https-insecure","title":"Git Repo using HTTPs - insecure","text":"<ul> <li>To ignore the SSL/TLS certificate <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo\n  password: my-password\n  username: my-username\n  insecure: true\n</code></pre></li> </ul>"},{"location":"argocd/repositories/#git-repo-using-https-and-tls","title":"Git Repo using HTTPs and Tls","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo\n  password: my-password\n  username: my-username\n  tlsClientCertData: \u2026.\n  tlsClientCertKey: \u2026.\n</code></pre>"},{"location":"argocd/repositories/#git-repo-using-ssh","title":"Git Repo using SSH","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo\n  sshPrivateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n</code></pre>"},{"location":"argocd/repositories/#git-repo-using-github-app","title":"Git Repo using GitHub App","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo\n githubAppID: 1\n githubAppInstallationID: 2\n githubAppPrivateKey: |\n   -----BEGIN OPENSSH PRIVATE KEY-----\n   ...\n   -----END OPENSSH PRIVATE KEY-----\n</code></pre>"},{"location":"argocd/repositories/#private-helm-repo","title":"Private Helm repo","text":"<ul> <li>Public standard Helm repos can be used directly in application.</li> <li>Non standard Helm repositories have to be registered explicitly.</li> <li>Private Helm repos needs to be registered in ArgoCD with proper authentication before using it in applications.</li> <li>ArgoCD support connecting to private Helm repos using username/password and tls cert/key.</li> <li>Registering Helm repos in ArgoCD can be done declaratively , CLI and Web UI.</li> </ul>"},{"location":"argocd/repositories/#private-helm-repo-declaratively","title":"Private Helm Repo - declaratively","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: helm\n  url: https://argoproj.github.io/argo-helm\n  password: my-password\n  username: my-username\n  tlsClientCertData: \u2026.\n  tlsClientCertKey: \u2026.\n</code></pre>"},{"location":"argocd/repositories/#private-helm-repo-cli-add-a-private-helm-repository-named-stable-via-https","title":"Private Helm Repo - CLI (Add a private Helm repository named 'stable' via HTTPS)","text":"<pre><code>argocd repo add https://charts.helm.sh/stable --type helm --name stable --username test --password test\n</code></pre>"},{"location":"argocd/repositories/#credential-templates","title":"Credential Templates","text":"<ul> <li>Used If you want to use the same credentials for multiple repositories in your organization without having to repeat credential configuration.</li> <li>Defined as same as repositories credentials information, with different label value \u201cargocd.argoproj.io/secret-type: repo-creds\u201d.</li> <li>In order for ArgoCD to use a credential template for any given repository, the following conditions must be met:</li> <li>The URL configured for a credential template (e.g. https://github.com/vyshnavlal ) must match as prefix for the repository URL (e.g. https://github.com/vyshnavlal/argocd-example-apps).</li> <li>The repository must either not be configured at all, or if configured, must not contain any credential information.</li> <li>Registering credentials in ArgoCD can be done declaratively , CLI and Web UI.</li> </ul>"},{"location":"argocd/repositories/#credential-templates-declaratively","title":"Credential Templates - declaratively","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-creds\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\nstringData:\n  type: git\n  url: https://github.com/vyshnavlal\n  password: my-password\n  username: my-username\n</code></pre>"},{"location":"argocd/repositories/#credential-templates-cli","title":"Credential Templates - CLI","text":"<ul> <li>Add credentials with user/pass authentication to use for all repositories under https://git.example.com/repos   <pre><code>argocd repocreds add https://git.example.com/repos/ --username git - -password secret\n</code></pre></li> </ul>"},{"location":"argocd/sync-policies/","title":"Sync policies","text":""},{"location":"argocd/sync-policies/#automated-sync","title":"Automated Sync","text":"<ul> <li>By default, ArgoCD polls Git repositories every 3 minutes to detect changes to the manifests.</li> <li>Argo CD can automatically sync apps when it detects differences between the desired manifests in Git, and the live state in the cluster.</li> <li>No need to do manual sync anymore.</li> <li>CI/CD pipelines no longer need direct access.</li> <li>An automated sync will only be performed if the application is OutOfSync.</li> <li>Automatic sync will not reattempt a sync if the previous sync attempt against the same commit-SHA and parameters had failed.</li> <li>Rollback cannot be performed against an application with automated sync enabled.</li> </ul>"},{"location":"argocd/sync-policies/#auto-sync-declaratively","title":"Auto Sync \u2013 Declaratively","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n name: kustomize-guestbook\n namespace: argocd\nspec:\n destination:\n  namespace: guestbook\n  server: \"https://kubernetes.default.svc\"\n project: default\n source:\n  path: kustomize-guestbook\n  repoURL: \"https://github.com/argoproj/argocd-example-apps.git\"\n  targetRevision: HEAD\nsyncPolicy:\n automated: {}\n</code></pre>"},{"location":"argocd/sync-policies/#auto-sync-cli","title":"Auto Sync \u2013 CLI","text":"<pre><code>argocd app create nginx-ingress --repo https://charts.helm.sh/stable --helm-chart nginx-ingress --revision 1.24.3 --dest-namespace default --dest-server https://kubernetes.default.svc --sync-policy automated\n</code></pre>"},{"location":"argocd/sync-policies/#automated-pruning","title":"Automated Pruning","text":"<ul> <li>Default \u2013 no prune: when automated sync is enabled, by default for safety automated sync will not delete resources when Argo CD detects the resource is no longer defined in Git.</li> <li>Pruning can be enabled to delete resources automatically as part of the automated sync.</li> </ul>"},{"location":"argocd/sync-policies/#auto-prune-declaratively","title":"Auto Prune \u2013 Declaratively","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n name: kustomize-guestbook\n namespace: argocd\nspec:\n destination:\n  namespace: guestbook\n  server: \"https://kubernetes.default.svc\"\n project: default\n source:\n  path: kustomize-guestbook\n  repoURL: \"https://github.com/argoproj/argocd-example-apps.git\"\n  targetRevision: HEAD\nsyncPolicy:\n automated:\n  prune: true\n</code></pre>"},{"location":"argocd/sync-policies/#auto-prune-cli","title":"Auto Prune \u2013 CLI","text":"<pre><code>argocd app create nginx-ingress --repo https://charts.helm.sh/stable --helm-chart nginx-ingress --revision 1.24.3 --dest-namespace default --dest-server https://kubernetes.default.svc --auto-prune\n</code></pre>"},{"location":"argocd/sync-policies/#automated-self-healing","title":"Automated Self Healing","text":"<ul> <li>By default, changes that are made to the live cluster will not trigger automated sync.</li> <li>ArgoCD has a feature to enable self healing when the live cluster state deviates from Git state.</li> </ul>"},{"location":"argocd/sync-policies/#auto-self-heal-declaratively","title":"Auto Self Heal \u2013 Declaratively","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n name: kustomize-guestbook\n namespace: argocd\nspec:\n destination:\n  namespace: guestbook\n  server: \"https://kubernetes.default.svc\"\n project: default\n source:\n  path: kustomize-guestbook\n  repoURL: \"https://github.com/argoproj/argocd-example-apps.git\"\n  targetRevision: HEAD\nsyncPolicy:\n automated:\n  selfHeal: true\n</code></pre>"},{"location":"argocd/sync-policies/#auto-self-heal-cli","title":"Auto Self Heal \u2013 CLI","text":"<pre><code>argocd app create nginx-ingress --repo https://charts.helm.sh/stable --helm-chart nginx-ingress --revision 1.24.3 --dest-namespace default --dest-server https://kubernetes.default.svc --self-heal\n</code></pre>"},{"location":"argocd/sync-policies/#sync-options","title":"Sync Options","text":"<ul> <li>Users can customize how resources are synced between target cluster and desired state.</li> <li>Most of the options available at application level.</li> </ul> <p><pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: example\nspec:\n  ....\n    syncPolicy:\n    syncOptions:\n</code></pre> - Some of the options available using resources annotations. <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io\u2026\u2026\u2026\u2026\n</code></pre></p>"},{"location":"argocd/sync-policies/#no-prune","title":"No Prune","text":"<ul> <li>ArgoCD can prevent an object from being pruned.</li> <li>The app will be in out of sync but still does not prune the resource.</li> <li>In the resource itself, can be used as annotation as below: <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/sync-options: Prune=false\n</code></pre></li> </ul>"},{"location":"argocd/sync-policies/#disable-kubectl-validation","title":"Disable Kubectl Validation","text":"<ul> <li>Some resources need to be applied without validating the resources \u201ckubectl apply --validate=false\u201d.</li> <li>You can achieve this in ArgoCD by at application level or resource level.</li> <li>Application level:     <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: example\nspec:\n  ....\n    syncPolicy:\n    syncOptions:\n      - Validate=false\n</code></pre></li> <li>Resource level using annotation :     <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/sync-options: Validate=false\n</code></pre></li> </ul>"},{"location":"argocd/sync-policies/#selective-sync","title":"Selective Sync","text":"<ul> <li>When syncing using auto sync ArgoCD applies every object in the application.</li> <li>Selective sync option will sync only out-of-sync resources. You need when you have thousands of resources in which sync take a long time and puts pressure on Api server.</li> <li>Can be applied at application level only:   <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: example\nspec:\n  ....\n    syncPolicy:\n    syncOptions:\n      - ApplyOutOfSyncOnly=true\n</code></pre></li> </ul>"},{"location":"argocd/sync-policies/#prune-last","title":"Prune Last","text":"<ul> <li>ArgoCD can control the sequence of creation/pruning resources, aka waves.</li> <li>You can prune some resources to happen as final using \u201cPrune Last\u201d.</li> <li>You can achieve this in ArgoCD by at application level or resource level.</li> <li>Application level:   <code>yaml     apiVersion: argoproj.io/v1alpha1     kind: Application     metadata:       name: example     spec:       ....         syncPolicy:         syncOptions:           - PruneLast=true</code></li> <li>Resource level using annotation :     <code>yaml     metadata:       annotations:         argocd.argoproj.io/sync-options: PruneLast=true</code>  ### Replace Resources</li> <li>By default ArgoCD use \u201ckubectl apply\u201d to deploy the resources changes.</li> <li>In some cases you need to \u201cReplace/Recreate\u201d the resources, ArgoCD can do this by using replace=true.</li> <li>You can achieve this in ArgoCD by at application level or resource level.</li> <li> <p>Application level:   <code>yaml     apiVersion: argoproj.io/v1alpha1     kind: Application     metadata:       name: example     spec:       ....         syncPolicy:         syncOptions:           - Replace=true</code></p> </li> <li> <p>Resource level using annotation :     <code>yaml     metadata:       annotations:         argocd.argoproj.io/sync-options: Replace=true</code></p> </li> </ul>"},{"location":"argocd/sync-policies/#fail-on-shared-resource","title":"Fail on Shared Resource","text":"<ul> <li>By default ArgoCD will apply the resources even if it was available in multiple applications.</li> <li>You can configure the sync to fail if any resource is found in other applications by using FailOnSharedResource=true.</li> <li>Can be applied at application level only:</li> <li>Application level:   <code>yaml     apiVersion: argoproj.io/v1alpha1     kind: Application     metadata:       name: example     spec:       ....         syncPolicy:         syncOptions:           - FailOnSharedResource=true</code></li> </ul>"},{"location":"associate-cloud-engineer/","title":"Index","text":"<p>Study materials for GCP Associate Cloud Engineer certification</p>"},{"location":"associate-cloud-engineer/compute-engine/","title":"Compute engine","text":""},{"location":"associate-cloud-engineer/compute-engine/#google-cloud-compute-engine-gce","title":"Google Cloud Compute Engine (GCE)","text":"<ul> <li>In data centers, applications are deployed to physical servers</li> <li>In Cloud, the applications are deploying to virtual machines</li> <li>GCP provides such virtual machines, called Compute Engine</li> </ul>"},{"location":"associate-cloud-engineer/compute-engine/#compute-engine-features","title":"Compute Engine - Features","text":"<ul> <li>Create and manage lifecycle of VM instances (Start, Stop, Suspend VM instances etc)</li> <li>Load balancing and Auto scaling multiple VM instances</li> <li>Attach storage (&amp;network storage) to VM instances</li> <li>Manage network connectivity (IP address allocation etc) and configuration for VM instances</li> </ul>"},{"location":"associate-cloud-engineer/compute-engine/#compute-engine-machine-family","title":"Compute Engine - Machine Family","text":"<ul> <li>General purpose (E2, N2, N2D, N1): Best price-performance ratio</li> <li>Web and application servers, Small-medium databases, Dev environments</li> <li>Memory optimized (M2, M1): Ultra high memory workloads</li> <li>Large in-memory databases and In-memory analytics</li> <li>Compute optimized (C2): Compute intensive workloads</li> <li>Gaming applications</li> </ul>"},{"location":"associate-cloud-engineer/compute-engine/#compute-engine-machine-types","title":"Compute Engine - Machine types","text":"<ul> <li>Different machine types are available for each machine family</li> <li>For example: e2-standard-2</li> <li>e2: Machine family</li> <li>standard: Type of workload</li> <li>2: Number of CPUs</li> <li>Memory, disk and networking capabilities increase along with vCPUs</li> </ul>"},{"location":"associate-cloud-engineer/compute-engine/#compute-engine-image","title":"Compute Engine - Image","text":"<ul> <li>The OS that you want to use in your VM</li> <li>Types of Images:</li> <li>Public image: Provided and maintained by Google or Open source communities or third party vendors</li> <li>Custom image: The image created by you for your projects</li> </ul>"},{"location":"associate-cloud-engineer/compute-engine/#internal-and-external-ip-addresses","title":"Internal and External IP addresses","text":"<ul> <li>External (Public) IP addresses can be accessed within Internet</li> <li>Internal (Private) IP addresses are internal to a network</li> <li>External IP address is unique publically however two different networks can have resources with same internal IP address</li> <li>All VM instances are assigned at least one internal IP address</li> <li>Creation of external IP address can be enabled for VM instances</li> <li>When you stop a VM instance, external IP address is lost</li> <li>Sometimes, GCP reuses the same external IP on restart</li> </ul>"},{"location":"associate-cloud-engineer/compute-engine/#static-ip-address","title":"Static IP address","text":"<ul> <li>IP addresses that are constant (Will not change during stop/start)</li> <li>The static IP address should be created in the same region as of VM instance</li> <li>Static IP can be switched to another VM instance in the same project</li> <li>Static IP remains attahced even if you stop the instance. You have to manually detach it.</li> <li>You're billed for an static IP when you're not using it</li> <li>Static IP addresses not attached to an instance or load balancer are billed at a higher hourly rate</li> </ul>"},{"location":"associate-cloud-engineer/introduction/","title":"Introduction","text":""},{"location":"associate-cloud-engineer/introduction/#before-the-cloud-scenario","title":"Before the cloud - Scenario","text":"<p>Consider an online shopping site hosted on an on-premises server. The site functions normally, but when Christmas arrives, the site's traffic increases rapidly! What will be done to satisfy the sudden increase in traffic?</p> <p>We need to buy infrastrcuture for peak load, also called PEAK LOAD provisioning</p> <p>So, once the traffic is reduced, what will the infrastructure do?</p>"},{"location":"associate-cloud-engineer/introduction/#before-the-cloud-challenges","title":"Before the cloud - Challenges","text":"<ul> <li>High cost for buying infrastructure</li> <li>Needs ahead of time planning (Can you guess the future?)</li> <li>Low infrastructure utilization</li> <li>Dedicated infrastructure maintainence team</li> </ul>"},{"location":"associate-cloud-engineer/introduction/#cloud-intro","title":"Cloud intro","text":"<p>How about provisioning (renting) resources when you want them and releasing them back when you don't need them?    - On-demand resource provisioning   - Elasticity</p>"},{"location":"associate-cloud-engineer/introduction/#cloud-advantages","title":"Cloud - Advantages","text":"<ul> <li>Pay only for what you use</li> <li>Auto-scaling</li> <li>No need to spend money for data center maintenance</li> <li>Can create resources globally in minutes</li> </ul>"},{"location":"associate-cloud-engineer/introduction/#gcp-google-cloud-platform","title":"GCP (Google Cloud Platform)","text":"<ul> <li>One of the top 3 cloud service providers</li> <li>Infrastructure that powers 8 services with over 1 billion users: Gmail, Google search, Youtube etc</li> <li>Net carbon-neutral cloud</li> </ul>"},{"location":"associate-cloud-engineer/regions-zones/","title":"Regions zones","text":"<p>Before cloud, if we want high availability and low latency, we need to provision two or more data centres in different regions, and if we want low latency, we should provision one data centre close to us. These procedures will be time-consuming and expensive.</p>"},{"location":"associate-cloud-engineer/regions-zones/#region","title":"Region","text":"<ul> <li>Specific geographical location to host your resources</li> <li>Google provides 20+ regions around the world and it's expanding every year</li> <li>For example, the us-central1 region denotes a region in the Central United States</li> <li>Advantages:</li> <li>High availability</li> <li>Low latency</li> <li>Global footprint (A company can provision resources gloabally around the world in minutes)</li> <li>Adhere to goverment regulations</li> </ul> <p>## Zones</p> <ul> <li>By using zones, we can achieve high-availability in the same region.</li> <li>Each region has three or more zones</li> <li>Increased availability and fault-tolerance in the same region</li> <li>Each zone has one or more discrete data centers</li> <li>Zones in a region are connected through low-latency links</li> <li>For example, \u00a0us-central1-a, us-central1-b, and us-central1-f.</li> </ul>"},{"location":"gitops/","title":"Index","text":""},{"location":"gitops/#why-gitops","title":"Why GitOps?","text":"<p>Previously, managing IT infrastructure was a complex task. Most infrastructure configurations were manually managed, configured and distributed by system administrators(IAC). And, because different team members simultaneously collaborated on these infrastructures, this often resulted in multiple errors.</p> <p>Since it allowed multiple teams to collaborate in the infrastructure configuration simultaneously, it required a review and approval process. It lacked tests to confirm changes from individual team members, and infrastructure updates were done manually. DevOps teams needed an automated, scalable way to document and provision application infrastructure; GitOps.</p>"},{"location":"gitops/#what-is-gitops","title":"What is GitOps?","text":"<p>In easy words, GitOps is a smart and intelligent way for Continuous Deployment of Cloudnative application. It is a way to do Kubernetes cluster management and application delivery.</p> <p>It works by using Git as a single source of truth for declarative infrastructure and applications, together with tools ensuring the actual state of infrastructure and applications converges towards the desired state declared in Git. With Git at the center of your delivery pipelines, developers can make pull requests to accelerate and simplify application deployments and operations tasks to your infrastructure or container-orchestration system (e.g. Kubernetes).</p> <p>The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment match the described state in the repository. If you want to deploy a new application or update an existing one, you only need to update the repository \u2014 the automated process handles everything else. It\u2019s like having cruise control for managing your applications in production.</p>"},{"location":"gitops/#three-components-of-gitops","title":"Three components of GitOps","text":"<p>GitOps is no single tool or product; Instead, it combines IAC and complete DevOps pipeline practices. To get started with GitOps, you should consider these components.</p> <ul> <li>Infrastrure as Code (IaC)</li> </ul> <p>IAC defines infrastructure using configuration files(code) rather than manually using a graphical user interface. - Merge Requests (MRs)</p> <p>Merge requests (MRs) are the modification method used by GitOps for any updates to the infrastructure. Teams can collaborate on the infrastructure through reviews and comments before changes are officially approved in the main branch. A merge request acts as an audit log and helps keep track of the changes implemented by each team member. With an inbuilt roll-back feature, revert changes that are to the desired version. - Continuous integration and continuous deployment (CI/CD)</p> <p>In a GitOps environment, infrastructure management is automated using a CI/CD(Continuous integration and Continuous deployment) pipeline. So, whenever a code is merged to the main branch, CI/CD acts as a loop. It initiates the change from the git repository to the environment. GitOps automation and continuous deployment eliminate human errors that arise when infrastructure configuration is done manually.</p>"},{"location":"gitops/#how-does-gitops-work","title":"How does GitOps work?","text":"<p>GitOps guarantees that a system\u2019s cloud infrastructure is reconfigured and syncs with the state of a Git repository as soon as the merged request is approved. </p> <p>Consider it this way, assume you developed a central repository containing all of the configuration files (YAML files), documentation, and code required for Kubernetes. You then automated it such that other system administrators responsible for Kubernetes deployment can simply clone the repository, modify the code, and submit a merge request to the central Git repository.</p> <p></p> <p>The workflow of a typical GitOps environment looks like this:</p> <ul> <li>You make a pull request for a new feature to IAC, configuration, or application code on the Git repository.</li> <li>The code will get reviewed and the changes approved.</li> <li>After that, the code gets merged into the Git repository.</li> <li>Immediately CI build pipelines are established and triggered from pull requests. It runs a few checks, and if all of them pass, it creates a new image and sends it to the image container.</li> <li>The deployment Automator identifies changes to the image repository when the merge request is finished, grabs it from the registry, and updates the YAML configuration file.</li> <li>This declares the changes operational, and the modification is automatically distributed to the cluster.</li> </ul>"},{"location":"gitops/#what-is-declarative-iac-vs-imperative-iac","title":"What is Declarative IaC v/s Imperative IaC","text":"<p>That is using script, Json, Yaml, terraform templates instead of running cli like kubectl, bash which is imperative.</p>"},{"location":"gitops/#benefits-of-gitops","title":"Benefits of GitOps","text":"<ul> <li>Allows team collaboration</li> </ul> <p>Since it uses version control, respective members can simultaneously contribute to the configurations. Team members can create a merge request that suggests changes to the code. There would not be any cases of conflicting changes as every new configuration goes through the review process before it\u2019s approved. - Version control</p> <p>GitOps is a version control system; therefore, it has all of the capabilities of git, such as detailed audit records of all changes made. Details like the committer\u2019s identity, the commit timestamp, and the commit ID are saved when a commit is made. As a result, you have total access to the system and can track all changes made to the infrastructure configuration. - Increases reliability</p> <p>Manually configuring your application infrastructure can be unreliable, but with an automated system, you can achieve fewer errors and faster problem resolution. - Automation</p> <p>Automating the entire deployment process saves time and increases productivity for a DevOps team. You can even increase the number of changes made to the configuration and spend more experimenting with new infrastructure configurations.</p>"},{"location":"gitops/#pull-based-deployments","title":"Pull-based Deployments","text":"<p>The Pull-based deployment strategy uses the same concepts as the push-based variant but differs in how the deployment pipeline works. Traditional CI/CD pipelines are triggered by an external event, for example when new code is pushed to an application repository. With the pull-based deployment approach, the operator is introduced. It takes over the role of the pipeline by continuously comparing the desired state in the environment repository with the actual state in the deployed infrastructure. Whenever differences are noticed, the operator updates the infrastructure to match the environment repository. Additionally the image registry can be monitored to find new versions of images to deploy.</p> <p>Just like the push-based deployment, this variant updates the environment whenever the environment repository changes. However, with the operator, changes can also be noticed in the other direction. Whenever the deployed infrastructure changes in any way not described in the environment repository, these changes are reverted. This ensures that all changes are made traceable in the Git log, by making all direct changes to the cluster impossible.</p> <p></p>"},{"location":"gitops/#gitops-ecosystem","title":"GitOps Ecosystem","text":""},{"location":"solutions-architect-associate/","title":"AWS Solutions Architect Associate Notes","text":"<p>Study materials for AWS Solutions Architect Associate certification</p>"},{"location":"solutions-architect-associate/acm/","title":"AWS Certificate Manager (ACM)","text":"<ul> <li>Provision, manage, and deploy TLS certificates</li> <li>Provide in-flight encryption (HTTPS)</li> <li>Supports both public (free of charge) and private TLS certificates</li> <li>Automatic TLS certificate renewal</li> <li>Load TLS certificates on</li> <li>ELB (CLB, ALB, NLB)</li> <li>CloudFront distributions</li> <li>APIs on API gateway</li> <li>Can't use ACM with EC2</li> </ul>"},{"location":"solutions-architect-associate/acm/#requesting-public-certificates","title":"Requesting Public certificates","text":"<ol> <li>List domain names to be included in certificate<ul> <li>FQDN: test.example.com</li> <li>Wildcard domain: *.example.com</li> </ul> </li> <li>Select validation method: DNS validation or Email validation<ul> <li>DNS validation is preferred for automation purpose</li> <li>Email validation will sent email to contact address in the WHOIS database</li> <li>DNS validation will leverage a CNAME record to DNS config </li> </ul> </li> <li>It will take few hours to get verified</li> <li>The public certificates will be enrolled for automatic renewal<ul> <li>ACM automatically renews ACM generated certificates 60days before expiry</li> </ul> </li> </ol>"},{"location":"solutions-architect-associate/acm/#importing-public-certificates","title":"Importing Public certificates","text":"<ul> <li>Option to generate certificate outside of ACM and then import it</li> <li>No automatic renewal, must import a new certificate before expiry</li> <li>ACM sends daily expiration events starting 45 days prior to expiration</li> <li>The number of days can be configured</li> <li>Events are appearing in EventBridge</li> <li>AWS config has a managed rule (acm-certificate-expiration-check)to check for expiring certificates</li> </ul>"},{"location":"solutions-architect-associate/acm/#integration-with-api-gateway","title":"Integration with API Gateway","text":"<ul> <li>Create a Custom domain name in API Gateway</li> <li>Edge-optimized (default): for global clients</li> <li>The TLS certificate must be in the same region as CloudFront, us-east-1</li> <li>Setup a CNAME or (better) A-Alias record in Route53</li> <li>Regional</li> <li>The TLS certificate must be imported on API Gateway, in the same region as the API stage</li> <li>Setup a CNAME or (better) A-Alias record in Route53</li> </ul>"},{"location":"solutions-architect-associate/amazon-keyspaces/","title":"Amazon Keyspaces (for Apache Cassandra)","text":"<ul> <li>Apache Cassandra is an opensource NoSQL distributed database</li> <li>Managed Apache Cassandra compatible database service</li> <li>Serverless, Scalable, HA</li> <li>Automatically scale tables up/down based on application's traffic</li> <li>Tables are replicated 3 times across multiple AZ</li> <li>Using the Cassandra Query Language (CQL)</li> <li>Single digit milliseconds latency at any scale, 1000s of requests per second</li> <li>Capacity: On-demand mode or Provisioned mode with auto-scaling</li> <li>Encryption, backup, Point-In-Time Recovery (PITR) upto 35 days</li> <li>Use cases:</li> <li>IoT devices info</li> <li>time-series data</li> </ul>"},{"location":"solutions-architect-associate/amazon-mq/","title":"Amazon MQ","text":"<ul> <li>If you have some traditional applications running from on-premise, they may use open protocols such as MQTT, AMQP, STOMP, Openwire, WSS, etc. When migrating to the cloud, instead of re-engineering the application to use SQS and SNS (AWS proprietary), we can use Amazon MQ (managed Apache ActiveMQ) for communication.</li> <li>Doesn\u2019t \u201cscale\u201d as much as SQS or SNS because it is provisioned</li> <li>Runs on a dedicated machine (can run in HA with failover)</li> <li>Has both queue feature (SQS) and topic features (SNS)</li> </ul>"},{"location":"solutions-architect-associate/amazon-mq/#high-availability","title":"High Availability","text":"<ul> <li>High Availability in Amazon MQ works by leveraging MQ broker in multi AZ (active and standby).</li> <li>EFS (NFS that can be mounted to multi AZ) is used to keep the files safe in case the main AZ is down. </li> </ul>"},{"location":"solutions-architect-associate/amazon-msk/","title":"Amazon MSK","text":"<ul> <li>Amazon Managed Streaming for Apache Kafka (Amazon MSK)</li> <li>Fully managed Apache Kafka on AWS</li> <li>Allows to create, update, and delete clusters</li> <li>Creates and manages Kafka broker nodes and Zookeeper nodes for you</li> <li>Deploy the MSK cluster in your VPC, multi-AZ (upto 3 for HA)</li> <li>Automatic recovery from common apache kafka failures</li> <li>Data is stored on EBS volumes for as long as you want</li> <li>MSK Serverless</li> <li>Run Apache Kafka on MSK without managing the capacity</li> <li>MSK automatically provsions resources and scales compute &amp; storage</li> </ul>"},{"location":"solutions-architect-associate/amazon-msk/#kinesis-data-streams-vs-amazon-msk","title":"Kinesis Data Streams vs Amazon MSK","text":"Kinesis Data Streams Amazon MSK 1MB message size limit 1MB default, configure for higher (ex:10MB) Data Streams with Shards Kafka Topics with partitions Shard splitting &amp; Merging Can only add Partitions to a Topic TLS in-flight encryption PLAINTEXT or TLS in-flight encryption KMS at-rest encryption KMS at-rest encryption"},{"location":"solutions-architect-associate/amazon-pinpoint/","title":"Amazon Pinpoint","text":"<ul> <li>Scalable 2-way (inbound/outbound) marketing communications service</li> <li>Support email, sms, push, voice, and in-app messaging</li> <li>Scales to billions of messages per day</li> <li>Use cases: run campaigns by sending marketing, bulk, transactional SMS messages</li> <li>Versus Amazon SNS or Amazon SES</li> <li>In SNS &amp; SES, you managed each message's audience, content, and delivery schedule</li> <li>In Pinpoint, you create message templates, delivery schedules, highly-targeted segments, and full campaigns</li> </ul>"},{"location":"solutions-architect-associate/amazon-qldb/","title":"Amazon QLDB","text":"<ul> <li>QLDB: Quantum Ledger Database</li> <li>Fully managed, serverless, HA, Replication across 3 AZ</li> <li>Used to review history of all the changes made to your application data over time</li> <li>Immutable system: No entry can be removed or modified, cryptographically verifiable</li> </ul> <ul> <li>2-3x performance than common ledger blockchain frameworks, manipulate data using SQL</li> <li>Difference with Amazon Managed Blockchain: no decentralization component, in accordance with financial regulation rules</li> <li>Ledger is a book recording financial transactions</li> </ul>"},{"location":"solutions-architect-associate/amazon-timestream/","title":"Amazon Timestream","text":"<ul> <li>Fully managed, fast, scalable, serverless time series database</li> <li>Automatically scales up/down to adjust capacity</li> <li>Store and analyze trillions of events per day</li> <li>1000s times faster and 1/10th th cost of relational databases</li> <li>Scheduled queries, multi-measure records, SQL compatibility</li> <li>Data storage tiering: recent data kept in memory and historical data kept in a cost-optimized storage</li> <li>Encryption in-trasit and rest</li> <li>Use cases:</li> <li>IoT apps</li> <li>operational applications</li> <li>real-time analytics</li> </ul>"},{"location":"solutions-architect-associate/api-gateway/","title":"API Gateway","text":"<ul> <li>Serverless REST APIs</li> <li>Invoke Lambda functions using REST APIs (API gateway will proxy the request to lambda)</li> <li>Supports WebSocket (stateful)</li> <li>Cache API responses</li> <li>Transform and validate requests and responses</li> <li>Can be integrated with any HTTP endpoint in the backend or any AWS API</li> <li>Support API Versioning</li> <li>Swagger/Open API import to quickly define APIs</li> <li>Generate SDK and API specifications <p>We can use an API Gateway REST API to directly access a DynamoDB table by creating a proxy for the DynamoDB query API.</p> </li> </ul> <p>API cache is not enabled for a method, it is enabled for a stage</p>"},{"location":"solutions-architect-associate/api-gateway/#endpoint-types","title":"Endpoint Types","text":"<ul> <li>Edge-Optimized (default)</li> <li>For global clients</li> <li>Requests are routed through the CloudFront edge locations (improves latency)</li> <li>The API Gateway lives in only one region but it is accessible efficiently through edge locations</li> <li>Regional</li> <li>For clients within the same region</li> <li>Could manually combine with your own CloudFront distribution for global deployment (this way you will have more control over the caching strategies and the distribution)</li> <li>Private</li> <li>Can only be accessed within your VPC using an Interface VPC endpoint (ENI)</li> <li>Use resource policy to define access</li> </ul>"},{"location":"solutions-architect-associate/api-gateway/#security","title":"Security","text":"<ul> <li>User authentication through</li> <li>IAM roles (useful for internal applications)</li> <li>Cognito (identity for external users - mobile users)</li> <li>Custom authorizer (your own logic)</li> <li>Cusom Domain Name HTTPS security through integration with AWS Certificate Manager (ACM)</li> <li>If using Edge optimized endpoint, then the certificate must be in us-east-1</li> <li>If using Regional endpoint, the certificate must be in API gateway region</li> <li>Must setup CNAME or A-alias record in Route53</li> </ul>"},{"location":"solutions-architect-associate/api-gateway/#serverless-crud-application","title":"Serverless CRUD Application","text":""},{"location":"solutions-architect-associate/app-runner/","title":"AWS App Runner","text":"<ul> <li>Fully managed service</li> <li>To deploy web applications and APIs at scale</li> <li>No infrastructure experience required</li> <li>Start with your source code or container image</li> <li>Automatically builds and deploy web app</li> <li>Automatic scaling, HA, Load balancer, encryption, VPC access support</li> <li>Use cases: web apps, APIs, microservices, rapid production deployments</li> </ul> <p>alt text</p>"},{"location":"solutions-architect-associate/appflow/","title":"Amazon AppFlow","text":"<ul> <li>Fully managed</li> <li>Enables you to securely transfer data b/w SaaS application and AWS</li> <li>Sources: Salesforce, SAP, Zendesk, Slack and ServiceNow</li> <li>Destinations: Amazon S3, Redshift, non-aws such as SnowFlake and Salesforce</li> <li>Frequency: on a schedule, in response to events, or on demand</li> <li>Data transformation capabilities like filtering and validation</li> <li>Encrypted over public internet or privately over AWS Privatelink</li> </ul>"},{"location":"solutions-architect-associate/application-migration-service/","title":"Application migration service","text":""},{"location":"solutions-architect-associate/application-migration-service/#aws-application-discovery-service","title":"AWS Application Discovery Service","text":"<ul> <li>Plan migration projects by gathering informations about on-premise data centers</li> </ul> <p>Agentless Discovery (AWS Agentless Discovery Connector) - VM inventory, configuration, and performance history such as CPU, memory and disk usage</p> <p>Agent based Discovery (AWS Application Discovery Agent) - System configuration, system performance, running processes, and details of the network connection between systems</p> <ul> <li>Resulting can be viewed within AWS Migration Hub</li> </ul>"},{"location":"solutions-architect-associate/application-migration-service/#aws-application-migration-service-mgn","title":"AWS Application Migration Service (MGN)","text":"<ul> <li>Lift-and-shift (rehost) solution which simplify migrating applications to aws</li> <li>Converts your physical, virtual, and cloud-based servers to run on aws</li> <li>Supports wide range of platforms, OS, and databases</li> <li>Minimal downtime and reduced costs</li> </ul>"},{"location":"solutions-architect-associate/athena/","title":"Athena","text":"<ul> <li>Serverless query service to perform analytics on data stored in S3 </li> <li>Uses SQL language to query the files (built on Presto engine)</li> <li>Supports CSV, JSON, ORC, Avro and Parquet file formats</li> <li>Runs directly on S3 (no copying needed)</li> <li>Pricing: $5 per TB of scanned</li> <li>Commonly used with Amazon Quicksight for reporting/dashboards</li> <li>Uses cases: analyze and query VPC Flow logs, ELB logs, CloudTrails logs etc</li> </ul>"},{"location":"solutions-architect-associate/athena/#performance-improvement","title":"Performance improvement","text":"<ul> <li>Use columnar data for cost-savings (due to less scan)</li> <li>Apache Parquet or ORC is recommended</li> <li>Use Glue to convert your data to Parquet or ORC</li> <li>Compress data for smaller retrievals (bzip2, gzip, lz4, snappy...)</li> <li>Partition datasets in S3 for easy querying on virtual columns</li> <li>Ex: s3://athena-examples/flight/parquet/year=2009/month=2/day=5/</li> <li>Use larger files (&gt; 128MB) </li> </ul>"},{"location":"solutions-architect-associate/athena/#federated-query","title":"Federated query","text":"<ul> <li>Allows to run SQL queries across data stored in relational, non-relational, object, and custom data sources (AWS or on-premises)</li> <li>Uses Data Source Connector that run on AWS Lambda</li> <li>Store the results back in S3</li> </ul> <p>alt</p> <p>Can be used along with AWS Transcribe (automatic speech recognition service that converts audio to text) to analyze audio for sentiment analysis.</p>"},{"location":"solutions-architect-associate/aurora/","title":"Aurora","text":"<ul> <li>Regional Service (supports global databases)</li> <li>Supports Multi AZ</li> <li>AWS managed Relational DB cluster</li> <li>Preferred over Relational Database Service (RDS)</li> <li>Storage auto-scaling in increments of 10GB (max 128TB)</li> <li>Up to 15 read replicas</li> <li>Asynchronous Replication (milliseconds)</li> <li>Supports only MySQL &amp; PostgreSQL</li> <li>Cloud-optimized (5x performance improvement over MySQL on RDS, over 3x the performance of PostgreSQL on RDS)</li> <li>Costs more than RDS (20% more)</li> <li>Failover is instantaneous (HA by native)</li> <li>Aurora Global Databases allows you to have an Aurora Replica in another AWS Region, with up to 5 secondary regions.</li> </ul>"},{"location":"solutions-architect-associate/aurora/#high-availability-read-scaling","title":"High Availability &amp; Read Scaling","text":"<ul> <li>Self healing (if some data is corrupted, it will be automatically healed)</li> <li>Storage is striped across 100s of volumes (more resilient)</li> <li>One Aurora instance takes writes (master)</li> <li>Automated failover</li> <li>A read replica is promoted as the new master in less than 30 seconds</li> <li>Aurora flips the CNAME record for your DB Instance to point at the healthy replica</li> <li>In case no replica is available, Aurora will attempt to create a new DB Instance in the same AZ as the original instance. This replacement of the original instance is done on a best-effort basis and may not succeed.</li> <li>Support for Cross Region Replication</li> <li>Aurora maintains 6 copies of your data across 3 AZ:</li> <li>4 copies out of 6 needed for writes (can still write if 1 AZ completely fails)</li> <li>3 copies out of 6 need for reads</li> </ul>"},{"location":"solutions-architect-associate/aurora/#endpoints","title":"Endpoints","text":"<ul> <li>Writer Endpoint (Cluster Endpoint)</li> <li>Always points to the master (can be used for read/write)</li> <li>Each Aurora DB cluster has one cluster endpoint</li> <li>Reader Endpoint</li> <li>Provides load-balancing for read replicas only (used to read only)</li> <li>If the cluster has no read replica, it points to master (can be used to read/write)</li> <li>Each Aurora DB cluster has one reader endpoint</li> <li>Custom endpoint</li> <li>Custom endpoint for a DB cluster represents a set of DB instances that you choose.</li> <li>Aurora performs load balancing and chooses one of the instances in the group to handle the connection.</li> <li>An Aurora DB cluster has no custom endpoints until one is created and up to five custom endpoints can be created for each provisioned cluster.</li> <li>The Reader endpoint is generally not used after defining Custom Endpoints</li> <li>Instance endpoint</li> <li>An instance endpoint connects to a specific DB instance within a cluster and provides direct control over connections to the DB cluster.</li> <li>Each DB instance in a DB cluster has its own unique instance endpoint. So there is one instance endpoint for the current primary DB instance of the DB cluster, and there is one instance endpoint for each of the Replicas in the DB cluster.</li> </ul>"},{"location":"solutions-architect-associate/aurora/#auto-scaling","title":"Auto scaling","text":"<ul> <li>Read replicas are auto scaled</li> <li>Endpoints are extended</li> <li>Suitable if existing Read replicas having high resource usage or getting high requests (to handle sudden increases in connectivity or workload)</li> <li>When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.</li> <li>scaling policy defines the minimum and maximum number of Aurora Replicas that Aurora Auto Scaling can manage.</li> </ul>"},{"location":"solutions-architect-associate/aurora/#aurora-serverless","title":"Aurora Serverless","text":"<ul> <li>Optional</li> <li>Automated database instantiation and auto scaling based on usage</li> <li>Good for unpredictable workloads</li> <li>No capacity planning needed</li> <li>Pay per second</li> </ul>"},{"location":"solutions-architect-associate/aurora/#aurora-multi-master","title":"Aurora Multi-Master","text":"<ul> <li>Optional</li> <li>Every node (replica) in the cluster can read and write</li> <li>Used for immediate failover for write node (high availability in terms of write). If disabled and the master node fails, need to promote a Read Replica as the new master (will take some time).</li> <li>Client needs to have multiple DB connections for failover</li> </ul>"},{"location":"solutions-architect-associate/aurora/#aurora-global-database","title":"Aurora Global Database","text":"<ul> <li>Entire database is replicated across regions to recover from region failure</li> <li>Designed for globally distributed applications with low latency local reads in each region</li> <li>1 Primary Region (read / write)</li> <li>Up to 5 secondary (read-only) regions (replication lag &lt; 1 second)</li> <li>Up to 16 Read Replicas per secondary region</li> <li>Helps for decreasing latency for clients in other geographical locations</li> <li>RTO of less than 1 minute (to promote another region as primary)</li> </ul>"},{"location":"solutions-architect-associate/aurora/#machine-learning","title":"Machine Learning","text":"<ul> <li>To add ML based predictions to applications via SQL</li> <li>Secure integration b/w Aurora and ML services</li> <li>Supported services:</li> <li>Amazon SageMaker (use with any ML model)</li> <li>Amazon Comprehend (for sentiment analysis)</li> </ul> <ul> <li>Use cases:</li> <li>Fraud detection</li> <li>Sentimemt analysis</li> <li>Ads targeting</li> <li>Product recommendations</li> </ul>"},{"location":"solutions-architect-associate/aurora/#backups","title":"Backups","text":"<ul> <li>Automated Backups</li> <li>Backup retention: 1 to 35 days (cannot be disabled)</li> <li>Point in time recovery in that timeframe</li> <li>DB Snapshots:</li> <li>Manually triggered</li> <li>Backup retention: unlimited</li> </ul>"},{"location":"solutions-architect-associate/aurora/#restore-options","title":"Restore options","text":"<ul> <li>Creates a new database</li> <li>Restoring MySQL Aurora cluster from S3</li> <li>Creates backup of your on-premises database using Percona XtraBackup</li> <li>Store it in Amazon S3</li> <li>Restore the backup file onto a new Aurora cluster running MySQL</li> </ul>"},{"location":"solutions-architect-associate/aurora/#database-cloning","title":"Database Cloning","text":"<ul> <li>Creates a new Aurora DB cluster from an existsing one</li> <li>Faster than snapshot &amp; restore</li> <li>New DB cluster uses the same volume and data of original but will change when data updates are made</li> <li>Very fast &amp; cost effective</li> <li>Useful to create a \"staging\" db from \"production\" db without impacting production database</li> </ul>"},{"location":"solutions-architect-associate/aurora/#encryption-network-security","title":"Encryption &amp; Network Security","text":"<ul> <li>Encryption at rest using KMS (same as RDS)</li> <li>Encryption in flight using SSL (same as RDS)</li> <li>You can\u2019t SSH into Aurora instances (same as RDS)</li> <li>Network Security is managed using Security Groups (same as RDS)</li> <li>EC2 instances should access the DB using [IAM DB Authentication] but they can also do it using credentials fetched from the [SSM Parameter Store] (same as RDS)</li> </ul>"},{"location":"solutions-architect-associate/aurora/#aurora-events","title":"Aurora Events","text":"<ul> <li>Invoke a Lambda function from an Aurora MySQL-compatible DB cluster with a native function or a stored procedure (same as RDS)</li> <li>Used to capture data changes whenever a row is modified</li> </ul>"},{"location":"solutions-architect-associate/aws-backup/","title":"Aws backup","text":"<ul> <li>Centrally manage and automate backups of AWS services across regions and accounts</li> <li>Fully managed service</li> <li>Supported services:</li> <li>EC2 / EBS</li> <li>S3</li> <li>RDS / Aurora / DynamoDB</li> <li>DocumentDB / Neptune</li> <li>EFS / FSx (Lustre &amp; Windows)</li> <li>Storage Gateway (Volume Gateway)</li> <li>PITR for supported service</li> <li>On-demand and scheduled backups</li> <li>Tag based backup policies</li> <li>Create backup policies known as backup plans</li> <li>Backup is storing in an internal bucket of S3</li> </ul>"},{"location":"solutions-architect-associate/aws-backup/#aws-backup-vault-lock","title":"AWS Backup Vault Lock","text":"<ul> <li>WORM (Write Once Read Many) model for backups</li> <li>Even the root user cannot delete backups</li> <li>Additional layer of defense to protect your backups against:</li> <li>Inadvertent or malicious delete operations</li> <li>Updates that shorten or alter retention periods</li> </ul>"},{"location":"solutions-architect-associate/aws-batch/","title":"AWS Batch","text":"<ul> <li>Fully managed batch processing at any scale</li> <li>A \"batch job\" is a job with a start and an end</li> <li>Batch will dynamically launch EC2 instances or Spot instances</li> <li>Provisions the right amount of compute/memory</li> <li>Submit or schedule batch jobs</li> <li>Defined as Docker Images and run on ECS</li> </ul>"},{"location":"solutions-architect-associate/aws-batch/#batch-vs-lambda","title":"Batch vs Lambda","text":"<ul> <li>Lambda:</li> <li>Time limit</li> <li>Limited runtimes</li> <li>Limited temporary disk space</li> <li>Serverless</li> <li>Batch:</li> <li>No time limit</li> <li>Any runtime as long it's packaged as a Docker image</li> <li>Rely on EBS/instance store for disk space</li> <li>Relies on EC2</li> </ul>"},{"location":"solutions-architect-associate/aws-config/","title":"AWS Config","text":"<ul> <li>Regional service</li> <li>Can be aggregated across regions and accounts</li> <li>Record configurations changes over time</li> <li>Evaluate compliance of resources using config rules</li> <li>Does not prevent non-compliant actions from happening (no deny)</li> <li>Evaluate config rules</li> <li>for each config change (ex. configuration of EBS volume is changed)</li> <li>at regular time intervals (ex. every 2 hours)</li> <li>Can make custom config rules (must be defined in Lambda functions) such as:</li> <li>Check if each EBS disk is of type gp2</li> <li>Check if each EC2 instance is t2.micro</li> <li>Can be used along with CloudTrail to get a timeline of changes in configuration and compliance overtime.</li> <li>Integrates with EventBridge or SNS to trigger notifications when AWS resources are non-compliant</li> </ul>"},{"location":"solutions-architect-associate/aws-config/#remediation","title":"Remediation","text":"<ul> <li>Automate remediation of non-compliant resources using SSM Automation Documents</li> <li>AWS-Managed Automation Documents</li> <li>Custom Automation Documents<ul> <li>to invoke a Lambda function for automation</li> </ul> </li> <li>You can set Remediation Retries if the resource is still non-compliant after auto remediation</li> <li>Ex. if IAM access key expires (non-compliant), trigger an auto-remediation action to revoke unused IAM user credentials.</li> </ul>"},{"location":"solutions-architect-associate/aws-shield/","title":"AWS Shield","text":"<ul> <li>DDOS: Distributed Denial of Service -&gt; Many request at the same time</li> </ul>"},{"location":"solutions-architect-associate/aws-shield/#shield-standard","title":"Shield Standard","text":"<ul> <li>Free service that is activated for every AWS customer</li> <li>Provides protection from SYN/UDP Floods, Reflection attacks and other layer 3 &amp; layer 4 attacks</li> </ul>"},{"location":"solutions-architect-associate/aws-shield/#shield-advanced","title":"Shield Advanced","text":"<ul> <li>DDoS mitigation service ($3,000 per month per organization)</li> <li>Protect against more sophisticated attacks on</li> <li>EC2 instances</li> <li>Elastic Load Balancing (ELB)</li> <li>CloudFront</li> <li>Global Accelerator</li> <li>Route 53</li> <li>24/7 access to AWS DDoS Response (DRP) team</li> <li>Get reimbursed for usage spikes due to DDoS</li> </ul>"},{"location":"solutions-architect-associate/cloudformation/","title":"CloudFormation","text":"<ul> <li>Infrastructure as Code (IaC) allows us to write our infrastructure as a config file which can be easily replicated &amp; versioned using Git</li> <li>Declarative way of outlining your AWS Infrastructure (no need to specify ordering and orchestration)</li> <li>Resources within a stack is tagged with an identifier (easy to track cost of each stack)</li> <li>CloudFormation creates the tasks for you, in the right order, with the exact configuration that you specify</li> <li>Ability to destroy and re-create infrastructure on the fly</li> <li>Deleting a stack deletes every single artifact that was created by CloudFormation (clean way of deleting resources)</li> <li>You can estimate the cost of your resources using the CloudFormation template</li> <li>Support (almost) all AWS resources</li> <li>You can use \"custom resources\" for resources that are not supported</li> </ul>"},{"location":"solutions-architect-associate/cloudformation/#cloudformation-templates","title":"CloudFormation Templates","text":"<ul> <li>YAML file that defines a CloudFormation Stack</li> <li>Templates have to be uploaded in S3 or upload directly and then referenced in CloudFormation</li> <li>To update a template, upload a new version of the template</li> <li>Template components:</li> <li>Resources: AWS resources declared in the template (mandatory)</li> <li>Parameters: Dynamic inputs for your template</li> <li>Mappings: Static variables for your template</li> <li>Outputs: References to what has been created (will be returned upon stack creation)</li> <li>Conditionals: List of conditions to perform resource creation</li> <li>Metadata</li> <li>Templates helpers:</li> <li>References</li> <li>Functions</li> </ul> <p>You can associate the CreationPolicy attribute with a resource to prevent its status from reaching create complete until CloudFormation receives a specified number of cfn-signal or the timeout period is exceeded.</p> <p>Use CloudFormation with securely configured templates to ensure that applications are deployed in secure configurations</p>"},{"location":"solutions-architect-associate/cloudformation/#stack-sets","title":"Stack Sets","text":"<ul> <li>Create, update, or delete stacks across multiple accounts and regions with a single operation</li> <li>When you update a stack set, all associated stack instances are updated throughout all accounts and regions.</li> </ul>"},{"location":"solutions-architect-associate/cloudformation/#updating-stacks","title":"Updating Stacks","text":"<ul> <li>CloudFormation provides two methods for updating stacks: </li> <li>Direct update<ul> <li>CloudFormation immediately deploys the submitted changes. You cannot preview the changes.</li> </ul> </li> <li>Change Sets<ul> <li>You can preview the changes CloudFormation will make to your stack, and then decide whether to apply those changes.</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/cloudfront/","title":"CloudFront","text":"<ul> <li>Global service</li> <li>Global Content Delivery Network (CDN)</li> <li>Edge Locations are present outside the VPC</li> <li>Supports HTTP/RTMP protocol (does not support UDP protocol)</li> <li>Caches content at edge locations, reducing load at the origin</li> <li>Geo Restriction feature</li> <li>Improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery)</li> <li>To block a specific IP at the CloudFront level, deploy a WAF on CloudFront</li> <li>Supports Server Name Indication (SNI) to allow SSL traffic to multiple domains</li> <li>DDOS protection, Integration with Shield, AWS WAF</li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#origin","title":"Origin","text":"<ul> <li>S3 Bucket</li> <li>For distributing static files</li> <li>Origin Access Identity (OAl) allows the S3 bucket to only be accessed by CloudFront</li> <li>Origin Access Control (OAC) is replacing OAI</li> <li>Can be used as ingress to upload files to S3</li> </ul> <ul> <li>Custom Origin (for HTTP)</li> <li>need to be publicly accessible on HTTP by public IPs of edge locations</li> <li>EC2 Instance</li> <li>ELB</li> <li>S3 Website (may contain client-side script)</li> <li>On-premise backend <p>To restrict access to ELB directly when it is being used as the origin in a CloudFront distribution, create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change.</p> </li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#alb-or-ec2-as-origin","title":"ALB or EC2 as origin","text":""},{"location":"solutions-architect-associate/cloudfront/#geo-restriction","title":"Geo Restriction","text":"<ul> <li>You can restrict who can access your distribution</li> <li>Allowlist: Allow your users to access the content only if they are on the list of approved countries</li> <li>Blocklist: Prevent your user from accessing the content if they are on the list of banned countries</li> <li>The \"country\" is determined using 3rd-party Geo-IP database</li> <li>Use case: Copyright laws to control access to content</li> <li>If you are on the list of countries that are banned from accessing the distribution, you should see the error shown below. <p>403 ERROR The request could not be satisfied.</p> </li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#price-classes","title":"Price classes","text":"<ul> <li>The cost of data out per edge location varies</li> <li>You can reduce the number of edge locations for cost reduction</li> <li>Three price class:</li> <li>Price Class All: all regions (best performance)</li> <li>Price Class 200: most regions (excludes the most expensive regions)</li> <li>Price Class 100: only the least expensive regions</li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#cache-invalidation","title":"Cache invalidation","text":"<ul> <li>CloudFront doesn't know about backend origin update and will only get the refreshed content after the TTL has expired</li> <li>You can force an entire or partial cache refresh (thus bypassing the TTL) by creating a cache invalidation</li> <li>You can invalidate all files (*) or a special path (/images/)</li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#edge-function","title":"Edge Function","text":"<ul> <li>A code that you write and attach to CloudFront distributions</li> <li>Runs close to your users to minimize latency</li> <li>Two types:</li> <li>CloudFront Functions</li> <li>Lambda@Edge</li> <li>No need to manage any servers, deployed globally</li> <li>Pay only for what you use</li> <li>Fully serverless</li> <li>Use case: Customize CDN content</li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#cloudfront-functions","title":"CloudFront Functions","text":"<ul> <li>Lightweight functions written JS</li> <li>For high-scale, latency-sensitive CDN customizations</li> <li>Sub ms startup times, millions of requests/second</li> <li>Used to change viewer requests and responses:</li> <li>Viewer Request: after CloudFront receives a request from viewer</li> <li>Viewer Response: before CloudFront forwards the response to the viewer</li> <li>Uses cases:</li> <li>Cache key normalization (Transform request attributes (headers, cookies, query strings, URL) to create an optimal cache key)</li> <li>Header manipulation (Insert/Modify/Delete HTTP headers in the request or response)</li> <li>URL rewrites or redirects</li> <li>Request authentication &amp; authorization (Create and validate user-generated tokens (eg; JWT) to allow/deny requests)</li> </ul>"},{"location":"solutions-architect-associate/cloudfront/#cloudfront-functions-vs-lambdaedge","title":"CloudFront Functions vs Lambda@Edge","text":""},{"location":"solutions-architect-associate/cloudtrail/","title":"CloudTrail","text":"<ul> <li>Provides governance, compliance and audit for the AWS Account</li> <li>Enabled by default</li> <li>Get a history of events/API calls made within your AWS account by:</li> <li>Console</li> <li>SDK</li> <li>CLI</li> <li>AWS Services</li> <li>Can put logs from CloudTrail into S3 (encrypted by default using SSE-S3) or CloudWatch Logs <p>Modifications to log files can be detected by enabling Log File Validation on the logging bucket</p> </li> <li>A trail can be applied to all regions (default) or a single region</li> <li>If a resource is deleted in AWS, investigate CloudTrail first</li> <li>Event retention: 90 days (To keep events beyond this period, log them to S3 and use Athena)</li> </ul>"},{"location":"solutions-architect-associate/cloudtrail/#event-types","title":"Event Types","text":"<ul> <li>Management Events</li> <li>Events of operations that modify AWS resources. Ex:<ul> <li>Creating a new IAM user</li> <li>Deleting a subnet</li> </ul> </li> <li>Enabled by default</li> <li>Can separate Read Events (that don\u2019t modify resources) from Write Events (that may modify resources)</li> <li>Data Events</li> <li>Events of operations that modify data<ul> <li>S3 object-level activity</li> <li>Lambda function execution</li> </ul> </li> <li>Disabled by default (due to high volume of data events)</li> <li>Can seperate Read and Write events</li> <li>Insight Events</li> <li>Enable CloudTrail Insights to detect unusual activity in your account<ul> <li>inaccurate resource provisioning</li> <li>hitting service limits</li> <li>bursts of AWS IAM actions</li> <li>gaps in periodic maintenance activity</li> </ul> </li> <li>CloudTrail Insights analyzes normal management events to create a baseline and then continuously analyzes write events to detect unusual patterns. If that happens, CloudTrail generates insight events that<ul> <li>show anomalies in the Cloud Trail console</li> <li>can can be logged to S3</li> <li>can trigger an EventBridge event for automation</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/","title":"CloudWatch","text":"<ul> <li>Serverless performance monitoring service</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#metrics","title":"Metrics","text":"<ul> <li>Variable to monitor in CloudWatch (CPUUtilization)</li> <li>Dimension is an attribute of a metric (instance id, environment, etc.)</li> <li>Up to 10 dimensions per metric</li> <li>Metric belong to namespaces</li> <li>Metrics have timestamps</li> <li>Can create Cloudwatch dashboards of metrics</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#cloudwatch-metric-streams","title":"CloudWatch Metric Streams","text":"<ul> <li>Continually stream CloudWatch metrics to a destination of your choice, with near-real-time-delivery and low latency</li> <li>Amazon Kinesis Data Firehouse (and then to its destinations)</li> <li>3rd party service provider (Datadog, New Relic, Splunk...)</li> <li>Option to filter metrics to only stream a subset of them</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#cloudwatch-logs","title":"CloudWatch Logs","text":"<ul> <li>Used to store application logs</li> <li>Logs Expiration: never expire, 30 days, etc.</li> <li>Log groups: arbitrary name, usually represening an application</li> <li>Log stream: instances within application/logs/containers</li> <li>Logs can be sent to:</li> <li>S3 buckets (exports)</li> <li>Kinesis Data Streams</li> <li>Kinesis Data Firehose</li> <li>Lambda functions</li> <li>OpenSearch</li> <li>Logs sources:</li> <li>SDK, CloudWatch Logs Agent, CloudWatch Unified Agent</li> <li>Elastic Beanstalk: collection of logs from application</li> <li>ECS: collection from containers</li> <li>AWS Lambda: collection from function logs</li> <li>VPC Flow logs: VPC specific logs</li> <li>API Gateway</li> <li>CloudTrail based on filter</li> <li>Route53: log DNS queries</li> <li>Metric Filters can be used to filter expressions and use the count to trigger CloudWatch alarms. Example filters:</li> <li>find a specific IP in the logs</li> <li>count occurrences of \u201cERROR\u201d in the logs</li> <li>Cloud Watch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards</li> <li>Logs can take up to 12 hours to become available for exporting to S3 (not real-time)</li> <li>The API call is CreateExportTask</li> <li>To stream logs in real-time, apply a Subscription Filter on logs</li> <li>Logs from multiple accounts and regions can be aggregated using subscription filters</li> </ul> <p>Metric Filters are a part of CloudWatch Logs (not CloudWatch Metrics)</p>"},{"location":"solutions-architect-associate/cloudwatch/#cloudwatch-logs-for-ec2","title":"CloudWatch Logs for EC2","text":"<ul> <li>By default, no logs from EC2 machine will go to CloudWatch and you need to run a CloudWatch agent on EC2 to push the log files you want</li> <li>An IAM role should be applied to EC2 to send logs to cloudwatch</li> <li>The CloudWatch agent can be setup on-premises too</li> <li>EC2 instances have metrics every 5 minutes</li> <li>With detailed monitoring (for a cost), you get metrics every 1 minute</li> <li>CloudWatch Logs Agent</li> <li>Old version of the agent</li> <li>CloudWatch Unified Agent</li> <li>Collect additional system level metrics such as RAM, processes etc</li> <li>Centralized configuration using SSM parameter store</li> <li>CPU (active, guest, idle, system, user, steal)</li> <li>Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops)</li> <li>RAM (free, inactive, used, total, cached)</li> <li>Netstat (number of TCP and UDP connections, net packets, bytes)</li> <li>Processes (total, dead, bloqued, idle, running, sleep)</li> <li>Swap Space (free, used, used %)</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#cloudwatch-alarm","title":"CloudWatch Alarm","text":"<ul> <li>Alarms are used to trigger notifications for any metric (can be based on Metric Filters too)</li> <li>Various options to trigger alarm (sampling, %, max, min, etc.)</li> <li>On a single metric</li> <li>Alarm States:</li> <li>OK</li> <li>INSUFFICIENT_DATA</li> <li>ALARM</li> <li>Period</li> <li>Length of time in seconds to evaluate the metric before triggering the alarm</li> <li>High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec</li> <li>Targets</li> <li>Stop, Terminate, Reboot, or Recover an EC2 Instance</li> <li>Trigger Auto Scaling Action (ASG)</li> <li>Send notification to SNS</li> <li>To test alarms and notifications, set the alarm state to ALARM using CLI</li> <li>Composite Alarms</li> <li>Monitoring the states of multiple other alarms</li> <li>Can use AND and OR conditions</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#ec2-instance-recovery","title":"EC2 Instance Recovery","text":"<ul> <li>Status check</li> <li>Instance status: check the VM</li> <li>System status: check the underlying hardware</li> <li>CloudWatch alarm to automatically recover an EC2 instance if it becomes impaired</li> <li>Terminated instances cannot be recovered</li> <li>After the recovery, the following are retained</li> <li>Placement Group</li> <li>Public IP</li> <li>Private IP</li> <li>Elastic IP</li> <li>Instance ID</li> <li>Instance metadata</li> <li>After the recovery, RAM contents are lost</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#container-insights","title":"Container Insights","text":"<ul> <li>Collect, aggregate, summarize metrics and logs from containers</li> <li>Available for containers on:</li> <li>Amazon ECS</li> <li>Amazon EKS</li> <li>Kubernetes platforms on EC2</li> <li>Fargate (both for ECS and EKS)</li> <li>Using a containerized version of CloudWatch agent to discover containers on Amazon EKS and Kubernetes</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#lambda-insights","title":"Lambda Insights","text":"<ul> <li>Monitoring and troubleshooting solution for serverless applications running on AWS Lambda</li> <li>Collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network</li> <li>Collects, aggregates, and summarizes diagnostic information such as cold starts and Lambda worker shutdowns</li> <li>Provided as a Lambda layer</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#contributor-insights","title":"Contributor Insights","text":"<ul> <li>Analyze log data and create time series that display contributor data</li> <li>Metrics about top-N contributors</li> <li>Helps you find top talkers and understand who or what is impacting system performance</li> <li>For ex: You can find bad hosts, identify the heaviest network users, or find the URLs that generate the most errors.</li> <li>Can build rules from scratch or use sample rules that AWS generated</li> </ul>"},{"location":"solutions-architect-associate/cloudwatch/#application-insights","title":"Application Insights","text":"<ul> <li>Provides automated dashboards that show potential problems with monitored applications, to help isolate ongoing issues</li> <li>Your applications run on Amazon EC2 Instances with select technologies only (Java, .NET, Microsoft IIS, databases...)</li> <li>Also can use other AWS resources such as Amazon EBS, RDS, ASG etc</li> <li>Powered by SageMaker</li> <li>Findings and alerts are sent to Amazon EventBridge and SSM OpsCenter</li> </ul>"},{"location":"solutions-architect-associate/comprehend-medical/","title":"Amazon Comprehend Medical","text":"<ul> <li>Amazon Comprehend Medical detects and returns useful information in unstructured clinical text such as physician's notes, discharge summaries, test results, and case notes.</li> <li>Uses NLP to detect Protected Health Information (PHI)</li> <li>HIPAA eligible service</li> <li>Store your documents in Amazon S3, analyze the real-time data with Kinesis Data Firehose, or use Amazon Transcribe to transcribe patient narratives into text that can be analyzed by Amazon comprehend medical.</li> <li>Amazon Comprehend Medical can analyze only text in English (US-EN).</li> </ul> <p>Documentation</p>"},{"location":"solutions-architect-associate/comprehend/","title":"Amazon Comprehend","text":"<ul> <li>For Natural language processing (NLP)</li> <li>Fully managed and serverless service</li> <li>Uses machine learning to find out insights and relationships in text<ul> <li>Language of the text</li> <li>Extracts key phrases, places, peoples, brands, or events</li> <li>Understands how positive or negative the text it is</li> <li>Analyzes text using tokenization and parts of speech</li> <li>Automatically organizes a collection of text files by topic</li> </ul> </li> <li>Sample use cases:<ul> <li>Analyze customer interactions (emails) to find what leads to a positive or negative experience</li> <li>Create and groups articles by topics that Comprehend will uncover</li> </ul> </li> <li>How it works:</li> </ul>"},{"location":"solutions-architect-associate/concepts/","title":"Scalability","text":"<ul> <li>Ability to handle greater loads by adapting</li> <li>Vertical Scalability (scaling up / down)</li> <li>increasing the size (performance) of the instance</li> <li>used in non-distributed systems, such as a database.</li> <li>limited by hardware</li> <li>From: t2.nano - 0.5G of RAM, 1vCPU </li> <li>To: u-12tb1.metal - 12 TiB of RAM, 448vCPUs</li> <li>Horizontal Scalability (elasticity) (scaling out / in)</li> <li>increasing the number of instances of the application</li> <li>used in distributed systems</li> <li>easier to achieve than vertical scalability</li> <li>Auto scaling group and Load balancer</li> </ul>"},{"location":"solutions-architect-associate/concepts/#high-availability","title":"High Availability","text":"<ul> <li>Ability to survive a hardware or AZ failure</li> <li>Can be achived by running your application / system in at least 2 data centers (== AZ)</li> <li>Auto scaling group multi AZ, Load balancer multi AZ</li> </ul> <p># DNS - Translates the human friendly hostnames into the machine IP addresses - Terminologies:   - Domain Registrar: registers domain names (Amazon Route 53, GoDaddy, etc.)   - DNS Records: A, AAAA, CNAME, NS, etc.   - Hosted Zone: contains DNS records (used to match hostnames to IP addresses)   - Name Server (NS): resolves DNS queries (Authoritative or Non-Authoritative)   - Top Level Domain (TLD)   - Second Level Domain (SLD) - Domain Name Structure</p> <p></p> <ul> <li>How DNS works</li> </ul> <p></p> <ul> <li>Your web browser wants to access the domain example.com which is being served by a server at IP 9.10.11.12. Your browser will first query the local DNS server which if it has that domain cached, it will return it right away. Otherwise, it will ask the same question to the Root DNS server. The root DNS server will extract the TLD (.com) from the domain and direct the local DNS to the TLD DNS Server that can serve .com TLD. The query to the TLD DNS server will be the same. The TLD DNS server returns the IP of the SLD DNS server which can store the IP of web server hosting example.com. Once again the same query is made to the SLD DNS Server which returns the IP 9.10.11.12 instead of NS (named server).</li> </ul>"},{"location":"solutions-architect-associate/concepts/#micro-services-architecture","title":"Micro-Services Architecture","text":"<ul> <li>Many services interact with each other directly using a REST API</li> <li>Allows us to have a leaner development lifecycle for each service</li> <li>Services can scale independently of each other</li> <li>Each service has a separate code repository (easy for development)</li> <li>Communication between services:</li> <li>Synchronous patterns: API Gateway, Load Balancers</li> <li>Asynchronous patterns: SQS, Kinesis, SNS</li> </ul>"},{"location":"solutions-architect-associate/concepts/#software-updates-distribution","title":"Software updates distribution","text":"<ul> <li>We have an application running on EC2, that distributes software updates once in a while</li> <li>When a new software update is out, we get a lot of request and the content is distributed in mass over the network, It's very costly</li> <li>We don't want to change our application, but want to optimize our cost and cpu, how can we do it?</li> </ul> <ul> <li>Why CloudFront?</li> <li>No changes to architecture</li> <li>Will cache software update files at the edge</li> <li>Software update files are not dynamic, they are static (never changing)</li> <li>Our EC2 instances are not serverless</li> <li>But CloudFront is, and will scale for us</li> <li>Our ASG will not scale as much, and we will save more in EC2</li> <li>We will also save in availability, network bandwidth cost etc</li> <li>Easy way to make an existing application more scalable and cheaper</li> </ul>"},{"location":"solutions-architect-associate/concepts/#big-data-ingestion-pipeline","title":"Big Data ingestion pipeline","text":"<ul> <li>IoT Core allows to harvest data from IoT devices</li> <li>Kinesis is great for real-time data collection</li> <li>Firehouse helps with data delivery to S3 in near real-time (1min)</li> <li>Lambda can help Firehouse with data transformations</li> <li>Amazon S3 can trigger notifications to SQS</li> <li>Lambda can subscribe to SQS</li> <li>Athena is a serverless SQL service and results are stored in S3</li> <li>The reporting bucket contains analyzed data and can be used by reporting tool such as AWS QuickSight, Redshift etc</li> </ul>"},{"location":"solutions-architect-associate/concepts/#encryption","title":"Encryption","text":""},{"location":"solutions-architect-associate/concepts/#encryption-in-flight-ssl","title":"Encryption in flight (SSL)","text":"<ul> <li>Data is encrypted before sending and decrypted after receiving</li> <li>SSL certificates helps with encryption (HTTPS)</li> <li>Ensures no MITM (man in the middle attack) can happen</li> </ul>"},{"location":"solutions-architect-associate/concepts/#server-side-encryption-at-rest","title":"Server side encryption at rest","text":"<ul> <li>Data is encrypted after being received by the server</li> <li>Data is decrypted before being sent</li> <li>It is stored in an encrypted form (data key)</li> <li>The encryption/decryption keys must be managed somewhere and the server must have access to it</li> </ul>"},{"location":"solutions-architect-associate/concepts/#client-side-encryption","title":"Client side encryption","text":"<ul> <li>Data is encrypted by the client and never decrypted by the server</li> <li>Data will be decrypted by the receiving client</li> <li>Server should not be able to decrypt the data</li> <li>Could leverage Envelope Encryption</li> </ul>"},{"location":"solutions-architect-associate/control-tower/","title":"AWS Control Tower","text":"<ul> <li>Easy way to setup and govern a secure and compliant multi-account AWS environment based on best practices</li> <li>Uses AWS Organizations to create accounts</li> </ul>"},{"location":"solutions-architect-associate/control-tower/#guardrails","title":"Guardrails","text":"<ul> <li>Provides ongoing governance for your Control Tower environment (AWS accounts)</li> <li>Preventive Guardrails - using SCPs (ex: restrict regions across all your accounts)</li> <li>Detective Guardrails - using AWS Config (ex: identify untagged resources)</li> </ul>"},{"location":"solutions-architect-associate/cost-explorer/","title":"Cost Explorer","text":"<ul> <li>Visualize and manage your costs and service usage over time</li> <li>Create custom reports that analyze cost and usage data</li> <li>Recommendations to choose an optimal savings plan </li> <li>View usage for the last 12 months &amp; forecast up to 12 months</li> </ul>"},{"location":"solutions-architect-associate/database-migration-service/","title":"Database Migration Service (DMS)","text":"<ul> <li>Migrate entire databases from on-premises to AWS cloud</li> <li>The source database remains available during migration</li> <li>Continuous Data Replication using CDC (change data capture)</li> <li>Requires EC2 instance running the DMS software to perform the replication tasks. If the amount of data is large, use a large instance. If multi-AZ is enabled, need an instance in each AZ.</li> </ul>"},{"location":"solutions-architect-associate/database-migration-service/#types-of-migration","title":"Types of Migration","text":"<p>Homogeneous Migration - When the source and target DB engines are the same (eg. Oracle to Oracle) - One step process:   - Use the Database Migration Service (DMS) to migrate data from the source database to the target database</p> <p>Heterogeneous Migration - When the source and target DB engines are different (eg. Microsoft SQL Server to Aurora) - Two step process:   - Use the Schema Conversion Tool (SCT) to convert the source schema and code to match that of the target database   - Use the Database Migration Service (DMS) to migrate data from the source database to the target database</p> <p>Migrating using Snow Family 1. Use the Schema Conversion Tool (SCT) to extract the data locally and move it to the Edge device 2. Ship the Edge device or devices back to AWS 3. After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket. 4. AWS DMS takes the files and migrates the data to the target data store (eg. DynamoDB)</p>"},{"location":"solutions-architect-associate/datasync/","title":"DataSync","text":"<ul> <li>Move large amounts of data from your on-premises NAS or file system via NFS or SMB protocol to AWS over the public internet using TLS</li> <li>Can synchronize to:</li> <li>S3 (all storage classes)</li> <li>EFS</li> <li>FSx (Windows, Lustre, NetApp, OpenZFS...)</li> <li>Replication tasks can be scheduled hourly, daily, and weekly </li> <li>File permission and metadata are preserved (NFS POSIX, SMB...)</li> </ul> <ul> <li>Can also be used to transfer between two storage services in AWS</li> </ul> <ul> <li>Perfect to move large amounts of historical data from on-premises to S3 Glacier Deep Archive (directly).</li> </ul>"},{"location":"solutions-architect-associate/direct-connect/","title":"Direct Connect (DX)","text":"<ul> <li>Dedicated private connection from an on-premise data center to a VPC</li> <li>More stable and secure than Site-to-Site VPN</li> <li>Supports both IPv4 and IPv6</li> <li>Access public (S3) &amp; private (EC2) resources on the same connection using Public &amp; Private Virtual Interface (VIF) respectively</li> <li>Connection to a data center is made from a Direct Connect Location</li> <li>Connects to a Virtual Private Gateway (VGW) on the VPC end</li> <li>Supports Hybrid Environments (on premises + cloud)</li> <li>Lead time &gt; 1 month</li> </ul> <ul> <li>If you want to setup a Direct Connect to one or more VPC in many different regions (same account), you must use a Direct Connect Gateway</li> </ul> <ul> <li>Using DX, we will create a Private VIF to the Direct Connect Gateway which will extend the VIF to Virtual Private Gateways in multiple VPCs (possibly across regions).</li> </ul>"},{"location":"solutions-architect-associate/direct-connect/#connection-types","title":"Connection Types","text":"<ul> <li>Dedicated Connection</li> <li>1 Gbps, 10Gbps and 100 Gbps (fixed capacity)</li> <li>Physical ethernet port dedicated to a customer</li> <li>Hosted Connection</li> <li>50Mbps, 500 Mbps, up to 10 Gbps</li> <li>On-demand capacity scaling (more flexible than dedicated connection)</li> <li>Capacity can be added or removed on-demand</li> </ul>"},{"location":"solutions-architect-associate/direct-connect/#encryption","title":"Encryption","text":"<ul> <li>Data in transit is not-encrypted but the connection is private (secure)</li> <li>For encryption in flight, use AWS Direct Connect + VPN which provides an IPsec-encrypted private connection</li> <li>Good for an extra level of security</li> </ul>"},{"location":"solutions-architect-associate/direct-connect/#resiliency","title":"Resiliency","text":"<ul> <li>Best way (redundant direct connect connections)</li> </ul> <ul> <li>Cost-effective way (VPN connection as a backup)</li> <li>Implement an IPSec VPN connection and use the same BGP prefix. Both the Direct Connect connection and IPSec VPN are active and being advertised using the Border Gateway Protocol (BGP). The Direct Connect link will always be preferred unless it is unavailable.</li> </ul>"},{"location":"solutions-architect-associate/direct-connect/#site-to-site-vpn-connection-as-a-backup","title":"Site-to-Site VPN connection as a backup","text":"<ul> <li>In case Direct Connect (DX) fails, you can setup a backup Direct Connect connection (expensive), or a site-to-site VPN connection</li> <li>So if the private network to DX fails, we can still access services using Site-to-Site VPN through public internet </li> </ul>"},{"location":"solutions-architect-associate/directory-services/","title":"AWS Directory Services","text":""},{"location":"solutions-architect-associate/directory-services/#what-is-microsoft-active-directory-ad","title":"What is Microsoft Active Directory (AD)?","text":"<ul> <li>Found on any Windows server with AD domain services</li> <li>Database of objects: User, Accounts, Computers, Printers, File Shares, Security Groups</li> <li>Centralized security management, create account, assign permissions</li> <li>Objects are organized in trees</li> <li>A group of trees is a forest</li> </ul>"},{"location":"solutions-architect-associate/directory-services/#aws-managed-microsoft-ad","title":"AWS Managed Microsoft AD","text":"<ul> <li>Create your own AD in AWS, manage users locally, supports MFA</li> <li>Establish \"trust\" connections with your on-premise AD</li> <li>Integration is out of the box to connect IAM Identity center with AWS Managed Microsoft AD</li> <li>To connect to a Self-managed directory with IAM Identity center, create a two way Trust relatioship using AWS Managed Microsoft AD</li> </ul>"},{"location":"solutions-architect-associate/directory-services/#ad-connector","title":"AD Connector","text":"<ul> <li>Directory Gateway (proxy) to redirect to on-premise AD, supports MFA</li> <li>Users are managed on on-premise AD</li> <li>To connect to a Self-managed directory with IAM Identity center, create an AD </li> </ul>"},{"location":"solutions-architect-associate/directory-services/#simple-ad","title":"Simple AD","text":"<ul> <li>AD-compatible managed directory on AWS</li> <li>Cannot be joined with on-premise AD</li> </ul>"},{"location":"solutions-architect-associate/disaster-recovery/","title":"Disaster Recovery","text":"<ul> <li>Any event that has a negative impact on a company\u2019s business continuity or finances is a disaster</li> <li>https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</li> </ul>"},{"location":"solutions-architect-associate/disaster-recovery/#types-of-disaster-recovery-dr","title":"Types of Disaster Recovery (DR)","text":"<ul> <li>On-premise =&gt; On-premise: traditional and very expensive</li> <li>On-premise =&gt; AWS Cloud: hybrid recovery</li> <li>AWS Cloud Region A =&gt; AWS Cloud Region B</li> </ul> <p>Recovery Point Objective (RPO): how often you backup your data (determines how much data are you willing to lose in case of a disaster) Recovery Time Objective (RTO): how long it takes to recover from the disaster (down time)</p> <p></p>"},{"location":"solutions-architect-associate/disaster-recovery/#strategies","title":"Strategies","text":"<p>Backup &amp; Restore - High RPO (hours) - Need to spin up instances and restore volumes from snapshots in case of a disaster =&gt; High RTO - Cheapest and easiest to manage</p> <p>Pilot Light - Critical parts of the app are always running in the cloud (eg. continuous replication of data to another region) - Low RPO (minutes) - Critical systems are already up =&gt; Low RTO - Ideal when RPO should be in minutes and the solution should be inexpensive - DB is critical so it is replicated continuously but EC2 instance is spin up only when a disaster strikes</p> <p>Warm Standby - A complete backup system is up and running at the minimum capacity. This system is quickly scaled to production capacity in case of a disaster. - Very low RPO &amp; RTO (minutes) - Expensive</p> <p>Multi-Site or Hot Site Approach - A backup system is running at full production capacity and the request can be routed to either the main or the backup system. - Multi-data center approach - Lowest RPO &amp; RTO (minutes or seconds) - Very Expensive</p>"},{"location":"solutions-architect-associate/disaster-recovery/#disaster-recovery-tips","title":"Disaster Recovery Tips","text":"<p>Backup - EBS snapshots, RDS automated backups/snapshots etc - Regular pushes to S3 / S3 IA / Glacier, Life cycle policy, Cross region replication - From On-premise: Snowball or Storage gateway</p> <p>High Availability - Use Route53 to migrate DNS over from region to region - RDS Multi AZ, ElastiCache Multi-AZ, EFS, S3 - Site to Site VPN as a recovery from Direct Connect</p> <p>Replication - RDS replication (cross region), AWS Aurora + global databases - Database replication from on-premise to RDS - Storage gateway</p> <p>Automation - CloudFormation /  Elastic BeanStalk to recreate a whole new environment - Recover / Reboot EC2 instances with cloudwatch if alarms fails - AWS Lambda functions for customized automations</p> <p>Chaos - Netflix has a \"simian-army\" randomly terminating EC2</p>"},{"location":"solutions-architect-associate/discussions/","title":"Classic Solutions Architecture Discussions","text":""},{"location":"solutions-architect-associate/discussions/#stateless-webapp-whatisthetimecom","title":"Stateless webapp - whatisthetime.com","text":"<ul> <li>whatisthetime.com allows people to know what time is it</li> <li>we don't need a database</li> </ul>"},{"location":"solutions-architect-associate/discussions/#starting-simple","title":"Starting simple","text":"<ul> <li>Create a public EC2 with t2 class and assign elastic IP address so that user can access the webapp using the IP</li> </ul>"},{"location":"solutions-architect-associate/discussions/#scaling-vertically","title":"Scaling vertically","text":"<ul> <li>More users are accessing the webapp and the t2 instance can't handle that much requests so we are scaling vertically by upgrading the instance class from t2 to m5</li> <li>There will be downtime when upgrading to m5</li> </ul>"},{"location":"solutions-architect-associate/discussions/#scaling-horizontally","title":"Scaling horizontally","text":"<ul> <li>More and more users are accessing the webapp and the m5 instance can't handle the requests. </li> <li>This time we are scaling it horizontally meaning adding more number of instances</li> <li>Add couple of m5 instances and attach elastic ip to all instances</li> <li>This approach is not good because users need to remember the IP addresses also we can only use 5 Elastic IP addresses per Region.</li> <li>Remove the elastic IP addresses and create Route 53 hosted zone and add a domain (api.whatisthetime.com) with the value of IP address of public EC2 instances with a TTL value of 1hr</li> <li>What if a single instance fails, causing users to experience up to an hour of downtime before being transferred to a functioning instance?</li> </ul>"},{"location":"solutions-architect-associate/discussions/#scaling-horizontally-with-a-load-balancer","title":"Scaling horizontally with a load balancer","text":"<ul> <li>Create private EC2 instances manually in an availability zone</li> <li>Create a public facing ELB, includes health checks as well</li> </ul> <ul> <li>The traffic b/w ELB and EC2 instances are restrcited using security group rules</li> <li>Update the record api.whatisthetime.com with an alias record to point to the ELB endpoint</li> </ul>"},{"location":"solutions-architect-associate/discussions/#scaling-horizontally-with-auto-scaling-group","title":"Scaling horizontally with auto-scaling group","text":"<ul> <li>Create an auto-scaling group in the AZ and rest of the architecture is same</li> <li>Now we have no downtime</li> </ul>"},{"location":"solutions-architect-associate/discussions/#making-our-app-multi-az","title":"Making our app multi-AZ","text":"<ul> <li>What if an earthquke strikes and our only AZ is gone, need to make our app multi-AZ</li> <li>Making the ELB to use multiple AZs</li> </ul> <ul> <li>Auto-scaling group place the instances in multiple AZs</li> <li>Even if one AZ is gone, we should have another</li> </ul>"},{"location":"solutions-architect-associate/discussions/#cost-saving-approach","title":"Cost saving approach","text":"<ul> <li>5 pillars for a well architected application</li> <li>costs</li> <li>performance</li> <li>reliability</li> <li>security</li> <li>operational excellence</li> </ul>"},{"location":"solutions-architect-associate/discussions/#stateful-web-app-myclothescom","title":"Stateful web app: Myclothes.com","text":"<ul> <li>myclothes.com allows users to buy clothes online</li> <li>There's a shopping cart</li> <li>Our website is having hundreads of users at the same time</li> <li>We need to scale, maintain horizontal scalability and keep our web app as stateless as possible</li> <li>Users should not lose their shopping cart</li> <li>Users should have their details (address,etc) in a database</li> </ul> <ul> <li>User accessing the webapp and ELB route the request to the instance in AZ1 and the shopping cart data is stored in AZ1 instance</li> <li>Suppose ELB re-route the second request to instance in AZ2 thus user lossing the shopping cart data</li> </ul>"},{"location":"solutions-architect-associate/discussions/#introduce-stickiness-session-affinity","title":"Introduce Stickiness (Session Affinity)","text":"<ul> <li>By enabling stickiness in ELB, every request from a user will go to the same instance and user will be able to access the shopping cart data everytime</li> </ul> <ul> <li>What if the instace failed? User will loss the shopping cart data</li> </ul>"},{"location":"solutions-architect-associate/discussions/#introduce-user-cookies","title":"Introduce User Cookies","text":"<ul> <li>Instead of storing the shopping cart data in EC2 servers, store the data in web cookies by user (in web browsers)</li> </ul> <ul> <li>Everytime the user connect to ELB, the shopping cart data will be fetch from web cookies</li> </ul>"},{"location":"solutions-architect-associate/discussions/#introduce-server-session","title":"Introduce Server Session","text":"<ul> <li>Instead of storing the whole shopping cart data in web cookies, store only the session_id in web cookies</li> <li>Ec2 instance store the session data in ElastiCache </li> </ul> <ul> <li>On the next requests, the EC2 instances can retrieve the session data from ElastiCache</li> <li>It's a secure method and no attackers can intrude into ElastiCache</li> <li>As an alternative, we can use amazon DynamoDB to store the session data</li> </ul>"},{"location":"solutions-architect-associate/discussions/#storing-user-data-in-database","title":"Storing user data in database","text":"<ul> <li>EC2 instance can store the user data (address, name, etc) in a Amazon RDS database</li> </ul>"},{"location":"solutions-architect-associate/discussions/#scaling-reads","title":"Scaling reads","text":"<ul> <li>Suppose the webapp getting large number of requests so we need to scale the reads</li> </ul> <ul> <li>We will be having a RDS master for writes</li> <li>We can enable read replicas so that every read requests will go to read replica instances</li> </ul>"},{"location":"solutions-architect-associate/discussions/#scaling-reads-alternative-lazy-loading","title":"Scaling reads alternative - Lazy loading","text":"<ul> <li>User talk to EC2 instance and it check if the data is cached in ElastiCache</li> <li>If the cache is not present in ElastiCache, Ec2 send requests to RDS and store the data in ElastiCache</li> <li>For next time, the EC2 retrive the data from ElastiCache (hit)</li> </ul> <ul> <li>By using this strategy, the RDS will receive less requests, which will make it less stressed.</li> </ul>"},{"location":"solutions-architect-associate/discussions/#multi-az-survive-disasters","title":"Multi-AZ: Survive disasters","text":"<ul> <li>Enable multi-az for ELB, Auto-scaling group, RDS, and ElastiCache (Redis)</li> </ul>"},{"location":"solutions-architect-associate/discussions/#security-groups","title":"Security groups","text":""},{"location":"solutions-architect-associate/discussions/#stateful-web-app-mywordpresscom","title":"Stateful web app - Mywordpress.com","text":"<ul> <li>We are trying to create a fully scalable wordpress website</li> <li>We want the website to access and correctly display picture uploads</li> <li> <p>Our user data, blog content should be stored in MySQL database</p> </li> <li> <p>RDS can be used to store the user data however Aurora is recommended with multi-az and read replica</p> </li> <li>Storing images with EBS is useful for single application</li> <li>If it's distributed application, make use EFS</li> </ul> <p></p>"},{"location":"solutions-architect-associate/discussions/#instantiating-applications-quickly","title":"Instantiating applications quickly","text":"<ul> <li>EC2 Instances</li> <li>Use a Golden AMI: Install your applications, OS dependencies etc beforehand and launch your instances from golden AMI. Golden AMI is an image that contains all your software installed and configured so that future EC2 instances can boot up quickly from that AMI.</li> <li>Bootstrap using User Data: For dynamic configuration, use user data scripts</li> <li>Hybrid: mix Golden AMI and User data (ElasticBeanstalk)</li> <li>RDS databases</li> <li>Restore from snapshot: the database will have schemas and data ready</li> <li>EBS volume</li> <li>Restore from snapshot: the disk will already be formatted and have data</li> </ul>"},{"location":"solutions-architect-associate/docker/","title":"Docker","text":"<ul> <li>Software development platform to deploy apps</li> <li>Apps are packaged in containers that can be run on any OS</li> <li>Any machine</li> <li>No compatability issues</li> <li>Predictable behaviour</li> <li>Less work</li> <li>Easier to maintain and deploy</li> <li>Works with any language, any OS, any technology</li> <li>Use cases: Microservices architecture, lift and shift apps from on-premises to aws cloud...</li> <li>Docker images are stored in Docker repositories</li> <li>Docker Hub (https://hub.docker.com/)<ul> <li>Public repository</li> <li>Find base images for many technologies or OS (Ubuntu, MySQL...)</li> </ul> </li> <li>Amazon ECR (Amazon Elastic Container Registry)<ul> <li>Private repository</li> <li>Public repository (Amazon ECR public gallery https://gallery.ecr.aws/)</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/docker/#docker-vs-vm","title":"Docker vs VM","text":"<ul> <li>Resources are shared with host =&gt; can run many containers in one server</li> </ul>"},{"location":"solutions-architect-associate/docker/#docker-container-management-on-aws","title":"Docker container management on AWS","text":"<ul> <li>Amazon Elastic Container Service (Amazon ECS)</li> <li>Amazon's own container platform</li> <li>Amazon Elastic Kubernetes Service (Amazon EKS)</li> <li>Amazon's managed Kubernetes (opensource)</li> <li>AWS Fargate</li> <li>Amazon's own Serverless platform</li> <li>Works with both ECS and EKS</li> <li>Amazon ECR</li> <li>Store container images</li> </ul>"},{"location":"solutions-architect-associate/documentdb/","title":"DocumentDB","text":"<ul> <li>Aurora version for MongoDB (which is a NoSQL database)</li> <li>Used to store, query, and index JSON data</li> <li>Similar deployment concepts as Aurora</li> <li>Fully managed, HA with replication across 3 AZ</li> <li>Storage automatically grows in increments of 10GB, upto 64TB</li> <li>Automatically scales to workloads with millions of requests per seconds</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/","title":"DynamoDB","text":"<ul> <li>Fully managed, HA with multi-az</li> <li>Distributed Database</li> <li>NoSQL DB with transaction support</li> <li>Not an in-memory database (uses storage devices)</li> <li>Storage auto-scaling</li> <li>Single digit millisecond response time at any scale</li> <li>Maximum size of an item: 400 KB</li> <li>Millions of requests per seconds, trillions of row, 100s of TB storage</li> <li>Integrated with IAM for security, authorization and administration</li> <li>Standard and Infrequest Access (IA) Table class</li> <li>Primary key (must be decided at creation) can be a single field or a pair of fields (partition key and sort key)</li> <li>Each table can have an infinite number of items (=rows)</li> <li>Each item has attributes (can be added over time-can be null)</li> <li>You can rapidly evolve schemas</li> <li>Data types supported are:</li> <li>Scalar types:  String, Boolean, Number, Binary, Null</li> <li>Document types: List, Map</li> <li>Set types: String set, Number set, Binary set</li> <li>Supports TTL (automatically delete an item after an expiry timestamp)</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/#capacity-modes","title":"Capacity modes","text":"<ul> <li>Provisioned Mode (default)</li> <li>Provision read &amp; write capacity</li> <li>Pay for the provisioned capacity (Read capacity unit (RCU) and Write capacity unit (WCU))</li> <li>Auto-scaling option (eg. set RCU and WCU to 80% and the capacities will be scaled automatically based on the workload)</li> <li>RCU and WCU are decoupled, so you can increase/decrease each value separately.</li> <li>On-demand Mode</li> <li>Capacity auto-scaling based on the workload</li> <li>Pay for what you use (more expensive)</li> <li>Great for unpredictable workloads, steep sudden spikes</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator (DAX)","text":"<ul> <li>Fully managed, highly available, in-memory cache</li> <li>Caches the queries and scans of DynamoDB items</li> <li>Solves read congestion (ProvisionedThroughputExceededException)</li> <li>Microseconds latency for cached data</li> <li>Doesn\u2019t require application code changes</li> <li>5 minutes TTL for cache (default)</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/#dynamodb-streams","title":"DynamoDB Streams","text":"<ul> <li>Ordered stream of notifications of item-level modifications (create/update/delete) in a table</li> <li>Destination can be</li> <li>Kinesis Data Streams</li> <li>AWS Lambda</li> <li>Kinesis Client Library applications</li> <li>Data Retention for up to 24 hours</li> <li>Use cases: sending welcome emails to new users after they sign up</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/#global-tables","title":"Global Tables","text":"<ul> <li>For low latency access in multiple-regions</li> <li>Applications can READ and WRITE to the table in any region and the change will automatically be replicated to other tables (active-active cross-region replication)</li> <li>Must enable DynamoDB Streams as a pre-requisite (DynamoDB Streams enable DynamoDB to get a changelog and use that changelog to replicate data across replica tables in other AWS Regions.)</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/#backups-for-dr","title":"Backups for DR","text":"<ul> <li>Continuous backups using point-time-recovery (PITR)</li> <li>Optionally enabled for the last 35 days</li> <li>The recovery process creates a new table</li> <li>On-demand backups</li> <li>Full backups for long-term retention, until explicitely deleted</li> <li>Doesn't affect performance or latency</li> <li>Can be configured and managed in AWS Backup (enables cross-region copy)</li> <li>The recovery process creates a new table</li> </ul>"},{"location":"solutions-architect-associate/dynamodb/#integration-with-s3","title":"Integration with S3","text":"<ul> <li>Export to S3 (must enable PITR)</li> <li>Works for any point of time in the last 35 days</li> <li>Retain snapshots for auditing</li> <li>Export in DynamoDB JSON or ION format</li> <li>Import from S3</li> <li>Import CSV, DynamoDB JSON or ION format</li> <li>Creates new table</li> <li>Import errors are logged in cloudwatch logs</li> </ul>"},{"location":"solutions-architect-associate/elastic-beanstalk/","title":"Elastic Beanstalk","text":"<ul> <li>Used to deploy applications on AWS infrastructure</li> <li>Platform as a Service (PaaS)</li> <li>Managed AWS service</li> <li>Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration, etc. but we have full control over the configuration</li> <li>Free (pay for the underlying resources)</li> <li>Supports versioning of application code</li> <li>Can create multiple environment (dev, test, prod)</li> <li>Supports the deployment of web applications from Docker containers and automatically handles load balancing, auto-scaling, monitoring, and placing containers across the cluster.</li> </ul>"},{"location":"solutions-architect-associate/elastic-beanstalk/#web-worker-environments","title":"Web &amp; Worker Environments","text":"<ul> <li>Web Environment (Web Server Tier): clients requests are directly handled by EC2 instances through a load balancer.</li> <li>Worker Environment (Worker Tier): clients\u2019s requests are put in a SQS queue and the EC2 instances will pull the messages to process them. Scaling depends on the number of SQS messages in the queue.    </li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/","title":"Elastic Block Storage (EBS)","text":"<ul> <li>Volume Network Drive (provides low latency access to data)</li> <li>Can only be mounted to 1 instance at a time (except EBS multi-attach)</li> <li>Bound to an AZ</li> <li>Must provision capacity in advance (size in GB &amp; throughput in IOPS)</li> <li>By default, upon instance termination, the root EBS volume is deleted and any other attached EBS volume is not deleted (can be over-ridden using DeleteOnTermination attribute)</li> <li>To replicate an EBS volume across AZ or region, need to copy its snapshot</li> <li>EBS Multi-attach allows the same EBS volume to attach to multiple EC2 instances in the same AZ</li> <li>DeleteOnTermination attribute can be controlled by AWS Console/AWS CLI</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#volume-types","title":"Volume Types","text":""},{"location":"solutions-architect-associate/elastic-block-storage/#general-purpose-ssd","title":"General Purpose SSD","text":"<ul> <li>Good for system boot volumes, virtual desktops</li> <li>Storage: 1 GB - 16 TB</li> <li>gp3</li> <li>3,000 lOPS baseline (max 16,000 - independent of size)</li> <li>125 MiB/s throughput (max 1000MiB/s - independent of size)</li> <li>gp2</li> <li>Max IOPS: 16,000 (at 5,334 GB)</li> <li>3 IOPS per GB</li> <li>Burst IOPS up to 3,000</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#provisioned-iops-ssd","title":"Provisioned IOPS SSD","text":"<ul> <li>Optimized for Transaction-intensive Applications with high frequency of small &amp; random IO operations. They are sensitive to increased I/O latency.</li> <li>Maintain high IOPS while keeping I/O latency down by maintaining a low queue length and a high number of IOPS available to the volume.</li> <li>Supports EBS Multi-attach (not supported by other types)</li> <li>io1 or io2</li> <li>Storage: 4 GB - 16 TB</li> <li>Max IOPS: 64,000 for Nitro EC2 instances &amp; 32,000 for non-Nitro</li> <li>50 IOPS per GB (64,000 IOPS at 1,280 GB)</li> <li>io2 have more durability and more IOPS per GB (at the same price as io1)</li> <li>io2 Block Express</li> <li>Storage: 4 GiB - 64 TB</li> <li>Sub-millisecond latency</li> <li>Max IOPS: 256,000</li> <li>1000 IOPS per GB</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#hard-disk-drives-hdd","title":"Hard Disk Drives (HDD)","text":"<ul> <li>Optimized for Throughput-intensive Applications that require large &amp; sequential IO operations and are less sensitive to increased I/O latency (big data, data warehousing, log processing)</li> <li>Maintain high throughput to HDD-backed volumes by maintaining a high queue length when performing large, sequential I/O</li> <li>Cannot be used as boot volume for an EC2 instance</li> <li>Storage: 125 GB - 16 TB</li> <li>Throughput Optimized HDD (st1)</li> <li>Optimized for large sequential reads and writes (Big Data, Data Warehouses, Log Processing)</li> <li>Max throughput: 500 MB/s</li> <li>Max IOPS: 500</li> <li>Cold HDD (sc1)</li> <li>For infrequently accessed data</li> <li>Cheapest</li> <li>Max throughput: 250 MB/s</li> <li>Max IOPS: 250</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#ebs-multi-attach-io1io2-family","title":"EBS Multi-Attach - io1/io2 family","text":"<ul> <li>Attach the same EBS volume to multiple EC2 instances in the same AZ</li> <li>Each instance has full read &amp; write permissions to the high performance volume</li> <li>Use case:</li> <li>Achieve higher application availability in clustered Linux applications (ex: Teradata)</li> <li>Application must manage concurrent write operations</li> <li>Upto 16 instances at a time</li> <li>Must use file system that's cluster aware (not XFS, EX4 etc)</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#encryption","title":"Encryption","text":"<ul> <li>Optional</li> <li>For Encrypted EBS volumes</li> <li>Data at rest is encrypted</li> <li>Data in-flight between the instance and the volume is encrypted</li> <li>All snapshots are encrypted</li> <li>All volumes created from the snapshot are encrypted</li> <li>Encrypt an un-encrypted EBS volume</li> <li>Create an EBS snapshot of the volume</li> <li>Copy the EBS snapshot and encrypt the new copy</li> <li>Create a new EBS volume from the encrypted snapshot (the volume will be automatically encrypted)</li> </ul> <p>All EBS types and all instance families support encryption but not all instance types support encryption.</p>"},{"location":"solutions-architect-associate/elastic-block-storage/#ebs-snapshots","title":"EBS Snapshots","text":"<ul> <li>Backup of your EBS volume</li> <li>Not necessary to detach volume to do snapshot, but recommended</li> <li>Can copy across AZ or region</li> <li>Data Lifecycle Manager (DLM) can be used to automate the creation, retention, and deletion of snapshots of EBS volumes</li> <li>Snapshots are incremental (which means that only the blocks on the device that have changed after your most recent snapshot are saved)</li> <li>Only the most recent snapshot is required to restore the volume</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#ebs-snapshots-features","title":"EBS Snapshots features","text":"<ul> <li>EBS Snapshot Archive</li> <li>Move a Snapshot to an \"archive tier\" that is 75% cheaper but takes 24 to 72 hours for restoring</li> <li>Recyle Bin for Snapshots</li> <li>Setup rules to retain deleted snapshots for accidental deletion and can specify retention (from 1 day to 1 year)</li> <li>Fast Snapshot Restore</li> <li>Force full initialization of snapshot (No latency on first use)</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#amazon-machine-image-ami","title":"Amazon Machine image (AMI)","text":"<ul> <li>AMI are a customization of EC2 instance</li> <li>You add your on software, configuration, operating system, monitoring...</li> <li>Faster boot/ configuration becuase all your software is pre-packaged</li> <li>AMI are built for a specific region and the AMI ID will be differ across regions (can be copied across regions)</li> <li>AMI are stored in S3 but can't be find in S3 console</li> <li>You can launch EC2 instances from</li> <li>A Public AMI: AWS provided</li> <li>Your own AMI: you make and maintain them yourself</li> <li>AWS Marketplace AMI: an AMI someone else made and potentially sells</li> <li>Once we deregister an AMI, the metadata for creating an instance will be deleted (removes the block device details) and we need to remove the snapshots seperately</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#ami-process-from-an-ec2-instance","title":"AMI Process (from an EC2 instance)","text":"<ul> <li>Start an EC2 instance and customize it</li> <li>Stop the instance (for data integrity)</li> <li>Build AMI - this will also create EBS snapshots</li> <li>Launch instances from AMI</li> </ul>"},{"location":"solutions-architect-associate/elastic-block-storage/#ami-permission","title":"AMI Permission","text":"<ul> <li>Public: The AMI can access for all users in that region</li> <li>Private: The AMI can access only from the account which created </li> <li>We can share the AMI across accounts so that they can also acces the AMI but the AMI reside only on the AMI owner account</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/","title":"Elastic Compute Cloud (EC2)","text":"<ul> <li>Regional Service</li> <li>EC2 (Elastic Compute Cloud) is an Infrastructure as a Service (IaaS)</li> <li>Stopping &amp; Starting an instance may change its public IP but not its private IP</li> <li>AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads</li> <li>There is a vCPU-based On-Demand Instance soft limit per region</li> <li>For EC2, Only memory, CPU, and network are provided by the host machine. The Disk will not and it will not be on that hardware.</li> <li>The hard-disk of EC2 is called EBS Volume</li> <li>EC2 &amp; EBS will be in seperate servers and both will be in same AZ, connected each other using high speed network</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#user-data","title":"User Data","text":"<ul> <li>Some commands that run when the instance is launched for the first time (doesn't execute for subsequent runs)</li> <li>Used to automate dynamic boot tasks (that cannot be done using AMIs)</li> <li>Installing updates</li> <li>Installing software</li> <li>Downloading common files from the internet</li> <li>Runs with the root user privilege</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#instance-classes","title":"Instance Classes","text":"<p>For Ex: m5.2xlarge :  m -&gt; instance class, 5 -&gt; generation, 2xlarge -&gt; size within the instance class. EC2 Instance Types. We can compare EC2 instace types using this site - General Purpose   - Great for a diversity of workloads such as web servers or code repositories   - Balance between compute, memory &amp; networking - Compute Optimized   - Great for compute intensive tasks     - Batch Processing     - Media Transcoding     - HPC     - Gaming Servers - Memory Optimized   - Great for in-memory databases or distributed web caches - Storage Optimized   - Great for storage intensive tasks (accessing local databases)     - OLTP (Online transaction processing) systems      - Distributed File System (DFS)</p>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#security-groups","title":"Security Groups","text":"<ul> <li>Only contain Allow rules</li> <li>We can't deny rules</li> <li>External firewall for EC2 instances (if a request is blocked by SG, instance will never know)</li> <li>Security groups rules can reference a resource by IP or Security Group</li> <li>Default SG</li> <li>inbound traffic from the same SG is allowed</li> <li>all outbound traffic is allowed</li> <li>New SG</li> <li>all inbound traffic is blocked</li> <li>all outbound traffic is allowed</li> <li>A security group can be attached to multiple instances </li> <li>An instance can have multiple security groups</li> <li>If multiple security groups are attached to an instance then, the rules will be combined</li> <li>An instance can't be launched without a security group. Atleast one is required</li> <li>Bound to a VPC (and hence to a region)</li> <li>Recommended to maintain a separate security group for SSH access</li> <li>Blocked requests will give a Time Out error</li> <li>Security groups is on instance-level</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#classic-ports-to-know","title":"Classic Ports to know","text":"<ul> <li>22 = SSH</li> <li>21 = FTP</li> <li>22 = SFTP</li> <li>80 = HTTP</li> <li>443 = HTTPS</li> <li>3389 = RDP</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#ec2-instance-connect","title":"EC2 Instance Connect","text":"<ul> <li>Browser based command line tool to access EC2 instance</li> <li>SSH protocol should be allowed in security group</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#iam-roles-for-ec2-instances","title":"IAM Roles for EC2 instances","text":"<ul> <li>Never enter AWS credentials into the EC2 instance or configure aws cli using access key and secret key, instead attach IAM Roles to the instances</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#purchasing-options","title":"Purchasing Options","text":""},{"location":"solutions-architect-associate/elastic-compute-cloud/#on-demand-instances","title":"On-demand Instances","text":"<ul> <li>Pay per use (no upfront payment)</li> <li>Highest cost</li> <li>No long-term commitment</li> <li>Recommended for short-term, uninterrupted and unpredictable workloads</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#standard-reserved-instances","title":"Standard Reserved Instances","text":"<ul> <li>Reservation Period: 1 year or 3 years</li> <li>Recommended for steady-state applications (like database)</li> <li>Sell unused instances on the Reserved Instance Marketplace</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#convertible-reserved-instances","title":"Convertible Reserved Instances","text":"<ul> <li>Can change the instance type</li> <li>Lower discount</li> <li>Cannot sell unused instances on the Reserved Instance Marketplace</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#scheduled-reserved-instances","title":"Scheduled Reserved Instances","text":"<ul> <li>reserved for a time window (ex. everyday from 9AM to 5PM)</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#spot-instances","title":"Spot Instances","text":"<ul> <li>Work on a bidding basis where you are willing to pay a specific max hourly rate for the instance. Your instance can terminate if the spot price increases.</li> <li>It is always possible that your Spot Instance might be interrupted.</li> <li>Good for workloads that are resilient to failure</li> <li>Distributed jobs (resilient if some nodes go down)</li> <li>Batch jobs</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#dedicated-hosts","title":"Dedicated Hosts","text":"<ul> <li>Server hardware is allocated to a specific company (not shared with other companies)</li> <li>3 year reservation period</li> <li>Billed per host</li> <li>Useful for software that have BYOL (Bring Your Own License) or for companies that have strong regulatory or compliance needs</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#dedicated-instances","title":"Dedicated Instances","text":"<ul> <li>Dedicated hardware</li> <li>Billed per instance</li> <li>No control over instance placement</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#on-demand-capacity-reservations","title":"On-Demand Capacity Reservations","text":"<ul> <li>Ensure you have the available capacity in an AZ to launch EC2 instances when needed</li> <li>Can reserve for a recurring schedule (ex. everyday from 9AM to 5PM)</li> <li>No need for 1 or 3-year commitment (independent of billing discounts)</li> <li>Need to specify the following to create capacity reservation: - AZ - Number of instances - Instance attributes</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#reserved-capacity-instances-comparison","title":"Reserved Capacity &amp; Instances Comparison","text":""},{"location":"solutions-architect-associate/elastic-compute-cloud/#spot-instances_1","title":"Spot Instances","text":""},{"location":"solutions-architect-associate/elastic-compute-cloud/#spot-requests","title":"Spot Requests","text":"<ul> <li>One-time: Request once opened, spins up the spot instances and the request closes</li> <li>Persistent:</li> <li>Request will stay disabled while the spot instances are up and running.</li> <li>It becomes active after the spot instance is interrupted.</li> <li>If you stop the spot instance, the request will become active only after you start the spot instance.</li> <li>You can only cancel spot instance requests that are open, active, or disabled.</li> <li>Cancelling a Spot Request does not terminate instances. You must first cancel a Spot Request, and then terminate the associated Spot Instances.</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#spot-fleets","title":"Spot Fleets","text":"<ul> <li>Combination of spot and on-demand instances (optional) that tries to optimize for cost or capacity</li> <li>Launch Templates must be used to have on-demand instances in the fleet</li> <li>Can consist of instances of different classes</li> <li>Strategies to allocate Spot Instances:</li> <li>lowestPrice - from the pool with the lowest price (cost optimization, short workload)</li> <li>diversified - distributed across all pools (great for availability, long workloads)</li> <li>capacityOptimized - pool with the optimal capacity for the number of instances</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#elastic-ip","title":"Elastic IP","text":"<ul> <li>Static Public IP that you own as long as you don't delete it</li> <li>Can be attached to an EC2 instance (even when it is stopped)</li> <li>Soft limit of 5 elastic IPs per account</li> <li>Doesn\u2019t incur charges as long as the following conditions are met (EIP behaving like any other public IP randomly assigned to an EC2 instance):</li> <li>The Elastic IP is associated with an Amazon EC2 instance</li> <li>The instance associated with the Elastic IP is running</li> <li>The instance has only one Elastic IP attached to it</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#placement-groups-placement-strategies","title":"Placement Groups (Placement Strategies)","text":"<ul> <li>Cluster Placement Group (optimize for network)</li> <li>All the instances are placed on the same hardware (same rack)</li> <li>Pros: Great network (10 Gbps bandwidth between instances)</li> <li>Cons: If the rack fails, all instances will fail at the same time</li> <li>Used in HPC (minimize inter-node latency &amp; maximize throughput) </li> <li>Spread Placement Group (maximize availability)</li> <li>Each instance is in a separate rack (physical hardware) inside an AZ</li> <li>Supports Multi AZ</li> <li>Up to 7 instances per AZ per placement group (ex. for 15 instances, need 3 AZ)</li> <li>Used for critical applications </li> <li>Partition Placement Group (balance of performance and availability)</li> <li>Instances in a partition share rack with each other</li> <li>If the rack goes down, the entire partition goes down</li> <li>Up to 7 partitions per AZ</li> <li>Used in big data applications (Hadoop, HDFS, HBase, Cassandra, Kafka) <p>If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has capacity for all the requested instances.</p> </li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#elastic-network-interface-eni","title":"Elastic Network Interface (ENI)","text":"<ul> <li>ENI is a virtual network card that gives a private IP to an EC2 instance</li> <li>A primary ENI is created and attached to the instance upon creation and will be deleted automatically upon instance termination.</li> <li>We can create additional ENIs and attach them to an EC2 instance to access it via multiple private IPs.</li> <li>We can detach &amp; attach ENIs across instances</li> <li>ENIs are tied to the subnet (and hence to the AZ)</li> <li>Blog on ENI</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#instance-states","title":"Instance States","text":"<ul> <li>Stop</li> <li>EBS root volume is preserved</li> <li>Terminate</li> <li>EBS root volume gets destroyed</li> <li>Hibernate</li> <li>Hibernation saves the contents from the instance memory (RAM) to the EBS root volume</li> <li>EBS root volume is preserved</li> <li>The instance boots much faster as the OS is not stopped and restarted</li> <li>When you start your instance:<ul> <li>EBS root volume is restored to its previous state</li> <li>RAM contents are reloaded</li> <li>Processes that were previously running on the instance are resumed</li> <li>Previously attached data volumes are reattached and the instance retains its instance ID</li> </ul> </li> <li>Should be used for applications that take a long time to start</li> <li>Not supported for Spot Instances</li> <li>Max hibernation duration = 60 days</li> <li>Standby</li> <li>Instance remains attached to the ASG but is temporarily put out of service (the ASG doesn't replace this instance)</li> <li>Used to install updates or troubleshoot a running instance</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#ec2-nitro","title":"EC2 Nitro","text":"<ul> <li>Newer virtualization technology for EC2 instances</li> <li>Better networking options (enhanced networking, HPC, IPv6)</li> <li>Higher Speed EBS (64,000 EBS IOPS max on Nitro instances whereas 32,000 on non-Nitro)</li> <li>Better underlying security</li> </ul>"},{"location":"solutions-architect-associate/elastic-compute-cloud/#vcpu-threads","title":"vCPU &amp; Threads","text":"<ul> <li>vCPU is the total number of concurrent threads that can be run on an EC2 instance</li> <li>Usually 2 threads per CPU core (eg. 4 CPU cores \u21d2 8 vCPU)</li> </ul>"},{"location":"solutions-architect-associate/elastic-container-registry/","title":"Elastic Container Registry (ECR)","text":"<ul> <li>AWS managed private Docker repository</li> <li>Public repository is also available (Amazon ECR public gallery)</li> <li>Pay for the storage you use to store docker images (no provisioning)</li> <li>Integrated with ECS &amp; IAM for security (permission erros =&gt; policy)</li> <li>Storage backed by S3</li> <li>Can upload Docker images on ECR manually or we can use a CICD service like CodeBuild</li> </ul>"},{"location":"solutions-architect-associate/elastic-container-service/","title":"Elastic Container Service (ECS)","text":"<ul> <li>Used to launch Docker containers on AWS = Launch ECS tasks on ECS clusters</li> <li>Integrates with ALB for load balancing to ECS tasks</li> <li>EFS is used as persistent multi-AZ shared storage for ECS tasks</li> </ul>"},{"location":"solutions-architect-associate/elastic-container-service/#launch-types","title":"Launch Types","text":"<ul> <li>EC2 Launch Type</li> <li>You must provision &amp; maintain EC2 instances (use ASG)</li> <li>Each EC2 instance must contain ECS Agent to register in the ECS cluster</li> <li>AWS take care of starting and stopping containers</li> <li>Containers run on underlying EC2 instances</li> <li>Not Serverless</li> </ul> <ul> <li>Fargate Launch Type</li> <li>Serverless</li> <li>No need to provision infrastructure</li> <li>No need to worry about infrastructure scaling</li> <li>You just create task defenitions</li> <li>ECS launches the required containers based on the CPU / RAM needed (we won\u2019t know where these containers are running)</li> </ul> <p>## IAM Roles for ECS Tasks   - EC2 Instance Profile (EC2 Launch type only)     - Used by the ECS agent to:       - Make API calls to ECS service       - Send container logs to Cloud Watch       - Pull Docker image from ECR       - Reference sensitive data in Secrets Manager or SSM Parameter Store</p> <p></p> <ul> <li>ECS Task Role<ul> <li>Each task can have a specific role</li> <li>Allows ECS tasks to access AWS resources</li> <li>Task Role is defined in the task definition</li> <li>Use different roles for the different ECS Services</li> <li>Use taskRoleArn parameter to assign IAM policies to ECS Task Role</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/elastic-container-service/#load-balancing","title":"Load Balancing","text":"<ul> <li>Application Load Balancer: Supported and works for most uses cases</li> <li>Network Load Balancer: Recommended only for high throughput/high performance use cases, or to pair it with AWS Private Link</li> <li>Elastic Load Balancer: Supported but not recommended (no advanced features, no Fargate support)</li> </ul>"},{"location":"solutions-architect-associate/elastic-container-service/#data-volumes","title":"Data Volumes","text":"<ul> <li>Mount EFS file systems onto ECS tasks</li> <li>Works for both EC2 and Fargate launch type</li> <li>Fargate + EFS = Serverless</li> <li>Persistent multi-AZ shared storage for containers <p>Amazon S3 cannot be mounted as a file system</p> </li> </ul> <p></p>"},{"location":"solutions-architect-associate/elastic-container-service/#auto-scaling","title":"Auto-scaling","text":""},{"location":"solutions-architect-associate/elastic-container-service/#fargate-launch-type","title":"Fargate Launch type","text":"<ul> <li>Automatically increase/decrease the desired number of ECS tasks</li> <li>Uses AWS Application Auto Scaling</li> <li>ECS Service Average CPU utilization</li> <li>ECS Service Average Memory utilization - scale on RAM</li> <li>ALB Request count per target - metric coming from the ALB</li> <li>Types:</li> <li>Target tracking: Scale based on target value for a specific CloudWatch metric</li> <li>Step scaling: Scale based on a specific CloudWatch alarm</li> <li> <p>Scheduled scaling: Scale based on a specific date/time (predictable changes)</p> </li> <li> <p>ECS Service auto scaling (Task level) != EC2 auto scaling (EC2 instance level)</p> </li> </ul>"},{"location":"solutions-architect-associate/elastic-container-service/#ec2-launch-type","title":"EC2 Launch type","text":"<ul> <li>ECS Service scaling by adding underlying EC2 instances</li> <li>Auto scaling Group scaling</li> <li>Scale ASG based on CPU utilization</li> <li>Add EC2 instances over time</li> <li>ECS Cluster Capacity provider</li> <li>Automatically provision and scale the infrastructure for your ECS tasks</li> <li>Paired with ASG</li> <li>Add EC2 instances when missing capacity (CPU, RAM...)</li> </ul>"},{"location":"solutions-architect-associate/elastic-container-service/#ecs-with-eventbridge-eventbridge-schedule-and-sqs","title":"ECS with EventBridge, EventBridge Schedule and SQS","text":"<ul> <li>with EventBridge</li> </ul> <ul> <li>with EventBridge Schedule</li> </ul> <ul> <li>with SQS</li> </ul>"},{"location":"solutions-architect-associate/elastic-file-system/","title":"Elastic File System (EFS)","text":"<ul> <li>AWS managed Network File System (NFS)</li> <li>Can be mounted to multiple EC2 instances across AZs</li> <li>Pay per use (no capacity provisioning)</li> <li>Auto scaling (up to PBs)</li> <li>Compatible with Linux based AMIs (POSIX file system)</li> <li>Uses security group to control access to EFS</li> <li>Uses NFSv4.1 protocol</li> <li>Encryption at rest using KMS</li> <li>Lifecycle management feature to move files to EFS-IA after N days</li> <li>POSIX Permissions to control access from hosts by IAM User or Group</li> <li>EFS port number: 2049 (NFS also)</li> </ul>"},{"location":"solutions-architect-associate/elastic-file-system/#performance-mode","title":"Performance Mode","text":"<ul> <li>General Purpose (default)</li> <li>latency-sensitive use cases (web server, CMS, etc.)</li> <li>for small size files</li> <li>Max I/O</li> <li>higher latency &amp; throughput (big data, media processing)</li> <li>for large size files</li> </ul>"},{"location":"solutions-architect-associate/elastic-file-system/#throughput-mode","title":"Throughput Mode","text":"<ul> <li>Bursting (default)</li> <li>Throughput: 50MB/s per TB</li> <li>Burst of up to 100MB/s</li> <li>Increasing the speed w.r.t the size of data</li> <li>Provisioned</li> <li>Fixed throughput (provisioned)</li> </ul>"},{"location":"solutions-architect-associate/elastic-file-system/#storage-tiers","title":"Storage Tiers","text":"<ul> <li>Standard or Regional (New name) - for frequently accessed files, Keeping the copy of the data in multiple AZ (Highly available, redundant, reliable)</li> <li>One Zone - Keeping the data only on one AZ</li> <li>Infrequent access (EFS-IA) - cost to retrieve files, lower price to store</li> </ul>"},{"location":"solutions-architect-associate/elastic-kubernetes-service/","title":"Elastic Kubernetes Service (EKS)","text":"<ul> <li>Used to launch managed Kubernetes (open-source) clusters on AWS</li> <li>Supports both EC2 (to deploy worker nodes) and Fargate (to deploy serverless containers) launch types</li> <li>Kubernetes is cloud-agnostic (can be used in any cloud - Azure, GCP...)</li> </ul> <ul> <li>Inside the EKS cluster, we have EKS nodes (EC2 instances) and EKS pods (tasks) within them. We can use a private or public load balancer to access these EKS pods. <p>CloudWatch Container Insights can be configured to view metrics and logs for an EKS cluster in the CloudWatch console</p> </li> <li>Use case:</li> <li>If your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes</li> </ul>"},{"location":"solutions-architect-associate/elastic-kubernetes-service/#node-types","title":"Node types","text":"<ul> <li>Managed Node Groups</li> <li>Creates and manages Nodes (EC2 instances) for you</li> <li>Nodes are part of ASG managed by EKS</li> <li>Support On-demand and Spot instances</li> <li>Self-Managed Nodes</li> <li>Nodes are created by you and registered to the EKS cluster and managed by an ASG</li> <li>You can use prebuilt AMI - Amazon EKS optimized AMI</li> <li>Support On-demand and Spot instances</li> <li>AWS Fargate</li> <li>No maintenance required; no nodes managed</li> </ul>"},{"location":"solutions-architect-associate/elastic-kubernetes-service/#data-volumes","title":"Data Volumes","text":"<ul> <li>Need to specify StorageClass manifest on your EKS cluster</li> <li>Leverages a Container Storage Interface (CSI) compliant driver</li> <li>Support for:</li> <li>Amazon EBS</li> <li>Amazon EFS (works with Fargate)</li> <li>Amazon FSx for Lustre</li> <li>Amazon FSx for NetApp ONTAP</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/","title":"Elastic Load Balancer","text":"<ul> <li>Regional Service</li> <li>Managed load balancer</li> <li>Supports Multi AZ</li> <li>Spread load across multiple EC2 instances</li> <li>Separate public traffic from private traffic</li> <li>Expose a single point of access (DNS) to your application</li> <li>Does not support weighted routing</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#health-checks","title":"Health Checks","text":"<ul> <li>Health checks allow ELB to know which instances are working properly (to know if instances it forwards traffic to are available to reply requests)</li> <li>Done on a port and a route, /health is common (Example, Protocol: HTTP    Port:4567   Endpoint: /health )</li> <li>If the response is not 200 (OK), then the instance is unhealthy</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#types-of-elb","title":"Types of ELB","text":"<ul> <li>Classic Load balancer (v1 - old generation) - 2009 - CLB</li> <li>HTTP, HTTPS, TCP, SSL (Secure TCP)</li> <li>Application Load balancer (v2 - new generation) - 2016 - ALB</li> <li>HTTP, HTTPS, WebSocket</li> <li>Network Load balancer (v2 - new generation) - 2017 - NLB</li> <li>TCP, TLS (Secure TCP), UDP</li> <li>Gateway Load balancer - 2020 - GWLB</li> <li>Operates at Layer 3 (Network Layer) - IP Protocol</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#classic-load-balancer-clb-deprecated","title":"Classic Load Balancer (CLB) - deprecated","text":"<ul> <li>The Classic Load Balancer is deprecated at AWS and will soon not be available in the AWS Console.</li> <li>Load Balancing to a single application</li> <li>Supports HTTP, HTTPS (layer 7) &amp; TCP, SSL (layer 3)</li> <li>Health checks are HTTP or TCP based</li> <li>Provides a fixed hostname (xxx.region.elb.amazonaws.com)</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#application-load-balancer-alb","title":"Application Load Balancer (ALB)","text":"<ul> <li>Load balancing to multiple applications (target groups)</li> <li>Operates at Layer 7 (HTTP, HTTPS and WebSocket)</li> <li>Provides a fixed hostname (xxx.region.elb.amazonaws.com)</li> <li>ALB terminates the upstream connection and creates a new downstream connection to the targets (connection termination)</li> <li>Great for micro services &amp; container-based applications (Docker &amp; ECS)</li> <li>Support redirects (From HTTP to HTTPS for example)</li> <li>Has a port mapping feature to redirect a dynamic port in ECS</li> <li>Client info is passed in the request headers</li> <li>Client IP =&gt; X-Forwarded-For</li> <li>Client Port =&gt; X-Forwarded-Port</li> <li>Protocol =&gt; X-Forwarded-Proto</li> <li>Target Groups</li> <li>Health checks are done at the target group level</li> <li>Target Groups could be<ul> <li>EC2 instances - HTTP</li> <li>ECS tasks - HTTP</li> <li>Lambda functions - HTTP request is translated into a JSON event</li> <li>Private IP Addresses</li> </ul> </li> <li>Listener Rules can be configured to route traffic to different target groups based on:</li> <li>Path (example.com/users &amp; example.com/posts)</li> <li>Hostname (one.example.com &amp; other.example.com)</li> <li>Query String (example.com/users?id=123&amp;order=false)</li> <li>Request Headers</li> <li>Source IP address</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#network-load-balancer-nlb","title":"Network Load Balancer (NLB)","text":"<ul> <li>Operates at Layer 4 (TCP, TLS, UDP)</li> <li>Can handle millions of request per seconds (extreme performance)</li> <li>Lower latency ~ 100 ms (vs 400 ms for ALB)</li> <li>1 static public IP per AZ (vs a static hostname for CLB &amp; ALB)</li> <li>Elastic IP can be assigned to NLB (helpful for whitelisting specific IP)</li> <li>Maintains the same connection from client all the way to the target</li> <li>No security groups can be attached to NLBs. They just forward the incoming traffic to the right target group as if those requests were directly coming from client. So, the attached instances must allow TCP traffic on port 80 from anywhere.</li> <li>Within a target group, NLB can send traffic to</li> <li>EC2 instances<ul> <li>If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address</li> </ul> </li> <li>IP addresses<ul> <li>Used when you want to balance load for a physical server having a static IP.</li> </ul> </li> <li>Application Load Balancer (ALB)<ul> <li>Used when you want a static IP provided by an NLB but also want to use the features provided by ALB at the application layer.</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#gateway-load-balancer-gwlb","title":"Gateway Load Balancer (GWLB)","text":"<ul> <li>Operates at layer 3 (Network layer) - IP Protocol</li> <li>Used to route requests to a fleet of 3rd party virtual appliances like Firewalls, Intrusion Detection and Prevention Systems (IDPS), etc.</li> <li>Performs two functions:</li> <li>Transparent Network Gateway (single entry/exit for all traffic)</li> <li>Load Balancer (distributes traffic to virtual appliances)</li> <li>Uses GENEVE protocol (on port 6081)</li> <li>Target groups for GWLB could be</li> <li>EC2 instances</li> <li>IP addresses</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#sticky-sessions-session-affinity","title":"Sticky Sessions (Session Affinity)","text":"<ul> <li>Requests coming from a client is always redirected to the same instance based on a cookie. After the cookie expires, the requests coming from the same user might be redirected to another instance.</li> <li>Only supported by CLB &amp; ALB</li> <li>Used to ensure the user doesn\u2019t loss his session data, like login or cart info, while navigating between web pages.</li> <li>Stickiness may cause load imbalance</li> <li>Cookies could be</li> <li>Application-based (TTL defined by the application)<ul> <li>Custom cookie</li> <li>Generated by the target</li> <li>Can include custom attributes required by the application</li> <li>Cookie name must be specified individually for each target group</li> <li>Don't use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)</li> <li>Application cookie</li> <li>Generated by the load balancer</li> <li>Cookie name is AWSALBAPP</li> </ul> </li> <li>Duration based cookie<ul> <li>Cookie generated by the load balancer</li> <li>Cookie name is AWSALB for ALB, AWSELB for CLB</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#cross-zone-load-balancing","title":"Cross-zone Load Balancing","text":"<ul> <li>Allows ELBs in different AZ containing unbalanced number of instances to distribute the traffic evenly across all instances in all the AZ registered under a load balancer. </li> <li>Supported Load Balancers</li> <li>Classic Load Balancer<ul> <li>Disabled by default</li> <li>No charges for inter AZ data</li> </ul> </li> <li>Application Load Balancer<ul> <li>Always on (can\u2019t be disabled)</li> <li>Can be disabled at target level</li> <li>No charges for inter AZ data</li> </ul> </li> <li>Network Load Balancer &amp; Gateway Load Balancer<ul> <li>Disabled by default</li> <li>Charges for inter AZ data</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#ssl-tls-certificates","title":"SSL &amp; TLS Certificates","text":"<ul> <li>A certificate is used to guarantee trust between two parties during a transaction</li> <li>TLS (Transport Layer Security) certificate ensure that the communication between the user and the server is encrypted and the server is who it says it is.</li> <li>TLS or HTTPS relies on symmetric encryption which is computationally light. But, if the key is sniffed while being sent from the server to the user, it could be used to decrypt the communication between the user and the server. So, we use asymmetric encryption to encrypt &amp; transfer the symmetric key from the user to the server and then use symmetric encryption for future communications.</li> </ul>"},{"location":"solutions-architect-associate/elastic-load-balancer/#load-balancer-ssl-certificates","title":"Load Balancer - SSL Certificates","text":"<ul> <li>The load balancer uses an X.509 certificate (SSL/TLS server certificate)</li> <li>You can manage certificate using ACM (AWS Certificate Manager)</li> <li>You can create and upload your own certificate </li> <li>HTTPS listner:</li> <li>You must specify a default certificate</li> <li>You can add an optional list of certs to support multiple domains</li> <li>Clients can use SNI (Server Name Indication) to specify the hostname they reach</li> </ul>"},{"location":"solutions-architect-associate/elastic-map-reduce/","title":"Elastic Map Reduce (EMR)","text":"<ul> <li>Used to create Hadoop Clusters (Big Data) to analyze and process vast amounts of data</li> <li>The clusters can be made of 100s of EC2 instances</li> <li>Bundled with Apache Spark, HBase, Presto, Flink, etc</li> <li>EMR takes care of all the provisioning and configuration</li> <li>Auto-scaling and Integrated with Spot Instances</li> <li>Can be used to process large amounts of log files</li> </ul>"},{"location":"solutions-architect-associate/elastic-map-reduce/#node-types-purchasing","title":"Node types &amp; Purchasing","text":"<ul> <li>Master Node: Manages the cluster, coordinate, manage health - long running</li> <li>Core Node: Run task and store data - long running</li> <li>Task Node (optional): Just to run task- usually Spot</li> <li>Purchasing options:</li> <li>On-demand: reliable, predictable, won't be terminated</li> <li>Reserved (min 1 year): cost savings (automatically use if available)</li> <li>Spot: cheaper, can be terminated, less reliable</li> <li>Can have long running or transient (temporary) cluster</li> </ul>"},{"location":"solutions-architect-associate/elastic-transcoder/","title":"Elastic Transcoder","text":"<ul> <li>Convert media files stored in S3 into media files in the formats required by consumer playback devices (phones etc)</li> <li>Can handle large volumes of media files and large file sizes</li> <li>Cost effective - duration based pricing model</li> <li>Fully managed</li> </ul>"},{"location":"solutions-architect-associate/elasticache/","title":"ElastiCache","text":"<ul> <li>Regional Service</li> <li>AWS managed caching service</li> <li>In-memory key-value store with sub-millisecond latency</li> <li>Need to provision an underlying EC2 instance</li> <li>Makes the application stateless because it doesn\u2019t have to cache locally</li> <li>Using ElastiCache requires heavy application code changes (setup the application to query the cache before and after querying the database)</li> <li>DB Cache (lazy loading)</li> </ul> <ul> <li>Application queries ElastiCache, if not available, get from RDS and store it in ElastiCache</li> <li>Help relieve load in RDS</li> <li>Cache must have an invalidation strategy to make sure only the most current data is used in there</li> <li>User session store</li> </ul> <ul> <li>User logs into any of the application</li> <li>The application writes the session data into ElastiCache</li> <li>The user hits another instance of our application</li> <li>The instance retrieves the data and the user is already logged in</li> <li>store user's session data like cart info</li> </ul>"},{"location":"solutions-architect-associate/elasticache/#redis-vs-memcached","title":"Redis vs Memcached","text":"Redis Memcached In-memory data store Distributed memory object cache Read Replicas (for scaling reads &amp; HA) No replication Backup &amp; restore No backup &amp; restore Single-threaded Multi-threaded HIPAA compliant Not HIPAA compliant Data is stored in an in-memory DB which is replicated Data is partitioned across multiple nodes (sharding) Redis Sorted Sets are used in realtime Gaming Leaderboards Good for auto-completion Multi-AZ support with automatic failover (disaster recovery)"},{"location":"solutions-architect-associate/elasticache/#security-access-management","title":"Security &amp; Access Management","text":"<ul> <li>Network security is managed using Security Groups (only allow EC2 security group for incoming requests)</li> <li>At rest encryption using KMS</li> <li>In-flight encryption using SSL</li> <li>Use Redis Auth to authenticate to ElastiCache for Redis</li> <li>Memcached supports SASL-based authentication</li> </ul>"},{"location":"solutions-architect-associate/encryption/","title":"Encryption","text":""},{"location":"solutions-architect-associate/encryption/#encryption-in-flight-ssl","title":"Encryption in flight (SSL)","text":"<ul> <li>Data is encrypted before sending and decrypted after receiving</li> <li>SSL certificates help with encryption (HTTPS)</li> <li>Encryption in flight ensures no MITM (man in the middle attack) can happen</li> </ul>"},{"location":"solutions-architect-associate/encryption/#server-side-encryption-at-rest","title":"Server side encryption at rest","text":"<ul> <li>Data is encrypted after being received by the server</li> <li>Data is decrypted before being sent</li> <li>It is stored in an encrypted form thanks to a key (usually a data key)</li> <li>The encryption/decryption keys must be managed by somewhere (like KMS) and the server must have access to it</li> </ul>"},{"location":"solutions-architect-associate/encryption/#client-side-encryption","title":"Client side encryption","text":"<ul> <li>Data is encrypted by the client and never decrypted by the server</li> <li>Data will be decrypted by the receiving client</li> <li>The server should not be able to decrypt the data</li> <li>Could leverage Envelope Encryption</li> </ul> <p>Documentaion</p>"},{"location":"solutions-architect-associate/eventbridge/","title":"EventBridge","text":"<ul> <li>Formerly CloudWatch Events</li> <li>Schedule: Cron jobs</li> <li>Ex: Schedule to trigger a script on Lambda function every hour</li> <li>Event pattern: Event rules to react to a service doing something</li> <li>Ex: Sent a SNS topic with Email notification whenever there occur a root user login</li> <li>Event Rules: how to process the events</li> </ul> <ul> <li>Event buses types:</li> <li>Default event bus: events from AWS services are sent to this</li> <li>Partner event bus: receive events from external SaaS applications</li> <li>Custom Event bus: for your own applications</li> <li>Event buses support cross-account access using Resource-based policies</li> <li>You can archive events sent to an event bus (indefinitely or set period)</li> <li>Can replay archived events</li> </ul>"},{"location":"solutions-architect-associate/eventbridge/#schema-registry","title":"Schema Registry","text":"<ul> <li>Defines how the data is structured in the event bus</li> <li>Schema can be versioned</li> </ul>"},{"location":"solutions-architect-associate/eventbridge/#resource-based-policy","title":"Resource-based policy","text":"<ul> <li>Manage permissions for a specific Event bus</li> <li>Ex: allow/deny events from another AWS account or AWS region</li> <li>Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region</li> </ul>"},{"location":"solutions-architect-associate/firewall-manager/","title":"Firewall Manager","text":"<ul> <li>Manage firewall rules across all the accounts of an AWS Organization </li> <li>Common set of security rules:</li> <li>WAF rules (ALB, API Gateways, CloudFront)</li> <li>AWS Shield Advanced (ALB, CLB, NLB, Elastic IP, CloudFront)</li> <li>Security Groups for EC2, ALB, and ENI resources in VPC</li> <li>AWS Network Firewall (VPC Level)</li> <li>Amazon Route 53 Resolver DNS Firewall</li> <li>Policies are created at Region level <p>Does not support NACL as of now</p> </li> </ul>"},{"location":"solutions-architect-associate/firewall-manager/#waf-vs-firewall-manager-vs-shield","title":"WAF vs Firewall Manager vs Shield","text":"<ul> <li>WAF, Shield, and Firewall Manager are used together for comprehensive protection</li> <li>For granular protection of your choice, WAF alone is the correct choice</li> <li>If you want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF</li> <li>Shield advanced adds additional features on top of AWS WAF, such as dedicated support from Shield Response Team (SRT) and advanced reporting</li> </ul>"},{"location":"solutions-architect-associate/forecast/","title":"Amazon Forecast","text":"<ul> <li>Fully managed service that uses ML to deliver highly accurate forecasts</li> <li>50% more accurate than looking at the data itself</li> <li>Reduce forecasting time from months to hours</li> </ul> <p>Use cases - Retail demand planning \u2013 Predict product demand, allowing you to more accurately vary inventory and pricing at different store locations. - Supply chain planning \u2013 Predict the quantity of raw goods, services, or other inputs required by manufacturing. - Resource planning \u2013 Predict requirements for staffing, advertising, energy consumption, and server capacity. - Operational planning \u2013 Predict levels of web traffic, AWS usage, and IoT sensor usage.</p> <p>How it works</p> <p></p> <p>Documentation</p>"},{"location":"solutions-architect-associate/fsx/","title":"FSx","text":"<ul> <li>Allows us to launch 3rd party high-performance file systems on AWS</li> <li>Useful when we don\u2019t want to use an AWS managed file system like S3</li> <li>Can be accessed from your on-premise infrastructure (VPN or Direct Connect)</li> </ul>"},{"location":"solutions-architect-associate/fsx/#fsx-for-windows","title":"FSx for Windows","text":"<ul> <li>Shared File System for Windows (like EFS but for Windows)</li> <li>Supports SMB protocol, Windows NTFS, Microsoft Active Directory integration, ACLs, user quotas, Microsoft Distributed Filesystem (DFS), Namespaces</li> <li>Built on SSD, scale up to 10s of GB/s, millions of IOPS, 100s PB of data</li> <li>Supports Multi-AZ (high availability)</li> <li>Data is backed-up daily to S3</li> <li>Does not integrate with S3 (cannot store cold data)</li> <li>Can be mounted on Linux EC2 instances</li> </ul>"},{"location":"solutions-architect-associate/fsx/#fsx-for-lustre","title":"FSx for Lustre","text":"<ul> <li>Parallel distributed file system for HPC (like EFS but for HPC)</li> <li>Scales up to 100s GB/s, millions of IOPS, sub-ms latencies</li> <li>Only works with Linux</li> <li>Seamless integration with S3</li> <li>Can read S3 buckets as a file system (through FSx)</li> <li>Can write the output back to S3 (through FSx)</li> </ul>"},{"location":"solutions-architect-associate/fsx/#fsx-deployment-options","title":"FSx Deployment Options","text":"<ul> <li>Scratch File System</li> <li>Temporary storage (cheaper)</li> <li>Data is not replicated (data lost if the file server fails)</li> <li>High burst (6x faster than persistent file system)</li> <li>Usage: short-term processing</li> <li>Persistent File System</li> <li>Long-term storage (expensive)</li> <li>Data is replicated within same AZ</li> <li>Failed files are replaced within minutes</li> <li>Usage: long-term processing, sensitive data</li> </ul>"},{"location":"solutions-architect-associate/fsx/#fsx-for-netapp-ontap","title":"FSx for NetApp ONTAP","text":"<ul> <li>Compatible with NFS, SMB, iSCSI protocol</li> <li>Works with</li> <li>Linux</li> <li>Windows</li> <li>MacOS</li> <li>VMWare cloud on aws</li> <li>Amazon Workspaces and App Stream 2.0</li> <li>Amazon EC2, ECS, and EKS</li> <li>Storage shrinks or grows automatically</li> <li>Snapshots, replication, low-cost, compression, and data de-duplication</li> <li>Point-in-time instantaneous cloning</li> </ul>"},{"location":"solutions-architect-associate/fsx/#fsx-for-openzfs","title":"FSx for OpenZFS","text":"<ul> <li>Compatible with NFS(v3, v4, v4.1, v4.2)</li> <li>Works with</li> <li>Linux</li> <li>Windows</li> <li>MacOS</li> <li>VMWare cloud on aws</li> <li>Amazon Workspaces and App Stream 2.0</li> <li>Amazon EC2, ECS, and EKS</li> <li>Upto 1,000,000 IOPS with &lt; 0.5ms latency</li> <li>Snapshots, compression and low-cost</li> <li>Point-in-time instantaneous cloning</li> </ul>"},{"location":"solutions-architect-associate/global-accelerator/","title":"Global Accelerator","text":"<ul> <li>Global service</li> <li>Improves availability of the application for global users</li> <li>Leverages the private AWS network to route requests to the application (faster)</li> <li>Supports globally distributed application endpoints</li> <li>Does not cache anything at the edge location</li> <li>Endpoint could be public or private (could span multiple regions)</li> <li>Elastic IP</li> <li>EC2 instances</li> <li>ALB </li> <li>NLB</li> <li>Not affected by client's DNS caching because the 2 anycast IPs are static (traffic dials and endpoint weights changes are effective within seconds)</li> <li>Good for</li> <li>non-HTTP use cases:<ul> <li>Gaming (UDP)</li> <li>IoT (MQTT)</li> <li>Voice over IP (VoIP)</li> </ul> </li> <li>HTTP use cases that require static IP addresses or fast regional failover</li> </ul>"},{"location":"solutions-architect-associate/global-accelerator/#working","title":"Working","text":"<ul> <li>2 anycast public IPs (static) are created for your application globally. Requests from clients hitting these IPs will automatically be routed to the nearest edge location. The Edge locations send the traffic to your application through the private AWS network.</li> <li>Traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed)</li> <li>Endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group</li> </ul>"},{"location":"solutions-architect-associate/global-accelerator/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>Global Accelerator performs health checks for the application</li> <li>Failover in less than 1 minute for unhealthy endpoints</li> </ul>"},{"location":"solutions-architect-associate/global-accelerator/#security","title":"Security","text":"<ul> <li>Only 2 static IP need to be whitelisted by the clients</li> <li>Can be integrated with AWS Shield for DDoS protection</li> </ul>"},{"location":"solutions-architect-associate/glue/","title":"Glue","text":"<ul> <li>Managed extract, transform, and load (ETL) service</li> <li>Fully serverless service</li> <li>Used to prepare &amp; transform data for analytics</li> </ul> <ul> <li>Used to get data from a store, process and put it in another store (could be the same store) <p>Glue ETL job can write the transformed data using a compressed file format to save storage cost</p> </li> </ul>"},{"location":"solutions-architect-associate/glue/#convert-data-into-parquet-format","title":"Convert data into Parquet format","text":"<ul> <li>Parquet format is of Columnar storage data</li> </ul> <ul> <li>The process of conversion can be automated using Lambda function or EventBridge, so whenever a data is inserted to S3 then Glue ETL job will be triggered</li> </ul>"},{"location":"solutions-architect-associate/glue/#glue-data-catalog","title":"Glue Data Catalog","text":"<ul> <li>Glue Data Crawlers crawl databases and collect metadata which is populated in Glue Data Catalog</li> <li>Metadata can be used for analytics</li> </ul>"},{"location":"solutions-architect-associate/glue/#things-to-know-at-high-level","title":"Things to know at high-level","text":"<ul> <li>Glue Job Bookmarks: Prevent re-processing of old data</li> <li>Glue Elastic Views: </li> <li>Combine and replicate data across multiple data stores using SQL</li> <li>No custom code, Glue monitors for changes in source data, serverless</li> <li>Leverages a \"virtual table\" </li> <li>Glue DataBrew: Clean and normalize data using pre-built tranformation</li> <li>Glue Studio: new GUI to create, run and monitor ETL jobs in Glue</li> <li>Glue Streaming ETL (built on Apache Spark structured streaming): compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)</li> </ul>"},{"location":"solutions-architect-associate/guardduty/","title":"Amazon GuardDuty","text":"<ul> <li>Intelligent threat discovery using ML to protect AWS account</li> <li>No management required (just enable)</li> <li>Input data includes:</li> <li>CloudTrail Logs (unusual API calls, unauthorized deployments)</li> <li>VPC Flow Logs (unusual internal traffic, unusual IP address)</li> <li>DNS Logs (compromised EC2 instances sending encoded data within DNS queries)</li> <li>EKS Audit Logs (suspicious activities and potential EKS cluster compromises)</li> <li>Setup EventBridge rules to target AWS Lambda or SNS for automation</li> <li>Can protect against CryptoCurrency attacks (has a dedicated \"finding\" for it)</li> <li>Disabling GuardDuty will delete all remaining data, including your findings and configurations</li> </ul>"},{"location":"solutions-architect-associate/high-performance-computing/","title":"High Performance Computing (HPC) on AWS","text":"<ul> <li>Cloud is perfect for HPC</li> <li>Cluster placement group for low latency inter-nodal communication</li> <li>EC2 Enhanced Networking (SR-IOV)</li> <li>Elastic Network Adapter (ENA)<ul> <li>Supported in both Linux &amp; Windows</li> </ul> </li> <li>Elastic Fabric Adapter (EFA)<ul> <li>Enhanced for HPC</li> <li>Supported in Linux only</li> <li>Leverages Message Passing Interface (MPI) standard</li> <li>Bypasses the underlying Linux OS to provide low-latency networking</li> </ul> </li> <li>AWS Batch</li> <li> <p>Used to run single jobs that span multiple EC2 instances (multi-node)</p> </li> <li> <p>AWS Parallel Cluster</p> </li> <li>Open-source cluster management tool to deploy HPC on AWS</li> <li>Configure with text files</li> <li>Automate creation of VPC, Subnet, cluster type and instance types</li> <li>Ability to enable EFA on the cluster</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/","title":"Identity &amp; Access Management (IAM)","text":"<ul> <li>Global Service (IAM entities like roles can be used in any region without recreation)</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#users-groups","title":"Users &amp; Groups","text":"<ul> <li>Users are people in your organization and can be grouped</li> <li>Groups are collections of users and have policies attached to them</li> <li>Groups cannot be nested</li> <li>User can belong to multiple groups</li> <li>User doesn't have to belong to a group</li> <li>Root User has full access to the account</li> <li>IAM User has limited permission to the account</li> <li>You should log in as an IAM user with admin access even if you have root access. This is just to be sure that nothing goes wrong by accident. <ul> <li>An IAM Group is not an identity and cannot be identified as a principal in an IAM policy</li> <li>Only users and services can assume a role (not groups)</li> <li>A new IAM user created using the AWS CLI or AWS API has no AWS credentials</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#policies","title":"Policies","text":"<ul> <li>Policies are JSON documents that outline permissions for users, groups or roles</li> <li>Policies assigned to a user are called inline policies</li> <li>Follow least privilege principle for IAM Policies</li> <li>Two types</li> <li>User based policies<ul> <li>IAM policies define which API calls should be allowed for a specific user</li> </ul> </li> <li>Resource based policies<ul> <li>Control access to an AWS resource</li> <li>Grant the specified principal permission to perform actions on the resource and define under what conditions this applies</li> </ul> </li> <li>An IAM principal can access a resource if the user policy ALLOWS it OR the resource policy ALLOWS it AND there\u2019s no explicit DENY.</li> <li>Policy Simulator</li> <li>Online tool that allows us to check what API calls an IAM User, Group or Role is allowed to perform based on the permissions they have.</li> <li>Policy Structure</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#trust-policies","title":"Trust Policies","text":"<ul> <li>Defines which principal entities (accounts, users, roles, federated users) can assume the role</li> <li>An IAM role is both an identity and a resource that supports resource-based policies.</li> <li>You must attach both a trust policy and an identity-based policy to an IAM role.</li> <li>The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#multi-factor-authentication-mfa","title":"Multi Factor Authentication (MFA)","text":"<ul> <li>We can set a password policy in AWS account and it can be used to enforce standards for password and to prevent brute force attack</li> <li>password rotation</li> <li> <p>password reuse</p> </li> <li> <p>MFA = password you know + security device you own</p> </li> <li>Main benefit of MFA is, even if password is stolen or hacked then the account will not be compromised</li> <li>MFA device options in AWS</li> <li>Virtual MFA device<ul> <li>Google Authenticator (phone only)                                             </li> <li>Authy (multi-device) </li> <li>Both support multiple tokens on a single device </li> </ul> </li> <li>Universal 2nd Factor (U2F) Security Key<ul> <li>YubiKey by Yubico (3rd party)</li> <li>Support multiple root and IAM users using a single security key</li> </ul> </li> <li>Hardware Key Fob MFA device<ul> <li>Gemalto (3rd party)</li> </ul> </li> <li>Hardware Key Fob MFA device for AWS GovCloud (US)<ul> <li>SurePassID (3rd party)</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#access-keys-cli-sdk","title":"Access Keys, CLI &amp; SDK","text":""},{"location":"solutions-architect-associate/identity-access-management/#aws-access-key","title":"AWS Access Key","text":"<ul> <li>Need to use access keys for AWS CLI and SDK</li> <li>Don't share access keys with anyone (every user can generate their own access keys)</li> <li>Access keys are only shown once and if you lose them you need to generate a new access key</li> <li>Access Key ID ~ username</li> <li>Secret Access Key ~ password</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#aws-cli","title":"AWS CLI","text":"<ul> <li>Open-source tool to interact with AWS services using commands</li> <li>Direct access to public APIs of AWS services</li> <li>The permissions will be same as of permissions set in AWS Console for the user</li> <li>Ex: $ aws iam list-users </li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#aws-sdk","title":"AWS SDK","text":"<ul> <li>Language specific APIs to access and manage AWS services programmatically</li> <li>Supports</li> <li>SDKs (JavaScript, Python, PHP, .NET, Java, Ruby, Go, Node.js, C++)</li> <li>Mobile SDKs (Android, iOS)</li> <li>IoT Device SDKs (Embedded C, Arduino)</li> <li>AWS CLI is built on AWS SDK for Python called Boto</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#aws-cloudshell","title":"AWS CloudShell","text":"<ul> <li>Browser based shell that gives command line access to AWS services.</li> <li>AWS CLI, Python, Node.js and more are pre-installed</li> <li>1 GB of storage free per AWS region</li> <li>Files saved in your home directory are available in future sessions for the same AWS region</li> <li>The default region will be the one from which you've accessed CloudShell</li> <li>Can download and upload files</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#roles","title":"Roles","text":"<ul> <li>Collection of policies for AWS services <p>If you are going to use an IAM Service Role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile (An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts). When you create an IAM service role for EC2, the role automatically has EC2 identified as a trusted entity.</p> </li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#reporting-tools","title":"Reporting Tools","text":"<ul> <li>Credentials Report</li> <li>lists all the users and the status of their credentials (MFA, password rotation, etc.)</li> <li>account level </li> <li>used to audit security for all the users</li> <li>Access Advisor</li> <li>shows the service permissions granted to a user and when those services were last accessed</li> <li>user-level</li> <li>used to revise policies for a specific user</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#guidelines","title":"Guidelines","text":"<ul> <li>Use root account only for account setup</li> <li>1 physical user = 1 AWS user</li> <li>Enforce MFA for both root and IAM users</li> <li>Never share IAM credentials &amp; Access Keys</li> <li>Create a strong password policy</li> <li>Create and use Roles for giving permissions to AWS services</li> <li>Use Access Keys for programmatic access </li> <li>Assign users to groups and assign permission to groups</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#permission-boundaries","title":"Permission Boundaries","text":"<ul> <li>Set the maximum permissions an IAM entity can get</li> <li>Can be applied to users and roles (not groups)</li> <li>Used to ensure some users can\u2019t escalate their privileges (make themselves admin)</li> <li>Ex: suppose a user has \"AdministratorAccess\" policy attached and Permissions boundary is set to \"AmazonS3FullAccess\", then the user can only access S3.</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#assume-role-vs-resource-based-policy","title":"Assume Role vs Resource-based Policy","text":"<ul> <li>When you assume an IAM Role, you give up your original permissions and take the permissions assigned to the role</li> <li>When using a resource based policy, the principal doesn\u2019t have to give up their permissions</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#aws-organizations","title":"AWS Organizations","text":"<ul> <li>Global service</li> <li>Allows to manage multiple accounts</li> <li>The main account is the management account and other accounts are member accounts</li> <li>Member accounts can only be part of one organization</li> <li>Consolidated billing across all accounts - single payment method</li> <li>Pricing benefit from aggregated usage (volume discount on EC2, S3...)</li> <li>Shared reserved instances and Savings plans discounts across accounts</li> <li>API is available to automate AWS account creation</li> </ul> <ul> <li>Advantages</li> <li>Use tagging standards for billing purposes</li> <li>Enable CloudTrail on all accounts, send logs to central S3 account</li> <li>Send CloudWatch logs to central logging account</li> <li> <p>Establish Cross account roles for admin purposes</p> </li> <li> <p>Organizational Unit (OU)</p> </li> </ul> <p></p> <ul> <li>Security: Service Control Policies (SCP)</li> <li>IAM policies applied to OU or account to restrict Users and Roles</li> <li>They do not apply to management account (full admin power)</li> <li> <p>Must have an explicit allow</p> </li> <li> <p>SCP Hierarchy</p> </li> </ul> <p></p> <ul> <li>Management account<ul> <li>Can do anything (no SCP apply)</li> </ul> </li> <li>Account A<ul> <li>Can do anything</li> <li>EXCEPT access Redshift (Explicit deny from OU)</li> </ul> </li> <li>Account B<ul> <li>Can do anything</li> <li>EXCEPT access Redshift (Explicit deny from Prod OU)</li> <li>EXCEPT access Lambda (Explicit deny from HR OU)</li> </ul> </li> <li>Account C<ul> <li>Can do anything</li> <li>EXCEPT access Redshift (Explicit deny from Prod OU)</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#iam-conditions","title":"IAM Conditions","text":"<ul> <li>aws:SourceIp: restrict the client IP from which the API calls are being made</li> <li>aws:RequestedRegion: restrict the region the API calls are made to</li> </ul> <ul> <li>ec2:ResourceTag: restrict based on tags</li> <li>aws:MultiFactorAuthPresent: to force MFA</li> </ul> <ul> <li>IAM for S3</li> <li>s3:ListBucket permission applies to arn:aws:s3:::test =&gt; Bucket level permission</li> <li>s3:GetObject, s3:PutObject, s3:DeleteObject applies to arn:aws:s3:::test/* =&gt; Object level permission</li> </ul> <ul> <li>aws:PrincipalOrgID: Can be used in any resource policies to restrict access to accounts that are member of an AWS Organization</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#iam-roles-vs-resource-based-policy","title":"IAM Roles vs Resource based policy","text":"<ul> <li>Cross account:</li> <li>attaching a resource based policy to a resource (ex: S3 bucket policy)</li> <li>OR using a Role as a proxy</li> <li>When you assume a role (user, application, or service), you give up your original permissions and take the permissions assigned to the role</li> <li>When using resource-based policy, the principal doesn't have to give up his permissions</li> <li>Amazon EventBridge - Security</li> <li>When a rule runs, it needs permissions on target</li> <li>Resource based policy: Lambda, SNS, SQS, CloudWatch, Logs, API Gateway...</li> <li>IAM Role: Kinesis Stream, Systems Manager run command, ECS task...</li> </ul>"},{"location":"solutions-architect-associate/identity-access-management/#aws-iam-identity-center","title":"AWS IAM Identity Center","text":"<ul> <li>Successor to AWS Single Sign-On</li> <li>One login for all your </li> <li>AWS accounts in AWS Organization</li> <li>Business cloud applications (ex; salesforce, Box, Microsoft 365,...)</li> <li>SAML2.0-enabled applications</li> <li>EC2 Windows instances</li> </ul> <ul> <li>Multi-Account permissions</li> <li>Manage access across AWS accounts in your AWS Organization</li> <li>Permission sets: a collection of one or more IAM policies assigned to users and groups to define AWS access</li> <li>Application assignments</li> <li>SSO access to many SAML 2.0 business applications (ex; salesforce, Box, Microsoft 365,...)</li> <li>Provide required URLs, certificates, and metadata</li> <li>Attribute-based Access Control (ABAC)</li> <li>Fine-grained permissions based on user's attributes stored in IAM Identity Center Identity Store</li> <li>Ex: cost center, title, locale,...</li> <li>Use case: Define permission once, then modify AWS access by changing the attributes</li> </ul>"},{"location":"solutions-architect-associate/inspector/","title":"Amazon Inspector","text":"<ul> <li>Detect vulnerabilities in</li> <li>EC2 instances using System Manager (SSM) Agent running on EC2 instances</li> <li>Amazon ECR - Assessment of containers as they are pushed to ECR</li> <li>Lambda functions - Identifies software vulnerabilities in function code and package dependencies</li> <li>Integration with AWS Security Hub</li> <li>Send findings to EventBridge </li> <li>Gives a risk score associated with all vulnerabilities for prioritization</li> <li>Detects vulnerabilities which could cause threats (detected by GuardDuty)</li> </ul>"},{"location":"solutions-architect-associate/instance-store/","title":"Instance Store","text":"<ul> <li>Hardware storage directly attached to EC2 instance (cannot be detached and attached to another instance)</li> <li>Highest IOPS of any available storage (millions of IOPS)</li> <li>Ephemeral storage (loses data when the instance is stopped, hibernated or terminated)</li> <li>Good for buffer / cache / scratch data / temporary content</li> <li>AMI created from an instance does not have its instance store volume preserved</li> <li>Risk of data loss if hardware fails</li> <li>Backup and Replication are your responsibility</li> </ul> <p>You can specify the instance store volumes only when you launch an instance. You can\u2019t attach instance store volumes to an instance after you\u2019ve launched it.</p>"},{"location":"solutions-architect-associate/intoduction-to-messaging/","title":"Introduction to Messaging","text":"<ul> <li>Deploying multiple applications need to communicate with each other</li> <li>Two patterns of application communication:</li> <li>Synchronous communication (application to application)</li> <li>Asynchronous/Event based communication (application to que to application)</li> <li>Synchronous between applications can be problematic if there are sudden spikes of traffic, in that case it's better to decouple your applications using:</li> <li>SQS: queue model</li> <li>SNS: pub/sub model</li> <li>Kinesis: real-time streaming model</li> <li>These services can scale independently from our application</li> </ul>"},{"location":"solutions-architect-associate/intro-to-aws/","title":"Region","text":"<ul> <li>Cluster of Availability Zones (usually 3, min 2, max 6) within a geographic region</li> <li>Ex: us-east-1, ap-south-1</li> <li>The regions are interconnected each other using AWS private network</li> <li>Some AWS Service like Route53 is a Global service and some like EC2 are region specific</li> <li>AWS Global Infrastructure</li> <li>Consideration when selecting a region</li> <li>Compliance (some countries require the data to be present in a data centre present in that country by law)</li> <li>Proximity to customers: reduced latency</li> <li>Available services within a Region: new services and new features aren\u2019t available in every Region</li> <li>Pricing: pricing varies region to region and is transparent in the service pricing page</li> </ul>"},{"location":"solutions-architect-associate/intro-to-aws/#availability-zone-az","title":"Availability Zone (AZ)","text":"<ul> <li>Each AZ is one or more discrete data centers with redundant power, networking, and connectivity</li> <li>Ex: us-east-1a, us-east-1b &amp; us-east-1c</li> <li>AZs are separated from each other (isolated from disasters)</li> <li>They\u2019re connected with high bandwidth, ultra-low latency networking <p>AZ name (eg. us-east-1a) is linked to an AWS account. Same AZ name for two AWS accounts might not refer to the same physical AZ. Use AZ ID (unique ID for each AZ) to coordinate AZ across accounts.</p> </li> </ul>"},{"location":"solutions-architect-associate/intro-to-aws/#aws-points-of-presence-edge-locations","title":"AWS Points of Presence (Edge Locations)","text":"<ul> <li>Content is delivered to end users with low latency</li> </ul>"},{"location":"solutions-architect-associate/kendra/","title":"Amazon Kendra","text":"<ul> <li>Fully managed document search service provided by ML</li> <li>Extract answers from within a document(text,pdf,html,powerpoint...)</li> <li>Natural language seach capabilities</li> <li>Learn from user interactions/feedback to promote preferred results (Incremental Leaning)</li> <li>Ability to manually fine-tune search results (Importance of data, freshness, custom,...)</li> </ul> <p>Documentation</p>"},{"location":"solutions-architect-associate/kinesis/","title":"Kinesis","text":"<ul> <li>Makes it easy to collect, process and analyze streaming data in real time</li> <li>Ingest real-time data such as application logs, metrics, website clickstreams, IoT telemetry data...</li> </ul>"},{"location":"solutions-architect-associate/kinesis/#kinesis-data-stream-kds","title":"Kinesis Data Stream (KDS)","text":"<ul> <li>Real-time data streaming service</li> <li>Used to ingest data in real time directly from source</li> <li>Throughput</li> <li>Publishing: 1MB/sec per shard or 1000 msg/sec per shard</li> <li>Consuming:<ul> <li>2MB/sec per shard (throughput shared between all consumers)</li> <li>Enhanced Fanout: 2MB/sec per shard per consumer (dedicated throughput for each consumer)</li> </ul> </li> <li>Throughput scales with shards (manual scaling)</li> <li>Not Serverless</li> <li>Billing per shard (provisioned)</li> <li>Data Retention: 1 day (default) to 365 days</li> <li>A record consists of a partition key (used to partition data coming from multiple publishers) and data blob (max 1MB)</li> <li>Records will be ordered in each shard</li> <li>Producers use SDK, Kinesis Producer Library (KPL) or Kinesis Agent to publish records</li> <li>Consumers use SDK or Kinesis Client Library (KCL) to consume the records</li> <li>Once data is inserted in Kinesis, it can\u2019t be modified or deleted (immutability)</li> <li>Ability to reprocess (replay) data</li> </ul>"},{"location":"solutions-architect-associate/kinesis/#kinesis-data-firehose-kdf","title":"Kinesis Data Firehose (KDF)","text":"<ul> <li>Used to load streaming data into a target location</li> <li>Writes data in batches efficiently (near real time)</li> <li>Can ingest data in real time directly from source</li> <li>Greater the batch size, higher the write efficiency</li> <li>Auto-scaling</li> <li>Serverless</li> <li>Destinations:</li> <li>AWS: Redshift, S3, ElasticSearch</li> <li>3rd party: Splunk, MongoDB, DataDog, NewRelic, etc.</li> <li>Custom: send to any HTTP endpoint</li> <li>Pay for data going through Firehose (no provisioning)</li> <li>Supports custom data transformation using Lambda functions</li> <li>Max record size: 1MB</li> <li>No replay capability</li> </ul>"},{"location":"solutions-architect-associate/kinesis/#kinesis-data-analytics-kda","title":"Kinesis Data Analytics (KDA)","text":""},{"location":"solutions-architect-associate/kinesis/#for-sql-application","title":"For SQL Application","text":"<ul> <li>Real time analytics on Kinesis Data Streams &amp; Firehouse using SQL</li> <li>Can add referance data from S3 to enrich streaming data</li> <li>Fully managed, serverless</li> <li>Pay for the data processed (no provisioning)</li> <li>Use cases:</li> <li>Time-series analytics</li> <li>Real-time dashboards</li> <li>Real-time metrics</li> </ul>"},{"location":"solutions-architect-associate/kinesis/#for-apache-flink","title":"For Apache Flink","text":"<ul> <li>Use Flink (Java, Scala, or SQL) to process and analyze streaming data</li> <li>Read from Kinesis Data Streams and Amazon MSK</li> </ul> <ul> <li>Run any Apache Flink application on a managed cluster on AWS</li> </ul>"},{"location":"solutions-architect-associate/kms/","title":"AWS KMS (Key Management Service)","text":"<ul> <li>Regional service (keys are bound to a region)</li> <li>Encrypt up to 4KB of data per call (if data &gt; 4 KB, use envelope encryption)</li> <li>Anytime you hear \"encryption\" for an AWS service, it's most likely KMS</li> <li>AWS manages encryption keys for us</li> <li>Fully integrated with IAM for authorization</li> <li>Easy way to control access to your data</li> <li>Able to audit KMS key usage using CloudTrail</li> <li>Need to set IAM Policy &amp; Key Policy to allow a user or role to access a KMS key</li> <li>Seamlessly integrated into most AWS services (RDS, EBS, S3...)</li> <li>Never ever store your secret in plain text, especially in your code!</li> <li>KMS key encryption is also available thrrough API calls (SDK/CLI)</li> <li>Encrypted secrets can be stored on the code / environment variables</li> </ul>"},{"location":"solutions-architect-associate/kms/#kms-keys-types","title":"KMS Keys Types","text":"<ul> <li>KMS Keys is the new name of KMS Customer Master Key</li> <li>Symmetric (AES-256 keys)</li> <li>Single encryption key that is used to encrypt and decrypt</li> <li>necessary for envelope encryption</li> <li>AWS services that are integrated with KMS use Symmetric CMKs</li> <li>You never get access to KMS Key unencrypted (must call KMS API to use)</li> <li>Asymmetric (RSA &amp;&amp; ECC key pairs)</li> <li>Public (Encrypt) and Private (Decrypt) key pairs</li> <li>Used for Encrypt/Decrypt, or Sign/Verify operations</li> <li>The public key is downloadable but you can't access the private key unencrypted</li> <li>Not eligible for automatic rotation (use manual rotation)</li> <li>No need to call the KMS API to encrypt data (data can be encrypted by the client)</li> <li> <p>Use case: encryption outside of AWS by users who can't call the KMS API</p> </li> <li> <p>Three types of KMS Keys:</p> </li> <li>AWS Managed key: free (aws/service-name, example: aws/rds or aws/ebs)<ul> <li>Default KMS key for each supported service</li> <li>Fully managed by AWS (cannot view, rotate or delete them)</li> </ul> </li> <li>Customer Managed Keys (CMK) created in KMS: $1/month<ul> <li>Option to enable automatic yearly rotation</li> </ul> </li> <li>Customer Managed Keys imported (must be 256-bit symmetric key): $1/month<ul> <li>Generated and imported from outside</li> <li>Not recommended</li> </ul> </li> <li>Deletion has a waiting period (pending deletion state) between 7 - 30 days (default 30 days). The key can be recovered during the pending deletion state.</li> <li>In addition pay for API calls to KMS ($0.03/10000 calls)</li> <li>Automatic key rotation</li> <li>AWS managed KMS key: automatic every 1 year</li> <li>Customer managed KMS key: (must be enabled) automatic every 1 year</li> <li>Imported KMS key: only manual rotation is possible using alias</li> </ul>"},{"location":"solutions-architect-associate/kms/#kms-key-policies","title":"KMS Key Policies","text":"<ul> <li>Control access to KMS keys, \"similar\" to S3 bucket policies</li> <li>Difference: you can't control access without them</li> <li>Default KMS key policy:</li> <li>Created if you don't provide a specific KMS key policy</li> <li>Complete access to the key to the root user = entire AWs account</li> <li>Custom KMS Key Policy:</li> <li>Define users, roles that can access the KMS key</li> <li>Define who can administer the key</li> <li>Useful for cross-account access of your KMS key</li> </ul>"},{"location":"solutions-architect-associate/kms/#copying-snapshots-across-accounts","title":"Copying snapshots across accounts","text":"<ol> <li>Create a snapshot, encrypted with your own KMS key (Customer Managed Key)</li> <li>Attach a KMS key policy to authorize cross-account access</li> <li>Share the encrypted snapshot</li> <li>(in target) Create a copy of the snapshot, encrypt it with a CMK in your account</li> <li>Create a volume from the snapshot</li> </ol>"},{"location":"solutions-architect-associate/kms/#cross-region-encrypted-snapshot-migration","title":"Cross-region Encrypted Snapshot Migration","text":"<ul> <li>Copy the snapshot to another region with re-encryption option using a new key in the new region (keys are bound to a region)</li> </ul>"},{"location":"solutions-architect-associate/kms/#kms-multi-region-keys","title":"KMS Multi-Region Keys","text":"<ul> <li>Identical KMS keys in different AWS regions that can be used interchangeably</li> <li>Multi-region keys have the same key ID, key material, automatic rotation...</li> <li>Encrypt in one region and decrypt in other regions</li> <li>No need to re-encrypt or making cross-region API calls</li> <li>KMS multi-region are not global (Primary + Replicas)</li> <li>Each multi-region is key is managed independently</li> <li>Use cases: global client-side encryption, encryption on Global DynamoDB, global Aurora</li> </ul>"},{"location":"solutions-architect-associate/kms/#s3-replication-with-encryption","title":"S3 replication with encryption","text":"<ul> <li>Unencrypted objects and objects encrypted with SSE-S3 are replicated by default</li> <li>Objects encrypted with SSE-C are never replicated</li> <li>for objects encrypted with SSE-KMS, you need enable the option</li> <li>You can use multi-region KMS keys, but they are currently treated as independent keys by Amazon S3</li> </ul>"},{"location":"solutions-architect-associate/kms/#encrypted-ami-sharing-process","title":"Encrypted AMI sharing process","text":"<ol> <li>AMI in source account is encrypted with KMS key from source account</li> <li>Must modify the image attribute to add a Launch Permission which corresponds to the specified target AWS account</li> <li>Must share the KMS keys used to encrypt the snapshot with the target account/IAM Role</li> <li>The IAM Role/User in the target account must have the permissions to DescribeKey, ReEncrypted, CreateGrant, Decrypt</li> <li>When launching an EC2 from the AMI, optionally the target account can specify a new KMS key in its own account to re-encrypt the volumes</li> </ol>"},{"location":"solutions-architect-associate/lake-formation/","title":"AWS Lake Formation","text":"<ul> <li>Data lake = central place to have all your data for analytics purpose</li> <li>Fully managed service that makes it easy to setup a data lake in days</li> <li>It automates many complex manual steps (collecting, cleansing, moving, cataloging data...) and de-duplicate (using ML transforms)</li> <li>Combine structured and un-structured data in data lake</li> <li>Out of the box source blueprints: S3, RDS, relational and NoSQL db...</li> <li>Fine grained access control for your applications (row and column-level)</li> <li>Built on top of AWS glue</li> <li>Can be used as a centralised place to manage permissions/access control</li> </ul>"},{"location":"solutions-architect-associate/lambda/","title":"Lambda","text":"<ul> <li>Function as a Service (FaaS)</li> <li>Serverless</li> <li>Auto-scaling (Lambda scales up and down automatically to handle your workloads, and you don't pay anything when your code isn't running.)</li> <li>Run on-demand</li> <li>Limited by time - short executions</li> <li>Pay per request (number of invocations) and compute time</li> <li>Integrated with CloudWatch for monitoring</li> <li>Easy to get more resources per function (upto 10GB of RAM)</li> <li>Not good for running containerized applications</li> <li>Can package and deploy Lambda functions as container images</li> </ul>"},{"location":"solutions-architect-associate/lambda/#performance","title":"Performance","text":"<ul> <li>Increasing RAM will improve CPU and network</li> <li>RAM: 128 MB - 10GB</li> </ul>"},{"location":"solutions-architect-associate/lambda/#supported-languages","title":"Supported Languages","text":"<ul> <li>Node.js (JavaScript)</li> <li>Python</li> <li>Java</li> <li>C#</li> <li>Golang</li> <li>Ruby</li> <li>Any other language using Custom Runtime API (community supported, example Rust)</li> <li>Lambda Container image</li> <li>Must implement the Lambda Runtime API</li> <li>ECS/Fargate is preferred for running arbitrary Docker images</li> </ul>"},{"location":"solutions-architect-associate/lambda/#use-cases","title":"Use cases","text":"<ul> <li>Serverless thumbnail creation using S3 &amp; Lambda</li> </ul> <ul> <li>Serverless CRON job using EventBridge &amp; Lambda</li> </ul>"},{"location":"solutions-architect-associate/lambda/#limits-per-region","title":"Limits - per region","text":"<ul> <li>Execution</li> <li>Memory allocation: 128MB-10GB (1MB increments)</li> <li>Maximum execution time: 900 seconds (15 minutes)</li> <li>Environment variables: 4KB</li> <li>Disk capacity in the \"function container\" (in /tmp): 512MB to 10GB</li> <li> <p>Concurrency executions: 1000 (can be increased)</p> </li> <li> <p>Deployment</p> </li> <li>Lambda function deployment size (compressed.zip): 50MB</li> <li>Size of uncompressed deployment (code + dependencies): 250MB</li> <li>Can use /tmp directory to load other files at startup</li> <li>Size of environment variables: 4KB</li> </ul>"},{"location":"solutions-architect-associate/lambda/#lambdaedge","title":"Lambda@Edge","text":"<ul> <li>Feature of CloudFront that lets you run code closer to your users, which improves performance and reduces latency.</li> <li>Lambda functions written in Node.Js or Python</li> <li>Scales to 1000s of requests/second</li> <li>Used to change CloudFront requests and responses</li> <li>Viewer request - after CF receives a request from viewer</li> <li>Viewer response - before CF forwards the response to viewer</li> <li>Origin request - before CF forwards the request to origin</li> <li>Origin response - after CF receives the response from the origin</li> <li>Author your functions in one AWS region (us-east-1), then CF replicates to its locations</li> <li>Longer execution time (several ms)</li> <li>Adjustable CPU or memory</li> <li>Code depends on a 3rd party libraries (eg; AWS SDk to access other AWS services)</li> <li>Network access to use external services for processing</li> <li>File system access or access to the body of HTTP requests</li> </ul>"},{"location":"solutions-architect-associate/lambda/#networking","title":"Networking","text":"<ul> <li>By default, Lambda function is launched outside your own VPC (in an AWS owned VPC)</li> <li>So it can not access resources in your VPC (RDS, ElastiCache, internal ELB...)</li> </ul> <ul> <li>Lambda in VPC</li> <li>You must define the VPC ID, the subnets, and the security groups</li> <li>Lambda will create an ENI (Elastic Network Interface) in your subnets</li> </ul>"},{"location":"solutions-architect-associate/lex-connect/","title":"Amazon Lex &amp; Connect","text":"<ul> <li> <p>Amazon Lex</p> <ul> <li>Same technology that powers Alexa</li> <li>Automatic speech recognition (ASR) to convert speech to text</li> <li>Natural language understanding to recognize the intent of texts, callers</li> <li>Helps builds chat bots, call center bots</li> </ul> </li> <li> <p>Amazon Connect</p> <ul> <li>Receive calls, create contact flows, cloud based virtual contact center</li> <li>Can integrate with other CRM systems or AWS</li> <li>No upfront payments, 80% cheaper than traditional contact center solutions</li> </ul> </li> </ul> <p>Phone call schedule an appointment </p> <p>Documentation</p>"},{"location":"solutions-architect-associate/macie/","title":"Amazon Macie","text":"<ul> <li>Amazon Macie is a data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS, such as Personally Identifiable Information (PII) in an S3 bucket.</li> <li>No management required (just enable)</li> <li>Notifies through an EventBridge event</li> </ul>"},{"location":"solutions-architect-associate/neptune/","title":"Neptune","text":"<ul> <li>AWS managed graph database</li> <li>Used for high relationship data (eg. social networking)</li> <li>Highly available across 3 AZ with up to 15 read replicas</li> <li>Point-in-time recovery due to continuous backup to S3</li> <li>Support for KMS encryption at rest + HTTPS for in-flight encryption</li> <li>Need to provision nodes in advance (pay for the provisioned nodes)</li> <li>Optimized for complex and hard queries</li> <li>Can store upto billions of relations and query the graph with milliseconds latency</li> </ul>"},{"location":"solutions-architect-associate/network-firewall/","title":"Network Firewall","text":"<ul> <li>Protect your entire Amazon VPC</li> <li>From Layer 3 to Layer 7 protection</li> <li>Uses the AWS Gateway Load balancer internally</li> <li>Rules can be centrally managed by Firewall Manager</li> </ul> <ul> <li>Support 1000s of rules</li> <li>Traffic filtering: Allow, drop, or alert for the traffic that matches the rules</li> <li>Active flow inspection to protect against network threats with intrusion-prevention capabilities</li> <li>Send logs of rule matches to S3, CloudWatch Logs, and Kinesis Data Firehouse</li> </ul>"},{"location":"solutions-architect-associate/on-premise-strategies-with-aws/","title":"On premise strategies with aws","text":"<ul> <li>Ability to download Amazon Linux 2 AMI as a VM (.iso format)</li> <li> <p>VMWare, KVM, VirtualBox, Microsoft Hyper-V</p> </li> <li> <p>VM Import/ Export</p> </li> <li>Migrate existsing applications into EC2</li> <li>Create a DR repository strategy for your on-premise VMs</li> <li> <p>Can export back to VMs from EC2 to on-premise</p> </li> <li> <p>AWS Application discovery service</p> </li> <li>Gather information about your on-premise servers to plan a migration</li> <li>Server utilization and dependency mappings</li> <li> <p>Track with AWS Migration hub</p> </li> <li> <p>AWS Database Migration service(DMS)</p> </li> <li>Replicate on-premise =&gt; AWS , AWS =&gt; AWS, AWS =&gt; on-premise</li> <li> <p>Works with various db technologies (Oracle, Mysql, DynamoDB etc)</p> </li> <li> <p>AWS server migration service (SMS)</p> </li> <li>Incremental replication of on-premise live servers to AWS</li> </ul>"},{"location":"solutions-architect-associate/opensearch/","title":"Amazon OpenSearch","text":"<ul> <li>Successor to Amazon ElasticSearch</li> <li>Can search on any field, even supports partial matches</li> <li>Need to provision a cluster of instances (pay for provisioned instances)</li> <li>Does not support SQL (It has it's own query language)</li> <li>Data ingestion from Kinesis Data Firehouse, AWS IoT, and CloudWatch logs</li> <li>Security through Cognito &amp; IAM, KMS Encryption, TLS</li> <li>Comes with Kibana (visualization) &amp; Logstash (log ingestion) - ELK stack</li> </ul>"},{"location":"solutions-architect-associate/personalize/","title":"Amazon Personalize","text":"<ul> <li>Fully managed ML servoce to build apps with real-time personalized recommendations</li> <li>Personalized product recommendations/reranking, customized direct marketing</li> <li>Ex: user brought gardening tools, provide recommedation on the next one to buy - Same technology used by amazon.com</li> <li>Integrates into existing websites, apps, sms, email marketing systems, ... </li> <li>Implement in days not months (you don't need to build, train and deploy ML solutions)</li> <li>Uses cases: retail stores, media and entertainment</li> </ul> <p>Documentation</p>"},{"location":"solutions-architect-associate/polly/","title":"Polly","text":"<p># Amazon Polly - Reverse of Amazon Transcribe - Turn text into lifelike speech using deep learning - Allowing you to create applications that talk</p>"},{"location":"solutions-architect-associate/polly/#lexicon-ssml","title":"Lexicon &amp; SSML","text":"<ul> <li>Customize the pronunciation of words with Pronunciation Lexicons<ul> <li>Stylized words: St3ph4ne =&gt; \"Stephane\"</li> <li>Acronyms: AWS =&gt; \"Amazon Web Services\"</li> </ul> </li> <li>Upload the lexicons and use them in the SynthesizeSpeech operation</li> <li>A lexicon file (.xml, .pls) can have up to 4,000 characters and up to 100 pronunciation rules.</li> <li>Generate speech from plain text or from documents marked up with  Speech Synthesis Markup Language (SSML) - enables more customization<ul> <li>emphasizing specific words or phrases</li> <li>using phonetic pronunciation </li> <li>including breathing sounds, whispering</li> <li>using Newscaster speaking style</li> </ul> </li> </ul> <p>Use cases - Content creation (audio can be used as complimentary media to written and/or visual communication) - E-learning - Telephony (contact centers can engage customers with natural sounding voices)</p> <p>Documentation</p>"},{"location":"solutions-architect-associate/quicksight/","title":"Amazon QuickSight","text":"<ul> <li>Serverless machine-learning powered business intelligence service to create interactive dashboards</li> <li>Fast, automatically scalable, embeddable, with per-session pricing</li> <li>Uses cases:</li> <li>Businesss analytics</li> <li>Building visualizations</li> <li>Perform ad-hoc analysis</li> <li>Get business insights using data</li> <li>In-memory computation using SPICE engine if data is imported into QuickSight</li> <li>Enterprise edition: Possibility to setup Column-level security (CLS)</li> <li>Integrations:</li> <li>Data Sources (AWS Services): RDS, Aurora, Redshift, Athena, S3, OpenSearch, Timestream</li> <li>Data Source (SaaS): Salesforce, Jira</li> <li>On-premises databases (JDBC): teradata</li> <li>Data Sources (imports): xlsx, csv, json, tsv, ELF &amp; CLF (log format)</li> <li>We can define Users (standard version) and Groups (Enterprise version)</li> <li>A dashboard is a read-only snapshot of an analysis that you can share</li> <li>You can share the analysis or the dashboard with Users or Groups</li> <li>To share a dashboard, you must first publish it</li> </ul>"},{"location":"solutions-architect-associate/rds-aurora-migrations/","title":"Rds aurora migrations","text":"<ul> <li>RDS MySQL to Aurora MySQL</li> <li>Option 1: DB snapshots from RDS mysql restored as Mysql aurora db</li> <li> <p>Option 2: Create an Aurora read replica from your rds mysql, and when the replication lag is 0, promote it as its own db cluster.</p> </li> <li> <p>External MySQL to Aurora MySQL</p> </li> <li>Option 1: Use Percona XtraBackup to create a file backup in Amazon S3, Create an Aurora mysql db from Amazon S3</li> <li> <p>Option 2: Create an Aurora Mysql db, Use the mysqldump utility to migrate mysql into Aurora (slower than s3 method)</p> </li> <li> <p>Use DMS if both databases are running</p> </li> <li> <p>For Aurora PostgreSQL migrations</p> </li> <li>If it is RDS then the process is same as of RDS Mysql to Aurora mysql.</li> <li>If it is External then, Create a backup and put it in Amazon S3, Import it using aws_s3 Aurora extension</li> <li>Use DMS if both databases are up and running.</li> </ul>"},{"location":"solutions-architect-associate/readme/","title":"AWS Certified Solutions Architect - Associate (SAA)","text":"<p>The industry-recognized AWS Certified Solutions Architect - Associate (SAA) showcases knowledge and skills in AWS technology across a wide range of AWS services. The focus of this certification is on the design of cost and performance optimized solutions, demonstrating a strong understanding of the AWS Well-Architected Framework. This certification enhances the career profile and earnings of certified professionals, and increases their credibility and confidence in stakeholder and customer interactions.  </p> <p>This exam does not require deep hands-on coding experience, although familiarity with basic programming concepts would be an advantage.</p>"},{"location":"solutions-architect-associate/readme/#who-should-take-this-exam","title":"Who should take this exam?","text":"<p>AWS Certified Solutions Architect - Associate is intended for anyone with one or more years of hands-on experience designing available, cost-efficient, fault-tolerant, and scalable distributed systems on AWS. Before you take this exam, we recommend you have: -   One year of hands-on experience with AWS technology, including using compute, networking, storage, and database AWS services as well as AWS deployment and management services -   Experience deploying, managing, and operating workloads on AWS as well as implementing security controls and compliance requirements - Familiarity with using both the AWS Management Console and the AWS Command Line Interface (CLI) -   Understanding of the AWS Well-Architected Framework, AWS networking, security services, and the AWS global infrastructure -   Ability to identify which AWS services meet a given technical requirement and to define technical requirements for an AWS-based application</p>"},{"location":"solutions-architect-associate/readme/#exam-overview","title":"Exam overview","text":"<p>Level: Associate Length: 130 minutes to complete the exam Cost: 150 USD Visit Exam pricing for additional cost information.  </p> <p>Format: 65 questions, either multiple choice or multiple response. Delivery method: Pearson VUE and PSI; testing center or online proctored exam.</p> <p>Note: The last day for candidates to complete an exam appointment through PSI is December 31, 2022 at 11:59 pm UTC. Starting January 1, 2023, all AWS Certification exams will be delivered through Pearson VUE.</p>"},{"location":"solutions-architect-associate/readme/#what-does-it-take-to-earn-this-certification","title":"What does it take to earn this certification?","text":"<p>To earn this certification, you\u2019ll need to take and pass the AWS Certified Solutions Architect - Associate exam (SAA-C03). The exam features a combination of two question formats: multiple choice and multiple response. Additional information, such as the exam content outline and passing score, is in the exam guide.  </p> <p>Download the exam guide \u00bb</p> <p>Review sample questions that demonstrate the format of the questions used on this exam and include rationales for the correct answers.</p> <p>Download the sample questions \u00bb</p>"},{"location":"solutions-architect-associate/readme/#languages-offered","title":"Languages offered","text":"<p>This exam is offered in the following languages: English, French (France), German, Italian, Japanese, Korean, Portuguese (Brazil), Simplified Chinese, and Spanish (Latin America). Additional languages Spanish (Spain) and Traditional Chinese are available through Pearson VUE only.</p>"},{"location":"solutions-architect-associate/readme/#prepare-for-your-exam","title":"Prepare for your exam","text":"<p>You\u2019ve set your goal. Now it\u2019s time to build knowledge and skills to propel your career. Check out these resources from AWS Training and Certification that are relevant to AWS Certified Solutions Architect - Associate. These recommended resources are opportunities to learn from the experts at AWS, but we don\u2019t require that you take any specific training before you take an exam.</p> <p>Get started with free resources or explore additional resources, including Official Practice Exams, with a subscription to AWS Skill Builder.</p> <p>For more info</p>"},{"location":"solutions-architect-associate/redshift/","title":"Redshift","text":"<ul> <li>fully managed</li> <li>Based on PostgreSQL</li> <li>Used for analytics and data warehousing</li> <li>Not used for OLTP</li> <li>Used for Online Analytical Processing (OLAP) and high performance querying</li> <li>Columnar storage of data (instead of row based) and parallel query execution in SQL</li> <li>Need to provision instances as a part of the Redshift cluster (pay for the instances provisioned)</li> <li>Can be integrate with Business Intelligence (BI) tools such as QuickSight or Tableau</li> <li>Faster querying than Athena due to indexes</li> <li>Data must be loaded into Redshift before queries can be run</li> </ul>"},{"location":"solutions-architect-associate/redshift/#redshift-cluster","title":"Redshift Cluster","text":"<ul> <li>Provision the node size in advance</li> <li>Redshift Cluster can have 1 to 128 compute nodes (128TB per node)</li> <li>Leader Node: query planning &amp; result aggregation</li> <li>Compute Nodes: perform queries &amp; send the result to leader node</li> <li>Can use Reserved instances for cost savings</li> </ul>"},{"location":"solutions-architect-associate/redshift/#snapshots-dr","title":"Snapshots &amp; DR","text":"<ul> <li>Redshift has Multi-AZ mode for some clusters</li> <li>Snapshots are point-in-time backups of a cluster and Stored internally in S3</li> <li>Incremental (only changes are saved)</li> <li>Can be restored into a new Redshift cluster</li> <li>Automated</li> <li>based on a schedule (every 8hrs) or storage size (every 5 GB)</li> <li>set retention</li> <li>Manual</li> <li>retains until you delete them</li> <li>Can configure Redshift to copy snapshots (automated or manual) of a cluster to another AWS region</li> </ul>"},{"location":"solutions-architect-associate/redshift/#loading-data-into-redshift","title":"Loading data into Redshift","text":"<ul> <li>Kinesis Data Firehose</li> <li>Sends data to S3 and issues a COPY command to load it into Redshift</li> <li>S3</li> <li>Use COPY command to load data from an S3 bucket into Redshift</li> <li>Without Enhanced VPC Routing<ul> <li>data goes through the public internet</li> </ul> </li> <li>With Enhanced VPC Routing<ul> <li>data goes through the VPC</li> </ul> </li> <li>EC2 Instance</li> <li>Using JDBC driver</li> <li>Used when an application needs to write data to Redshift</li> <li>Better to write data in batches</li> </ul>"},{"location":"solutions-architect-associate/redshift/#redshift-spectrum","title":"Redshift Spectrum","text":"<ul> <li>Query data present in S3 without loading it into Redshift</li> <li>Must have a Redshift cluster available to start the query</li> <li>Query is executed by 1000s of Redshift Spectrum nodes</li> </ul>"},{"location":"solutions-architect-associate/rekognition/","title":"Amazon Rekognition","text":"<ul> <li>Find objects, peoples, texts, scenes in images and videos using ML</li> <li>Facial analysis and Facial search to do user verification and people counting</li> <li>You can create a database of familiar faces and compare against celebrities</li> </ul> <p>Use cases:</p> <ul> <li>Labelling</li> <li>Content moderation</li> <li>Text detection</li> <li>Face detection and analysis (age, gender, range, emotions...)</li> <li>Face search and verification</li> <li>Celebrity detection</li> <li>Pathing (ex: for sports game analysis)</li> <li>Video segment detection</li> <li>Streaming video events detection</li> </ul>"},{"location":"solutions-architect-associate/rekognition/#amazon-rekognition-content-moderation","title":"Amazon Rekognition - Content Moderation","text":"<ul> <li>Detect content that is inappropriate, unwanted or offensive (ex: nudity, pornography, violence...)</li> <li>Used in social media, broadcast media, e-commerce, and advertising situations to create a safer user experience</li> <li>Set a Minimum confidence threshold for items that will be flagged</li> </ul> <p>image -&gt; Amazon Rekognition -&gt; Confidence level &amp; Threshold -&gt; Optional manual review in A2I - Flag sensitive content for manual review in Amazon Augmented AI (A2I) - Help comply with regulations</p>"},{"location":"solutions-architect-associate/rekognition/#amazon-rekognition-identity-verification","title":"Amazon Rekognition - Identity Verification","text":"<ul> <li>Validate selfie picture</li> <li>Compare selfie picture with User ID</li> <li>Detect duplicate users</li> <li>Classify ID document (ex: driver's license, passport...)</li> <li>Extract user data</li> </ul>"},{"location":"solutions-architect-associate/rekognition/#amazon-rekognition-media-analysis","title":"Amazon Rekognition - Media analysis","text":"<ul> <li>Black frames detection (ex: to insert advertisements)</li> <li>Credits detection (detect opening and ending credits to insert binge makers such as next episode...)</li> <li>Shot detection (Count start, end, and duration of shots which can be used for promotional videos)</li> <li>Color bars detection (detect sections of video that display SMPTE color bars)</li> <li>Slates &amp; studio logo detection (detect and remove the slate when preparing content for final reviewing)</li> </ul>"},{"location":"solutions-architect-associate/rekognition/#amazon-rekognition-send-connected-home-smart-alerts","title":"Amazon Rekognition - Send connected home smart alerts","text":"<ul> <li>Accurate detection of people, pets, and packages even in varying lighting conditions</li> <li>Integrate Amazon Kinesis Video Streams with Amazon Rekognition Streaming Video Events to enable live video stream analysis.</li> <li>Specify video duration (can control how much video you need to process)</li> <li>Choose relevant objects (can choose desired objects such as pets, people, and packages for detection from live stream)</li> <li>Create real-time alerts (When detects people, pets or packages, it sends a smart alert that includes the video stream output with the detected label, bounding boxes, hero image, and the time-stamp)</li> <li>Create bespoke experiences (such as smart search to find specific events of people, pets or packages, smart alert with Alexa for announcements)</li> </ul>"},{"location":"solutions-architect-associate/rekognition/#amazon-rekognition-custom-labels","title":"Amazon Rekognition Custom Labels","text":"<ul> <li>How it works</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/","title":"Relational Database Service (RDS)","text":"<ul> <li>AWS Managed SQL Database</li> <li>Supported Engines</li> <li>Postgres</li> <li>MySQL</li> <li>MariaDB</li> <li>Oracle</li> <li>Microsoft SQL Server</li> <li>Aurora (AWS Proprietary database)</li> <li>Regional Service</li> <li>Supports Multi AZ</li> <li>Storage backed by EBS (gp2 or io1)</li> <li>We don't have access or can't SSH in to the underlying instance</li> <li>DB connection is made on port 3306</li> <li>Security Groups are used for network security (must allow incoming TCP traffic on port 3306 from specific IPs)</li> <li>Point in Time restore</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#auto-scaling","title":"Auto Scaling","text":"<ul> <li>Automatically scales the RDS storage within the max limit (Maximum Storage Threshold)</li> <li>Condition for automatic storage scaling:</li> <li>Free storage is less than 10% of allocated storage</li> <li>Low-storage lasts at least 5 minutes</li> <li>6 hours have passed since last modification</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#read-replicas","title":"Read Replicas","text":"<ul> <li>Up to 5 read replicas (within AZ, cross AZ or cross region)</li> </ul> <ul> <li>Asynchronous Replication (seconds)</li> <li>Replicas can be promoted to their own DB</li> <li>Applications must update the connection string to leverage read replicas</li> <li>Used for SELECT(=read) only kind of statements (not INSERT,DELETE,UPDATE)</li> <li>Network fee for replication</li> <li>Same region: free</li> <li>Cross region: paid <p>The Read Replicas can be setup as Multi AZ for Disaster Recovery (DR)</p> </li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#multi-az","title":"Multi AZ","text":"<ul> <li>Increase availability of the RDS database by replicating it to another AZ</li> </ul> <ul> <li>Synchronous Replication</li> <li>Connection string does not require to be updated (both the databases can be accessed by one DNS name, which allows for automatic DNS failover to standby database)</li> <li>When failing over, RDS flips the CNAME record for the DB instance to point at the standby, which is in turn promoted to become the new primary.</li> <li>Cannot be used for scaling as the standby database cannot take read/write operation</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#rds-from-single-az-to-multi-az","title":"RDS - From Single-AZ to Multi-AZ","text":"<ul> <li>Zero downtime operation (No need to stop the db)</li> <li>Just click on \"modify\" for the database and enable Multi-AZ</li> </ul> <ul> <li>The following happens internally:</li> <li>A snapshot is taken</li> <li>A new DB is restored from the snapshot in a new AZ</li> <li>Synchronization is established b/w these two databases</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#backups","title":"Backups","text":"<ul> <li>Automated Backups (enabled by default)</li> <li>Daily full backup of the database (during the defined maintenance window)</li> <li>Backup retention: 1 to 35 days (set 0 to disable automated backups)</li> <li>Transaction logs are backed-up every 5 minutes (point in time recovery)</li> <li>DB Snapshots:</li> <li>Manually triggered</li> <li>Backup retention: unlimited <p>In a stopped RDS database, you will still pay for the storage. If you plan on stopping it for a long time, you should snapshot &amp; restore instead</p> </li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#restore-options","title":"Restore options","text":"<ul> <li>Creates a new database</li> <li>Restoring MySQL RDS database from S3</li> <li>Creates backup of your on-premises database</li> <li>Store it in Amazon S3</li> <li>Restore the backup file onto a new RDS instance running MySQL</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#rds-custom","title":"RDS Custom","text":"<ul> <li>Managed database services for applications that require operating system and database customization (For Oracle and Microsoft SQL Server)</li> <li>Access to underlying OS and database so we can:</li> <li>Configure settings</li> <li>Install patches</li> <li>Enable native features</li> <li>Access the underlying EC2 instance using SSH or SSM Session Manager</li> <li>De-activate automation mode to perform customization (better to take db snapshot before)</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#encryption","title":"Encryption","text":"<ul> <li>At rest encryption</li> <li>KMS AES-256 encryption</li> <li>Encrypted DB =&gt; Encrypted Snapshots, Encrypted Replicas and vice versa</li> <li>In flight encryption</li> <li>SSL certificates</li> <li>Force all connections to your DB instance to use SSL by setting the rds.force_ssl parameter to true</li> <li>To enable encryption in transit, download the AWS-provided root certificates &amp; used them when connecting to DB</li> <li>To encrypt an un-encrypted RDS database:</li> <li>Create a snapshot of the un-encrypted database</li> <li>Copy the snapshot and enable encryption for the snapshot</li> <li>Restore the database from the encrypted snapshot</li> <li>Migrate applications to the new database, and delete the old database</li> <li>To create an encrypted cross-region read replica from a non-encrypted master:</li> <li>Encrypt a snapshot from the unencrypted master DB instance</li> <li>Create a new encrypted master DB instance</li> <li>Create an encrypted cross-region Read Replica from the new encrypted master</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#access-management","title":"Access Management","text":"<ul> <li>Username and Password can be used to login into the database</li> <li>EC2 instances &amp; Lambda functions should access the DB using IAM DB Authentication (AWSAuthenticationPlugin with IAM) - token based access</li> <li>EC2 instance or Lambda function has an IAM role which allows is to make an API call to the RDS service to get the auth token which it uses to access the MySQL database.</li> </ul> <ul> <li>Only works with MySQL and PostgreSQL</li> <li>Auth token is valid for 15 mins</li> <li>Network traffic is encrypted in-flight using SSL</li> <li>Central access management using IAM (instead of doing it for each DB individually)</li> <li>EC2 &amp; Lambda can also get DB credentials from SSM Parameter Store to authenticate to the DB - credentials based access</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#rds-proxy","title":"RDS Proxy","text":"<ul> <li>Fully managed database proxy for RDS</li> <li>Allows apps to pool and share db connections established with the database</li> <li>Improving the database efficiency by reducing the stress on database resources (eg; cpu,ram) and minimize open connections (and timeouts)</li> <li>Serverless, auto-scaling, HA (multi-az)</li> <li>If Lambda functions directly access your database, they may open too many connections under high load</li> <li>The Lambda function must be deployed in your VPC, bz RDS proxy is never publicly accessible</li> </ul> <ul> <li>Supports RDS (MySQL, PostgreSQL, MariaDB) and Aurora (MySQL, PostgreSQL)</li> <li>No code changes required for most apps</li> <li>Enfore IAM authentication for DB, and securely store credentials in AWS Secret Manager</li> <li>Cannot publicly accessible (must be accessed from vpc)</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#rds-events","title":"RDS Events","text":"<ul> <li>RDS events only provide operational events on the DB instance (not the data)</li> <li>To capture data modification events, use native functions or stored procedures to invoke a Lambda function.</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#monitoring","title":"Monitoring","text":"<ul> <li>CloudWatch Metrics for RDS</li> <li>Gathers metrics from the hypervisor of the DB instance<ul> <li>CPU Utilization</li> <li>Database Connections</li> <li>Freeable Memory</li> </ul> </li> <li>Enhanced Monitoring</li> <li>Gathers metrics from an agent running on the RDS instance<ul> <li>OS processes</li> <li>RDS child processes</li> </ul> </li> <li>Used to monitor different processes or threads on a DB instance (ex. percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance)</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#maintenance-upgrade","title":"Maintenance &amp; Upgrade","text":"<ul> <li>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete. This is why it should be done during the maintenance window.</li> </ul>"},{"location":"solutions-architect-associate/relational-database-service/#rds-databases-ports","title":"RDS Databases ports:","text":"<ul> <li>PostgreSQL: 5432</li> <li>MySQL: 3306</li> <li>Oracle RDS: 1521</li> <li>MSSQL Server: 1433</li> <li>MariaDB: 3306 (same as MySQL)</li> <li>Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)</li> </ul>"},{"location":"solutions-architect-associate/route53/","title":"Route53","text":"<ul> <li>Global Service</li> <li>AWS managed Authoritative DNS (customer can update the DNS records and have full control over the DNS)</li> <li>Also a Domain Registrar (for registering domain names)</li> <li>Only AWS service which provides 100% availability SLA <p>It is recommended to use DNS names or URLs instead of IPs wherever possible</p> </li> </ul>"},{"location":"solutions-architect-associate/route53/#hosted-zone","title":"Hosted Zone","text":"<ul> <li>A container for DNS records that define how to route traffic to a domain and its subdomains.</li> <li>Hosted zone is queried to get the IP address from the hostname</li> <li>Two types:</li> <li>Public Hosted Zone<ul> <li>resolves public domain names</li> <li>can be queried by anyone on the internet</li> </ul> </li> <li>Private Hosted Zone<ul> <li>resolves private domain names</li> <li>can only be queried from within the VPC</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/route53/#ttl-time-to-live","title":"TTL (Time-To-Live)","text":"<ul> <li>The time for which a DNS resolver caches a response</li> <li>Amazon Route 53 does not have a default TTL for any record type</li> <li>You must always specify a TTL for each record so that caching DNS resolvers can cache your DNS records to the length of time specified through the TTL.</li> <li>Each DNS record has a TTL (Time To Live) which orders clients for how long to cache these values and not overload the DNS Resolver with DNS requests. The TTL value should be set to strike a balance between how long the value should be cached vs. how many requests should go to the DNS Resolver.</li> </ul>"},{"location":"solutions-architect-associate/route53/#record-types","title":"Record Types","text":"<ul> <li>A - maps a hostname to IPv4</li> <li>AAAA - maps a hostname to IPv6</li> <li>CNAME - maps a hostname to another hostname</li> <li>The target is a domain name which must have an A or AAAA record</li> <li>Cannot point to root domains (Zone Apex) Ex: you can\u2019t create a CNAME record for example.com, but you can create for something.example.com</li> <li>NS (Name Servers) - controls how traffic is routed for a domain</li> <li>Alias - maps a hostname to an AWS resource</li> <li>AWS proprietary</li> <li>Can point to root (zone apex) and non-root domains</li> <li>Alias Record is of type A or AAAA (IPv4 / IPv6)</li> <li>Targets can be<ul> <li>Elastic Load Balancers</li> <li>CloudFront Distributions</li> <li>API Gateway</li> <li>Elastic Beanstalk environments</li> <li>S3 Websites</li> <li>VPC Interface Endpoints</li> <li>Global Accelerator accelerator</li> <li>Route 53 record in the same hosted zone</li> </ul> </li> <li>Target cannot be EC2 DNS hostname</li> </ul>"},{"location":"solutions-architect-associate/route53/#routing-policies","title":"Routing Policies","text":"<ul> <li>Simple</li> <li>Route to one or more resources</li> <li>If multiple values are returned, client chooses one at random (client-side load balancing)</li> <li>No health check (if returning multiple resources, some of them might be unhealthy)</li> </ul> <ul> <li>Weighted</li> <li>Control the % of requests that go to each specific resource</li> <li>Assign each record a relative weight<ul> <li>traffic (%) = \\(\\frac{Weight for a specific record}{Sum of all the weights for all records}\\)</li> </ul> </li> <li>DNS records must have the same name and record and can be associated with health checks</li> <li>Uses cases:<ul> <li>Load balancing between regions</li> <li>testing a new application version by sending a small amount of traffic</li> </ul> </li> <li>Assign a weight of 0 to a record to stop sending traffic to a resource</li> <li>If you set Weight to 0 for all of the records in the group, traffic is routed to all resources with equal probability. </li> </ul> <ul> <li>Latency-based</li> <li>Redirect to the resource that has the lowest latency</li> <li>Latency is based on traffic b/w users and AWS Regions</li> <li>Health checks</li> <li>Can be used for Active-Active failover strategy</li> </ul> <ul> <li>Failover</li> <li>Primary &amp; Secondary Records (if the primary application is down, route to secondary application)</li> <li>Health check must be associated with the primary record</li> <li>Used for Active-Passive failover strategy</li> </ul> <ul> <li>Geolocation</li> <li>Routing based on the client's location</li> <li>Specify location by continent, country or by state (If there's overlapping, most precise location is selected)</li> <li>Should create a \u201cDefault\u201d record (in case there\u2019s no match on location)</li> <li>Use cases: restrict content distribution &amp; language preference</li> <li>Can be associated with health checks</li> <li>Geoproximity</li> <li>Route traffic to your resources based on the geographic location of clients and the resources</li> <li>Ability to shift more traffic to resources based on the defined bias.<ul> <li>To expand (bias: 1 to 99) \u2192 more traffic to the resource</li> <li>To shrink (bias: -1 to -99) \u2192 less traffic to the resource</li> </ul> </li> <li>Resources can be:<ul> <li>AWS resources (specify AWS region)</li> <li>Non-AWS resources (specify Latitude and Longitude)</li> </ul> </li> <li>Uses Route 53 Traffic Flow</li> <li>Multi-value</li> <li>Route traffic to multiple resources (max 8)</li> <li>Health Checks (only healthy resources will be returned)</li> </ul>"},{"location":"solutions-architect-associate/route53/#health-checks","title":"Health Checks","text":"<ul> <li>HTTP Health Checks are only for public resources</li> <li>Allows for Automated DNS Failover</li> <li>Three types:</li> <li> <p>Monitor an endpoint (application or other AWS resource)</p> <ul> <li>Multiple global health checkers check the endpoint health</li> <li>Must configure the application firewall to allow incoming requests from the IPs of Route 53 Health Checkers</li> <li>Supported protocols: HTTP, HTTPS and TCP</li> </ul> <p></p> <ul> <li>Healthy/Unhealthy threshold -3 (default)</li> <li>Interval - 30 sec (can set to 10 sec - higher cost)</li> <li>Health checks only pass when the endpoint responds with 2xx or 3xx status codes</li> <li>Monitor other health checks (Calculated Health Checks)</li> <li>Combine the results of multiple Health Checks into one (AND, OR, NOT)</li> </ul> <p></p> <ul> <li>Specify how many of the health checks need to pass to make the parent pass</li> <li>Can monitor upto 256 child health checks</li> <li>Usage: perform maintenance to your website without causing all health checks to fail</li> <li>Monitor CloudWatch Alarms (to perform health check on private resources)</li> <li>Route 53 health checkers are outside the VPC. They can\u2019t access private endpoints (private VPC or on-premises resources).</li> </ul> <p></p> <ul> <li>Create a CloudWatch Metric and associate a CloudWatch Alarm to it, then create a Health Check that checks the CW alarm.</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/route53/#godaddy-with-route-53","title":"GoDaddy with Route 53","text":"<ul> <li>Use GoDaddy as registrar and Route 53 as DNS</li> <li>Once we register a domain at GoDaddy, we need to update the name servers (NS) of domain to match the name servers of a public hosted zone created in Route 53. This way, GoDaddy will use Route 53\u2019s DNS.</li> </ul>"},{"location":"solutions-architect-associate/sagemaker/","title":"Amazon SageMaker","text":"<ul> <li>Build, train, and deploy ML models at scale</li> <li>Fully managed service for developers/data scientists to build ML models</li> <li>Typically difficult to do all the processes in one place + provision servers</li> <li>Machine learning process (simplified): predicting your exam score</li> </ul> <p>Documentation</p>"},{"location":"solutions-architect-associate/secrets-manager/","title":"Secrets Manager","text":"<ul> <li>For storing secrets only</li> <li>Ability to force rotation of secrets every n days (not available in Parameter Store)</li> <li>A secret consists of multiple key-value pairs</li> <li>Secrets are encrypted using KMS </li> <li>Mostly used for RDS authentication</li> <li>need to specify the username and password to access the database</li> <li>link the secret to the database to allow for automatic rotation of database login info</li> </ul>"},{"location":"solutions-architect-associate/secrets-manager/#multi-region-secrets","title":"Multi-region secrets","text":"<ul> <li>Replicate secrets across multiple AWS regions</li> <li>Keeps read replicas in sync with primary secret</li> <li>Ability to promote a read replica secret to a standalone secret</li> <li>Uses cases: multi-region apps, DR strategies, multi-region DB</li> </ul>"},{"location":"solutions-architect-associate/serverless-blogging-website/","title":"Serverless Blogging Website","text":""},{"location":"solutions-architect-associate/serverless-blogging-website/#requirements","title":"Requirements","text":"<ul> <li>This website should scale globally</li> <li>Blogs are rarely written, but often read</li> <li>Some of the website is purely static files, the rest is a dynamic REST API (public)</li> <li>Caching must be implement where possible</li> <li>Any new users that subscribes should receive a welcome email</li> <li>Any photo uploaded to the blog should have a thumbnail generated</li> </ul>"},{"location":"solutions-architect-associate/serverless-blogging-website/#architecture","title":"Architecture","text":"<ul> <li>Static content being distributed using CloudFront with S3</li> <li>The REST API was serverless, didn't need Cognito beacuse public</li> <li>Leveraged a Global DynamoDB table to serve the data globally (we could have used Aurora Global Database)</li> <li>Enabled DynamoDB Stream to trigger a Lambda function, The Lambda function had an IAM role which could use SES (Simple Email Service) used to send emails in serverless way</li> <li>S3 can trigger SQS/SNS/Lambda to notify of events</li> </ul>"},{"location":"solutions-architect-associate/serverless-todo-list-app/","title":"Serverless ToDo List App","text":""},{"location":"solutions-architect-associate/serverless-todo-list-app/#requirements","title":"Requirements","text":"<ul> <li>Expose as REST API with HTTPS</li> <li>Serverless architecture</li> <li>Users should be able to directly interact with their own folder in S3</li> <li>Users should authenticate through a managed serverless service</li> <li>Users can write and read to-dos, but they mostly read them</li> <li>The database should scale, and have some high read throughput</li> </ul>"},{"location":"solutions-architect-associate/serverless-todo-list-app/#architecture","title":"Architecture","text":"<ul> <li>Serverless REST API: HTTPS, API Gateway, Lambda, DynamoDB</li> <li>Giving users access to a folder in S3</li> <li>Using Cognito to generate temporary credentials with STS to access S3 bucket with restricted policy. </li> <li>App users can directly access AWS resources this way</li> <li>Pattern can be applied to DynamoDB, Lambda...</li> <li>Improving read throughputs</li> <li>Implement a DAX layer to cache DynamoDB read queries.</li> <li>Caching can also be implemented as the API gateway level if the read responses don\u2019t change much.</li> </ul>"},{"location":"solutions-architect-associate/serverless/","title":"Serverless","text":"<ul> <li>New paradigm in which developers don't have to manage servers anymore...</li> <li>They just deploy codes (functions)</li> <li>Initially Serverless == FaaS (Function As A Service)</li> <li>Serverless does not mean there are no servers... It means you just don't manage/provision/see them</li> </ul>"},{"location":"solutions-architect-associate/serverless/#serverless-in-aws","title":"Serverless in AWS","text":"<ul> <li>AWS Lambda</li> <li>DynamoDB</li> <li>AWS Cognito</li> <li>AWS API Gateway</li> <li>Amazon S3</li> <li>AWS SNS &amp; SQS</li> <li>AWS Kinesis Data Firehouse</li> <li>Aurora serverless</li> <li>Step Functions</li> <li>Fargate</li> </ul>"},{"location":"solutions-architect-associate/simple-email-service/","title":"Simple Email Service (Amazon SES)","text":"<ul> <li>Fully managed service to send emails securely, globally and at scale</li> <li>Allows inbound/outbound emails</li> <li>Reputation dashboard, performance insights, anti-spam feedback</li> <li>Supports DKIM and SPF</li> <li>Flexible IP deployment: shared, dedicated, and customer-owned IPs</li> <li>Uses cases: transactional, marketing and bulk email communications</li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/","title":"Simple Queue Service (SQS)","text":"<ul> <li>Fully managed service</li> <li>Used to asynchronously decouple applications</li> <li>Supports multiple producers &amp; consumers</li> <li>When the consumer has successfully processed a message, it is removed from the queue</li> <li>Unlimited number of messages in queue</li> <li>Max 10 messages per batch</li> <li>Consumers could be EC2 instances, Lambda functions or On-premises servers</li> <li>Producing messages using SendMessage API</li> <li>Delete messages using DeleteMessage API</li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/#queue-types","title":"Queue Types","text":"<ol> <li>Standard Queue</li> <li>Unlimited throughput, unlimited number of messages in queue</li> <li>Low latency (&lt;10 ms on publish and receive)</li> <li>Can have duplicate messages (at least once delivery, occasionally) and out of order messages (best effort ordering)</li> <li>Max message size: 256KB</li> <li>Default message retention: 4 days (max: 14 days)</li> <li>FIFO Queue</li> <li>FIFO = First-In-First-Out (ordering of messages in the queue)</li> <li>Limited throughput</li> <li>300 msg/s without batching (batch size = 1)</li> <li>3000 msg/s with batching (batch size = 10)</li> <li>No duplicate messages</li> <li>The queue name must end with .fifo to be considered a FIFO queue</li> <li>Sending messages to a FIFO queue requires:</li> <li>Group ID: messages will be ordered and grouped for each group ID</li> <li>Message deduplication ID: for deduplication of messages</li> </ol>"},{"location":"solutions-architect-associate/simple-queue-service/#consumer-auto-scaling","title":"Consumer Auto Scaling","text":"<ul> <li>We can attach an ASG to the consumer instances which will scale based on the CW metric = Queue length / Number of EC2 instances. CW alarms can be triggered to step scale the consumer application.</li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/#sqs-to-decouple-bw-application-tiers","title":"SQS to decouple b/w application tiers","text":""},{"location":"solutions-architect-associate/simple-queue-service/#sqs-as-a-buffer-to-database-writes","title":"SQS as a buffer to database writes","text":"<ul> <li>If the load is too big, some transaction may be lost </li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/#encryption","title":"Encryption","text":"<ul> <li>In-flight encryption using HTTPS API</li> <li>At-rest encryption using KMS keys</li> <li>Client-side encryption also supported</li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/#access-management","title":"Access Management","text":"<ul> <li>IAM Policies to regulate access to the SQS API</li> <li>SQS Access Policies (resource based policy like bucket policy)</li> <li>Used for cross-account access to SQS queues</li> <li>Used for allowing other AWS services to send messages to an SQS queue</li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/#configurations","title":"Configurations","text":""},{"location":"solutions-architect-associate/simple-queue-service/#message-visibility-timeout","title":"Message Visibility Timeout","text":"<ul> <li>Once a message is polled by a consumer, it becomes invisible to other consumers for the duration of message visibility timeout. After the message visibility timeout is over, the message is visible in the queue.</li> <li>If a consumer crashes while processing the message, it will be visible in the queue after the visibility timeout</li> <li>If a message is not processed within the visibility timeout, it will be processed again (by another consumer). However, a consumer could call the ChangeMessageVisibility API to change the visibility timeout for that specific message. This will give the consumer more time to process the message.</li> <li>Default: 30s</li> <li>Can be configured for the entire queue</li> <li>High: if the consumer crashes, re-processing will take long</li> <li>Low: may get duplicate processing of messages</li> </ul>"},{"location":"solutions-architect-associate/simple-queue-service/#long-polling","title":"Long Polling","text":"<ul> <li>When a consumer requests messages from the queue, it can optionally wait for messages to arrive if there are none in the queue</li> <li>Decreases the number of API calls made to SQS (cheaper)</li> <li>Reduces latency (incoming messages during the polling will be read instantaneously) which will make the application efficient</li> <li>Wait time can be between 1 sec to 20 sec (20sec preferrable)</li> <li>Long Polling is preferred over Short Polling</li> <li>Can be enabled at the queue level or at the API level by using WaitTimeSeconds</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/","title":"Simple Storage Service (S3)","text":"<ul> <li>Global Service</li> <li>Object store (key-value pairs)</li> <li>Buckets must have a globally unique name</li> <li>Buckets are defined at the regional level</li> <li>Objects have a key (full path to the object) <p>s3://my_bucket/my_folder/another_folder/my_file.txt</p> </li> <li>The key is composed of bucket + prefix + object name</li> <li>There\u2019s no concept of directories within buckets (just keys with very long names that contain slashes)</li> <li>Max Object Size: 5TB</li> <li>Durability: 99.999999999% (total 11 9's)</li> <li>SYNC command can be used to copy data between buckets, possibly in different regions</li> <li>S3 delivers strong read-after-write consistency (if an object is overwritten and immediately read, S3 always returns the latest version of the object)</li> <li>S3 is strongly consistent for all GET, PUT and LIST operations</li> <li>Multi-Part Upload is recommended as soon as the file is over 100 MB.</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#pre-signed-url","title":"Pre-signed URL","text":"<ul> <li>Pre-signed URLs for S3 have temporary access token as query string parameters which allow anyone with the URL to temporarily access the object before the URL expires (default 1h)</li> <li>Pre-signed URLs inherit the permission of the user who generated it</li> <li>Allow only logged-in users to access the object</li> <li>URL expiration:</li> <li>S3 console: 12hrs</li> <li>AWS CLI: default 3600 secs, can configure expiration with --expires-in parameter in seconds</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#access-management","title":"Access Management","text":"<ul> <li>User based security</li> <li>IAM policies define which API calls should be allowed for a specific user</li> <li>Preferred over bucket policy for fine-grained access control</li> <li>Explicit DENY in an IAM Policy will take precedence over an S3 bucket policy.</li> <li>Resource based security (Bucket Policy)</li> <li>Grant public access to the bucket</li> <li>Force objects to be encrypted at upload</li> <li>Cross-account access</li> <li>Object Access Control List (ACL) - applies to the objects while uploading</li> <li>Bucket Access Control List (ACL) - access policy that applies to the bucket <p>An IAM principal can access an S3 object if \"the user IAM permissions ALLOW it OR the resource policy ALLOWS it AND there is no explicit DENY\"</p> </li> <li> <p>JSON based policies</p> <p></p> <ul> <li>Resources: buckets and objects</li> <li>Effect: Allow or Deny</li> <li>Action: Set of API to allow or deny</li> <li>Principal: The account or user to apply the policy to</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-static-websites","title":"S3 Static Websites","text":"<ul> <li>Host static websites (may contain client-side scripts) and have them accessible on the public internet over HTTP only (for HTTPS, use CloudFront with S3 bucket as the origin)</li> <li>The website URL will be either of the following: <p>bucket-name.s3-website-region.amazonaws.com bucket-name.s3-website.region.amazonaws.com</p> </li> <li>If you get a 403 (Forbidden) error, make sure the bucket policy allows public reads</li> <li>To host an S3 static website on a custom domain using Route 53, the bucket name should be the same as your domain or subdomain Ex. for subdomain portal.example.com, the name of the bucket must be portal.example.com</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#bucket-versioning","title":"Bucket Versioning","text":"<ul> <li>Enabled at the bucket level</li> <li>Protects against unintended deletes</li> <li>Ability to restore to a previous version</li> <li>Any file that is not versioned prior to enabling versioning will have version \u201cnull\u201d</li> <li>Suspending versioning does not delete the previous versions, just disables it for the future</li> <li>To restore a deleted object, delete it's \"delete marker\"</li> <li>Versioning can only be suspended once it has been enabled.</li> <li>Once you version-enable a bucket, it can never return to an unversioned state.</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#replication","title":"Replication","text":"<ul> <li>Supports cross-region, same-region and cross-account replication</li> <li>Versioning must be enabled for source and destination buckets</li> <li>Asynchronous replication</li> <li>Objects are replicated with the same version ID</li> <li>Must give proper IAM permissions to S3</li> <li>For DELETE operations:</li> <li>Replicate delete markers from source to target (optional)</li> <li>Permanent deletes are not replicated</li> <li>There is no chaining of replication. So, if bucket 1 has replication into bucket 2, which has replication into bucket 3. Then objects created in bucket 1 are not replicated to bucket 3.</li> <li>Uses cases:</li> <li>CRR: compliance, lower-latency access, cross-account replication</li> <li>SRR: log aggregation, live replication b/w production and test accounts</li> <li>After you enable replication, only new objects are replicated</li> <li>You can replicate existing objects using S3 Batch Replication</li> <li>Replicates existing objects and objects that failed replication</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#storage-classes","title":"Storage Classes","text":"<ul> <li>Data can be transitioned between storage classes manually or automatically using lifecycle rules</li> <li>Data can be put directly into any storage class</li> <li>Standard</li> <li>99.99% availability</li> <li>Most expensive</li> <li>Instant retrieval</li> <li>No cost on retrieval (only storage cost)</li> <li>For frequently accessed data</li> <li>Infrequent Access</li> <li>For data that is infrequently accessed, but requires rapid access when needed</li> <li>Lower storage cost than Standard but cost on retrieval</li> <li>Can move data into IA from Standard only after 30 days</li> <li>Two types:<ul> <li>Standard IA</li> <li>99.9% Availability</li> <li>One-Zone IA</li> <li>99.5% Availability</li> <li>Data is lost if AZ fails</li> <li>Storage for infrequently accessed data that can be easily recreated</li> </ul> </li> <li>Glacier</li> <li>For data archival</li> <li>Cost for storage and retrieval</li> <li>Can move data into Glacier from Standard anytime</li> <li>Objects cannot be directly accessed, they first need to be restored which could take some time (depending on the tier) to fetch the object.</li> <li>Default encryption for data at rest and in-transit</li> <li>Three types:<ul> <li>Glacier Instant Retrieval</li> <li>99.9% availability</li> <li>Millisecond retrieval</li> <li>Minimum storage duration of 90 days</li> <li>Great for data accessed once a quarter</li> <li>Glacier Flexible Retrieval</li> <li>99.99% availability</li> <li>3 retrieval flexibility (decreasing order of cost):<ul> <li>Expedited (1 to 5 minutes)</li> <li>Standard (3 to 5 hours)</li> <li>Bulk (5 to 12 hours)</li> </ul> </li> <li>Minimum storage duration of 90 days</li> <li>Glacier Deep Archive</li> <li>99.99% availability</li> <li>2 flexible retrieval:<ul> <li>Standard (12 hours)</li> <li>Bulk (48 hours)</li> </ul> </li> <li>Minimum storage duration of 180 days</li> <li>Lowest cost</li> </ul> </li> <li>Intelligent Tiering</li> <li>99.9% availability</li> <li>Moves objects automatically between Access Tiers based on usage</li> <li>Small monthly monitoring and auto-tiering fee</li> <li>No retrieval charges</li> <li>Moving between Storage Classes</li> <li>In the diagram below, transition can only happen in the downward direction</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#lifecycle-rules","title":"Lifecycle Rules","text":"<ul> <li>Used to automate transition or expiration actions on S3 objects</li> <li>Transition Action (transitioned to another storage class)</li> <li>Expiration Action (delete objects after some time)</li> <li>delete a version of an object</li> <li>delete incomplete multi-part uploads</li> <li>Lifecycle Rules can be created for a prefix (ex s3://mybucket/mp3/*) or objects tags (ex Department: Finance)</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-analytics","title":"S3 Analytics","text":"<ul> <li>Provides analytics to determine when to transition data into different storage classes</li> <li>Does not work for ONEZONE-IA &amp; GLACIER</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#requester-pays-buckets","title":"Requester Pays Buckets","text":"<ul> <li>In general, the bucket owner pays for all Amazon S3 storage and data transfer costs associated with the objects</li> <li>With Requester Pays buckets, Requester pays the cost of the request and the data downloaded from the bucket. The bucket owner only pays for the storage.</li> <li>Used to share large datasets with other AWS accounts</li> <li>The requester must be authenticated in AWS (cannot be anonymous)</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-notification-events","title":"S3 Notification Events","text":"<ul> <li>Optional</li> <li>Generates events for operations performed on the bucket or objects</li> <li>Object name filtering using prefix and suffix matching</li> <li>Deliver events in seconds but can sometimes take a minute or longer</li> <li>Targets:</li> <li>SNS topics</li> <li>SQS Standard queues (not FIFO queues)</li> <li>Lambda functions</li> <li>Amazon EventBridge<ul> <li>All events from S3 bucket goes to Amazon EventBridge and which have rules to route the events to go to over 18 AWS services as destinations</li> <li>Advanced filtering options with JSON rules (metadata, object size, name...)</li> <li>Multiple destinations</li> <li>EventBridge capabilities - Archive, Reply events, Reliable delivery</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#performance","title":"Performance","text":"<ul> <li>3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix</li> <li>Recommended to spread data across prefixes for maximum performance</li> <li>SSE-KMS may create bottleneck in S3 performance</li> <li>Performance Optimizations</li> <li>Multi-part Upload<ul> <li>parallelizes upload</li> <li>recommended for files &gt; 100MB</li> <li>must use for files &gt; 5GB</li> </ul> </li> <li>Byte-range fetches<ul> <li>Parallelize download requests by fetching specific byte ranges in each request</li> </ul> </li> <li>S3 Transfer Acceleration<ul> <li>Speed up upload and download for large objects (&gt;1GB) for global users</li> <li>Data is ingested at the nearest edge location and is transferred over AWS private network (uses CloudFront internally)</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-select","title":"S3 Select","text":"<ul> <li>Select a subset of data from S3 using SQL queries (server-side filtering)</li> <li>Less network cost</li> <li>Less CPU cost on the client-side</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-batch-operations","title":"S3 Batch Operations","text":"<ul> <li>Perform bulk operations on existing objects with a single request</li> <li>Ex:</li> <li>Modify object meta-data and properties</li> <li>Copy objects b/w S3 buckets</li> <li>Encrypt unencrypted objects</li> <li>Modify ACLs, Tags</li> <li>Restore objects from S3 Glacier</li> <li>Invoke Lambda function to perform custom action on each object</li> <li>Manager retries, tracks progress, sends completion notifications, generate reports</li> </ul> <ul> <li>Use S3 inventory to get object list and use S3 Select to filter your objects</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#encryption","title":"Encryption","text":"<ul> <li>Can be enabled at the bucket level or at the object level</li> <li>Server Side Encryption (SSE)</li> <li>SSE-S3<ul> <li>Keys managed by S3</li> <li>AES-256 encryption</li> <li>HTTP or HTTPS can be used</li> <li>Must set header: \"x-amz-server-side-encryption\": \"AES256\"</li> <li>Enabled by default for new buckets &amp; objects</li> </ul> </li> <li>SSE-KMS<ul> <li>Keys managed by KMS</li> <li>HTTP or HTTPS can be used</li> <li>Must set header: \"x-amz-server-side-encryption\": \"aws:kms\"</li> <li>KMS advantages: user control + audit key usage using CloudTrail</li> <li>With SSE-KMS, the encryption happens in AWS, and the encryption keys are managed by AWS but you have full control over the rotation policy of the encryption key. Encryption keys stored in AWS.</li> <li>Limitations:</li> <li>You may be impacted by KMS limits</li> <li>upload: calls GenerateDataKey KMS API, download: Decrypt KMS API</li> </ul> </li> <li>SSE-C<ul> <li>Keys managed by the client</li> <li>Client sends the key in HTTPS headers for encryption/decryption (S3 discards the key after the operation)</li> <li>Does NOT store the key</li> <li>HTTPS must be used</li> </ul> </li> <li>Client Side Encryption</li> <li>Customer fully manages the key and encryption cycle</li> <li>Customer encrypts the object before sending it to S3 and decrypts it after retrieving it from S3</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#default-encryption","title":"Default Encryption","text":"<ul> <li>SSE-S3 encryption is automatically applied to new objects and we can change that to SSE-KMS if we want</li> <li>Bucket policy can be used to force a specific type of encryption (SSE-KMS or SSE-C) on the objects uploaded to S3 <p>Bucket policies are evaluated before Default encryption</p> </li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-cors","title":"S3 CORS","text":"<ul> <li>Cross-Origin Resource Sharing (CORS)</li> <li>An origin is a combination of scheme (protocol), host (domain) and port</li> <li>Eg: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)</li> <li>Same origin: http://example.com/api1 &amp; http://example.com/api2</li> <li>Different origins: http://api1.example.com &amp; http://api2.example.com</li> <li>CORS is a web browser based security to allow requests to other origins while visiting the main origin only if the other origin allows for the requests from the main origin, using CORS Headers (Access-Control-Allow-Origin &amp; Access-Control-Allow-Methods)</li> </ul> <ul> <li>Assume a web browser accesses www.example.com and the request is routed to the origin web server. The origin server responds with index.html, which includes the address of the cross-origin server where the images are stored. Because web browsers support CORS, they will first perform a preflight request using the OPTIONS method to ask the cross-origin server to check and allow requests from the origin server. If the cross-origin server has CORS configured to allow requests from the origin server, it will send a preflight response with CORS headers. When the origin server receives the CORS headers, the web browser can make the original requests to cross-origin to serve images.</li> <li>For cross-origin access to the S3 bucket, we need to enable CORS on the bucket</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#mfa-delete","title":"MFA Delete","text":"<ul> <li>MFA required to</li> <li>permanently delete an object version</li> <li>suspend versioning on the bucket</li> <li>Bucket Versioning must be enabled</li> <li>Can only be enabled or disabled by the root user</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-access-logging","title":"S3 Access Logging","text":"<ul> <li>Most detailed way of logging access to S3 buckets (better than CloudTrail)</li> <li>Store S3 access logs into another bucket</li> <li>Logging bucket should not be the same as monitored bucket (logging loop)</li> <li>Does not support Data Events &amp; Log File Validation (use CloudTrail for that)</li> <li>Target logging bucket must be in same aws region</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#glacier-vault-lock","title":"Glacier Vault Lock","text":"<ul> <li>WORM (Write Once Read Many) model for Glacier</li> <li>Create a Lock policy (can no longer be changed or deleted)</li> <li>For compliance and data retention</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#object-lock-versioning-must-be-enabled","title":"Object Lock (Versioning must be enabled)","text":"<ul> <li>WORM (Write Once Read Many) model</li> <li>Block an object version modification or deletion for a specified amount of time</li> <li>Modes:</li> <li>Compliance mode<ul> <li>A protected object version cannot be overwritten or deleted by any user, including the root user</li> <li>The object's retention mode can\u2019t be changed, and the retention period can\u2019t be shortened</li> </ul> </li> <li>Governance mode<ul> <li>Only users with special permissions can overwrite or delete the object version or alter its lock settings</li> </ul> </li> <li>Retention period: protect the object for a fixed period, it can be extended</li> <li>LegalHold:</li> <li>Protect the object indefenitely, independent from retention period</li> <li>can be freely placed and removed using the s3:PutObjectLegalHold IAM permission</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-access-points","title":"S3 Access Points","text":"<ul> <li>Each access point gets its own DNS and policy to limit who can access it</li> <li>A specific IAM user/group</li> <li>One policy per access point=&gt; Its easier to manage than complex bucket policies</li> </ul>"},{"location":"solutions-architect-associate/simple-storage-service/#s3-object-lambda","title":"S3 Object Lambda","text":"<ul> <li>Use AWS Lambda functions to change the object before its retrieved by the caller application</li> <li>Only one S3 bucket is needed, on top of which we creates S3 Access Point and S3 Object Lambda Access Points</li> </ul>"},{"location":"solutions-architect-associate/site-to-site-vpn/","title":"Site-to-Site VPN","text":"<ul> <li>Easiest and most cost-effective way to connect a VPC to an on-premise data center</li> <li>IPSec Encrypted connection through the public internet</li> <li>Virtual Private Gateway (VGW): VPN concentrator on the VPC side of the VPN connection</li> <li>Customer Gateway (CGW): Software application or physical device on customer side of the VPN connection</li> <li>If you need to ping EC2 instances from on-premises, make sure you add the ICMP protocol on the inbound rules of your security groups</li> <li>What IP address to use in Customer Gateway Device (On-premises)?</li> <li>Public Internet-routable IP address for your Customer Gateway device</li> <li> <p>If it's behind a NAT device that's enabled for NAT traversal (NAT-T), use the public IP address of NAT device</p> <p></p> </li> <li> <p>Important step: enable Route Propagation for the virtual private gateway in the route table that is associated with your subnets</p> </li> </ul>"},{"location":"solutions-architect-associate/site-to-site-vpn/#vpn-cloudhub","title":"VPN CloudHub","text":"<ul> <li>Low-cost hub-and-spoke model for network connectivity between a VPC and multiple on-premise data centers</li> <li>Every participating network can communicate with one another through the VPN connection </li> <li>It's a VPN connection so goes over the public internet</li> <li>To set it up, connect multiple VPN connections on the same VGW, setup dynamic routing and configure route tables</li> </ul>"},{"location":"solutions-architect-associate/snow-family/","title":"Snow Family","text":"<ul> <li>Offline data migration to S3</li> <li>Used when it takes a long time to transfer data over the network</li> <li>Takes around 2 weeks to transfer the data</li> <li>Snowball cannot import to Glacier directly (transfer to S3, configure a lifecycle policy to transition the data into Glacier)</li> <li>Pay per data transfer job</li> <li>Hardware devices for</li> <li>Data Migration (between AWS &amp; on-premise data center)</li> <li>Edge Computing</li> <li>Need to install OpsHub software on your computer to manage Snow Family devices</li> </ul>"},{"location":"solutions-architect-associate/snow-family/#devices","title":"Devices","text":"<ul> <li>Snowcone</li> <li>2 vCPUs, 4GB RAM, wired or wireless access</li> <li>8/14 TB storage</li> <li>USB-C power using a cord or the optional battery</li> <li>Good for space-constrained environment</li> <li>DataSync Agent is preinstalled</li> <li>Does not support Storage Clustering</li> <li>Snowball Edge</li> <li>Compute Optimized<ul> <li>52 vCPUs, 208 GB of RAM</li> <li>40 TB storage (HDD), 8TB storage (SSD)</li> <li>Optional GPU (useful for video processing or machine learning)</li> <li>Supports Storage Clustering</li> </ul> </li> <li>Storage Optimized<ul> <li>Up to 24 vCPUs, 32 GB of RAM</li> <li>80 TB storage (HDD)</li> <li>Supports Storage Clustering (up to 15 nodes)</li> <li>Transfer up to petabytes</li> </ul> </li> <li>Snowmobile</li> <li>100 PB storage</li> <li>Used when transferring &gt; 10PB</li> <li>Transfer up to exabytes</li> <li>Does not support Storage Clustering</li> </ul>"},{"location":"solutions-architect-associate/snow-family/#data-migration","title":"Data Migration","text":"<ul> <li>Provides block storage and Amazon S3-compatible object storage</li> <li>Usage process</li> <li>Request Snowball devices from the AWS console for delivery</li> <li>Install the snowball client / AWS OpsHub on your servers</li> <li>Connect the snowball to your servers and copy files using the client</li> <li>Ship back the device when you\u2019re done (goes to the right AWS facility)</li> <li>Data will be loaded into an S3 bucket</li> <li>Snowball is completely wiped</li> <li>Devices for data migration</li> <li>Snowcone</li> <li>Snowball Edge - Storage Optimized</li> <li>Snowmobile</li> </ul>"},{"location":"solutions-architect-associate/snow-family/#edge-computing","title":"Edge Computing","text":"<ul> <li>Process data while it\u2019s being created on an edge location (could be anything that doesn\u2019t have internet or access to cloud)</li> <li>Long-term deployment options for reduced cost (1 and 3 years discounted pricing)</li> <li>Devices for edge computing</li> <li>Snowcone &amp; Snowcone SSD</li> <li>Snowball Edge (Compute &amp; Storage optimized)</li> <li>Can run EC2 Instances &amp; AWS Lambda functions locally on Snow device (using AWS loT Greengrass)</li> </ul>"},{"location":"solutions-architect-associate/ssm-parameter-store/","title":"SSM Parameter Store","text":"<ul> <li>Serverless</li> <li>Used to store configuration and secrets</li> <li>Parameter versioning</li> <li>Seamless Encryption with KMS for encryption and decryption of stored secrets</li> <li>Parameters are stored in hierarchical fashion</li> <li>Security through IAM</li> <li>Notifications with Amazon Event Bridge</li> <li>Integration with CloudFormation</li> </ul>"},{"location":"solutions-architect-associate/ssm-parameter-store/#tiers","title":"Tiers","text":"Standard Tier Advanced Tier Number of parameters 10,000 100,000 Max parameter size 4KB 8KB Parameter Policy Not supported Supported Cost Free Paid"},{"location":"solutions-architect-associate/ssm-parameter-store/#parameter-policies","title":"Parameter Policies","text":"<ul> <li>Only supported in advanced tier</li> <li>Assign policies to a parameter for additional features</li> <li>Expire the parameter after some time (TTL)</li> <li>Parameter expiration notification</li> <li>Parameter change notification</li> </ul>"},{"location":"solutions-architect-associate/ssm-session-manager/","title":"SSM Session Manager","text":"<ul> <li>Allows you to start a secure shell on your EC2 and on-premise servers without SSH keys or a bastion host</li> <li>No port 22 needed (better security)</li> <li>Support Linux, MacOS, and Windows</li> <li>Send session log data to S3 or CloudWatch logs</li> <li>Sessions are secured using an AWS Key Management Service key</li> <li>IAM instance profile needs to attach to the instance with the policy AmazonSSMMangedInstanceCore</li> </ul>"},{"location":"solutions-architect-associate/step-functions/","title":"AWS Step Functions","text":"<ul> <li>Used to build serverless workflows to orchestrate Lambda functions</li> <li>Represent flow as a JSON state machine</li> <li>Maximum workflow execution time: 1 year</li> <li>Features: sequence, parallel, conditions, timeouts, error handling, etc.</li> <li>Can integrate with EC2, ECS, On-premises servers, API Gateway, SQS queues etc</li> <li>Possibility of implementing human approval feature</li> </ul>"},{"location":"solutions-architect-associate/storage-gateway/","title":"Storage Gateway","text":"<ul> <li>Bridge between on-premises data and S3 for Hybrid Cloud</li> <li>Not suitable for one-time sync of large amounts of data (use DataSync instead)</li> <li>Optimizes data transfer by sending only changed data</li> </ul>"},{"location":"solutions-architect-associate/storage-gateway/#types-of-storage-gateway","title":"Types of Storage Gateway","text":"<ul> <li>S3 File Gateway</li> <li>Used to expand on-premise NFS by leveraging S3</li> <li>Configured S3 buckets are accessible on premises using the NFS and SMB protocol</li> <li>Most recent data is cached at file gateway for low latency access</li> <li>Integrated with Active Directory (AD) for user authentication</li> <li>Bucket access using IAM roles for each File gateway</li> </ul> <ul> <li>FSx File Gateway</li> <li>Used to expand on-premise Windows-based storage by leveraging FSx for Windows</li> <li>Windows native compatibility (SMB, NTFS, Active Directory)</li> <li>Most recent data is cached at file gateway for low latency access</li> <li>Useful for group file shares and home directories</li> </ul> <ul> <li>Volume Gateway</li> <li>Used for on-premise storage volumes</li> <li>Uses iSCSI protocol</li> <li>Two kinds of volumes:<ul> <li>Cached volumes: storage extension using S3 with caching (most recent data) at the volume gateway</li> <li>Stored volumes: entire dataset is on premise, scheduled backups to S3 as EBS snapshots</li> </ul> </li> </ul> <ul> <li>Tape Gateway</li> <li>Used to backup on-premises data using tape-based process to S3 as Virtual Tapes</li> <li>Virtual Tape Library (VTL) backed by Amazon S3 and Glacier</li> <li>Uses iSCSI protocol</li> </ul>"},{"location":"solutions-architect-associate/storage-gateway/#storage-gateway-hardware-appliance","title":"Storage Gateway - Hardware Appliance","text":"<ul> <li>Storage Gateway requires on-premises virtualization. If you don\u2019t have virtualization available, you can use a Storage Gateway - Hardware Appliance. It is a mini server that you need to install on-premises.</li> <li>Does not work with FSx File Gatway</li> </ul>"},{"location":"solutions-architect-associate/systems-manager/","title":"Systems Manager","text":"<ul> <li>AWS Systems Manager is the operations hub for your AWS applications and resources and a secure end-to-end management solution for hybrid cloud environments that enables secure operations at scale.</li> </ul>"},{"location":"solutions-architect-associate/systems-manager/#run-command","title":"Run Command","text":"<ul> <li>Execute a script or just run a command across multiple instances (using resource groups) without the need of SSH</li> <li>Command output can be shown in AWS Console, sent to S3 bucket or CloudWatch Logs</li> <li>Send notifications to SNS about command status (In progress, Success, Failed...)</li> <li>Integrated with IAM &amp; CloudTrail</li> <li>Can be invoked using EventBridge</li> </ul>"},{"location":"solutions-architect-associate/systems-manager/#patch-manager","title":"Patch Manager","text":"<ul> <li>Automates the process of patching like OS Updates, applications updates, security updates,... on EC2 instances and on-premises servers</li> <li>Support Linux, MacOS, and Windows</li> <li>Patch on-demand or on a schedule using Maintenance Windows</li> <li>Scan instances and generate patch compliance report (missing patches)</li> </ul>"},{"location":"solutions-architect-associate/systems-manager/#maintenance-windows","title":"Maintenance Windows","text":"<ul> <li>Defines a schedule for when to perform actions on your instances</li> <li>Ex: OS patching, updating drivers, installing software,...</li> <li>Contains:</li> <li>Schedule</li> <li>Duration</li> <li>Set of registered instances</li> <li>Set of registered tasks</li> </ul>"},{"location":"solutions-architect-associate/systems-manager/#automation","title":"Automation","text":"<ul> <li>Simplifies common maintenance and deployment tasks of EC2 instances and other AWS resources</li> <li>Ex: Restart instances, create AMI, EBS snapshot</li> <li>Automation Runbook: SSM Documents to define actions performed on your EC2 instances or AWS resources (pre-defined or custom)</li> <li>Can be triggered using</li> <li>Manually using AWS Console, AWS CLI, or SDK</li> <li>Amazon EventBridge</li> <li>On a schedule using Maintenance Windows</li> <li>By AWS Config for rules remediations</li> </ul>"},{"location":"solutions-architect-associate/textract/","title":"Amazon Textract","text":"<ul> <li>Automatically extracts text,handwriting,and data from any scanned documents using AI and ML</li> <li>Extract data from forms and tables</li> <li>Read and process any type of document (pdfs, images...)</li> <li>Use cases:</li> <li>Financial services (Invoices, financial reports)</li> <li>Healthcare (Medical records, insurance claims)</li> <li>Public sector (tax forms, ID documents, passports)</li> </ul>"},{"location":"solutions-architect-associate/traffic-mirroring/","title":"Traffic Mirroring","text":"<ul> <li>Capture and inspect network traffic in your VPC without disturbing the normal flow of traffic</li> <li>Inbound and outbound traffic through ENIs (eg. attached to EC2 instances) will be mirrored to the destination (NLB) for inspection without affecting the original traffic.</li> <li>Capture the traffic</li> <li>From (Source) ENIs</li> <li>To (Targets) an ENI or NLB </li> <li>Source and Target can be in the same or different VPCs (VPC Peering)</li> <li>Use cases: content inspection, threat monitoring, troubleshooting, etc.</li> </ul>"},{"location":"solutions-architect-associate/transcribe/","title":"Amazon Transcribe","text":"<ul> <li>Automatically convert speech to text (convert audio to text)</li> <li>Uses deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately</li> <li>Automatically remove personally identifiable information (PII) using Redaction (such as age, name...)</li> <li>Supports automatic language identification for multi-lingual audio</li> <li>Use cases</li> <li>transcribe customer service calls</li> <li>automate closed captioning and subtitles</li> <li>generate metadata for media assets to create a fully searchable archive</li> <li>document clinical conversations into electronic health record (EHR) systems for analysis (HIPAA eligible and trained to understand medical terminology)</li> </ul>"},{"location":"solutions-architect-associate/transcribe/#how-amazon-transcribe-works","title":"How Amazon Transcribe works","text":"<p>Amazon Transcribe converts speech to text. A basic transcription request produces a transcript data that contains data about the transcribed content, including confidence scores and timestamps for each word or punctuation mark. Transcription method can be separated in to two main categories - Batch transcription jobs: Transcribe media files that have been uploaded to S3 bucket - Streaming transcriptions: Transcribe media streams in real time</p> <p></p>"},{"location":"solutions-architect-associate/transfer-family/","title":"Transfer Family","text":"<ul> <li>AWS managed service to transfer files in and out of S3 using FTP (instead of using proprietary methods)</li> <li>Supported Protocols</li> <li>FTP (File Transfer Protocol) - unencrypted in flight</li> <li>FTPS (File Transfer Protocol over SSL) - encrypted in flight</li> <li>SFTP (Secure File Transfer Protocol) - encrypted in flight</li> <li>Supports Multi AZ</li> <li>Pay per provisioned endpoint per hour + fee per GB data transfers</li> <li>Clients can either connect directly to the FTP endpoint or optionally through Route 53</li> <li>Transfer Family will need permission to read or put data into S3 or EFS</li> <li>Store and manage user's credentials within the service, can integrate with existing authentication systems (MS Active Directory, LDAP, Okta, Amazon Cognito)</li> </ul>"},{"location":"solutions-architect-associate/transferring-large-data-sets-to-aws/","title":"Transferring large data sets to aws","text":"<ul> <li>Over the internet / Site-to-Site VPN:</li> <li>Immediate to setup</li> <li>Will take 200(TB)1000(GB)1000(MB)*8(Mb)/100Mbps = 16000000s = 185d</li> <li>Over Direct Connect 1Gbps:</li> <li>Long for the one-time setup (over a month)</li> <li>will take 200(TB)1000(GB)8(Gb)/1Gbps = 1600000s = 18.5d</li> <li>Over Snowball:</li> <li>Will take 2-3 snowballs in parallel</li> <li>Takes about 1 week for the end to end transfer</li> <li>Can be combined with DMS</li> <li>For on-going replication / transfers: Site-to-Site VPN or DX with DMS or DataSync</li> </ul>"},{"location":"solutions-architect-associate/transit-gateway/","title":"Transit Gateway","text":"<ul> <li>Transitive peering between thousands of VPCs and on-premise data centers using hub-and-spoke (star) topology</li> <li>Works with Direct Connect Gateway, VPN Connection and VPC</li> <li>Bound to a region, can work cross-region (using Resource Access Manager (RAM))</li> <li>Route Tables to control communication within the transitive network</li> <li>Supports IP Multicast (not supported by any other AWS service)</li> </ul>"},{"location":"solutions-architect-associate/transit-gateway/#increasing-bw-of-site-to-site-vpn-connection","title":"Increasing BW of Site-to-Site VPN connection","text":"<ul> <li>ECMP (Equal Cost Multi-path) routing is a routing strategy to allow to forward a packet over multiple best path</li> <li>To increase the bandwidth of the connection between Transit Gateway and corporate data center, create multiple site-to-site VPN connections, each with 2 tunnels (2 x 1.25 = 2.5 Gbps per VPN connection).</li> </ul> <ul> <li>Only one VPN connection to a VPC having 2 tunnels out of which only 1 is used (1.25 Gbps)</li> </ul>"},{"location":"solutions-architect-associate/transit-gateway/#share-dx-between-multiple-accounts","title":"Share DX between multiple Accounts","text":"<ul> <li>Share Transit Gateway across accounts using Resource Access Manager (RAM) connection between VPCs in the same region but different accounts</li> </ul>"},{"location":"solutions-architect-associate/translate/","title":"Amazon Translate","text":"<ul> <li>Natural and accurate language translation</li> <li>Amazon Translate allows you to localize content - such as websites and applications - for international users, and to easily translate large volume of text efficiently.</li> <li>Process and manage your company's incoming data</li> <li>Enable language independent processing by integrating Amazon Translate with other AWS services<ul> <li>Make subtitles and captioning available in many languages with Amazon Transcribe</li> <li>Speak translated content with Amazon Polly</li> <li>Translate document repositories stored in Amazon S3</li> </ul> </li> </ul> <p>Documentation</p>"},{"location":"solutions-architect-associate/trusted-advisor/","title":"Trusted Advisor","text":"<ul> <li>Service that analyzes your AWS accounts and provides recommendations on:</li> <li>Cost Optimization<ul> <li>low utilization EC2 instances, EBS volumes, idle load balancers, etc.</li> <li>Reserved instances &amp; savings plans optimizations</li> </ul> </li> <li>Performance<ul> <li>High utilization EC2 instances, CloudFront CDN optimizations</li> <li>EC2 to EBS throughput optimizations, Alias records recommendations</li> </ul> </li> <li>Security<ul> <li>MFA enabled on Root Account, IAM key rotation, exposed Access Keys</li> <li>S3 Bucket Permissions for public access, security groups with unrestricted ports</li> </ul> </li> <li>Fault Tolerance<ul> <li>EBS snapshots age, Availability Zone Balance</li> <li>ASG Multi-AZ, RDS Multi-AZ, ELB configuration, etc.</li> </ul> </li> <li>Service Limits<ul> <li>whether or not you are reaching the service limit for a service and suggest you to increase the limit beforehand</li> </ul> </li> <li>No installation needed</li> <li>Weekly email notifications</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/","title":"Virtual Private Cloud (VPC)","text":"<ul> <li>Regional resource</li> <li>Soft limit of 5 VPCs per region</li> <li>Only the Private IPv4 ranges are allowed</li> </ul> <p>New EC2 instances are launched into the default VPC if no subnet is specified</p>"},{"location":"solutions-architect-associate/virtual-private-cloud/#classless-inter-domain-routing-cidr","title":"Classless Inter-Domain Routing (CIDR)","text":"<ul> <li>Way to define a range of IP addresses</li> </ul> <ul> <li>Two parts</li> <li>Base IP - 192.168.0.0</li> <li> <p>Subnet Mask (defines how many bits are frozen from the left side) - /16</p> </li> <li> <p>Private IP ranges:</p> </li> <li>10.0.0.0 - 10.255.255.255 (10.0.0.0/8) \u21d2 used in big networks (24 bits can change)</li> <li>172.16.0.0 - 172.31.255.255 (172.16.0.0/12) \u21d2 AWS default VPC</li> <li>192.168.0.0 - 192.168.255.255 (192.168.0.0/16) \u21d2 home networks</li> <li>Rest of the IP ranges are Public</li> <li>Max 5 CIDR per VPC</li> <li>Min. size is /28 (16 IP addresses)</li> <li>Max. size is /16 (65536 IP addresses)</li> <li>Your VPC CIDR should not overlap with your other networks</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#subnets","title":"Subnets","text":"<ul> <li>Sub-ranges of IP addresses within the VPC</li> <li>Each subnet is bound to an AZ</li> <li>Default VPC only has public subnets (1 public subnet per AZ, no private subnet)</li> <li>AWS reserves 5 IP addresses (first 4 &amp; last 1) in each subnet. These 5 IP addresses are not available for use. Example: if CIDR block 10.0.0.0/24, then reserved IP addresses are </li> <li>10.0.0.0 - Network address</li> <li>10.0.0.1 - reserved for VPC router</li> <li>10.0.0.2 - reserved for mapping to Amazon provided DNS</li> <li>10.0.0.3 - reserved for future use</li> <li>10.0.0.255 - Network broadcast address</li> </ul> <p>To make the EC2 instances running in private subnets accessible on the internet, place them behind an internet-facing (running in public subnets) Elastic Load Balancer. There is no concept of Public and Private subnets. Public subnets are subnets that have: - \u201cAuto-assign public IPv4 address\u201d set to \u201cYes\u201d - The subnet route table has an attached Internet Gateway. This allows the resources within the subnet to make requests that go to the public internet. A subnet is private by default.  Since the resources in a private subnet don't have public IPs, they need a NAT gateway for address translation to be able to make requests that go to the public internet. NAT gateway also prevents these private resources from being accessed from the internet.</p>"},{"location":"solutions-architect-associate/virtual-private-cloud/#internet-gateway-igw","title":"Internet Gateway (IGW)","text":"<ul> <li>Allows resources in a VPC to connect to the Internet</li> </ul> <ul> <li>Attached to the VPC</li> <li>Should be used to connect public resources to the internet (use NAT gateway for private resources since they need network address translation)</li> <li>Route table of the public subnets must be edited to allow requests destined outside the VPC to be routed to the IGW</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#bastion-hosts","title":"Bastion Hosts","text":"<ul> <li>A EC2 instance running in the public subnet (accessible from public internet), to allow users to SSH into the instances in the private subnet.</li> </ul> <ul> <li>Security groups of the private instances should only allow traffic from the bastion host.</li> <li>Bastion host should only allow port 22 traffic from the IP address you need</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#network-address-translation-nat-instance","title":"Network Address Translation (NAT) Instance","text":"<ul> <li>An EC2 instance launched in the public subnet which performs network address translation to enable private instances to use the public IP of the NAT instance to access the internet. This is exactly the same as how routers perform NAT. This also prevents the private instances from being accessed from the public internet.</li> </ul> <ul> <li>Must disable EC2 setting: source / destination check on the NAT instance as the IPs can change.</li> <li>Must have an Elastic IP attached to it</li> <li>Route Tables for private subnets must be configured to route internet-destined traffic to the NAT instance (its elastic IP)</li> <li>Can be used as a Bastion Host</li> <li>Disadvantages</li> <li>Not highly available or resilient out of the box. Need to create an ASG in multi-AZ + resilient user-data script</li> <li>Internet traffic bandwidth depends on EC2 instance type</li> <li>You must manage Security Groups &amp; rules:<ul> <li>Inbound:</li> <li>Allow HTTP / HTTPS traffic coming from Private Subnets</li> <li>Allow SSH from your home network (access is provided through Internet Gateway)</li> <li>Outbound:</li> <li>Allow HTTP / HTTPS traffic to the Internet</li> </ul> </li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#nat-gateway","title":"NAT Gateway","text":"<ul> <li>AWS managed NAT with bandwidth autoscaling (up to 45Gbps)</li> <li>Uses an Elastic IP and Internet Gateway behind the scenes </li> <li>Preferred over NAT instances</li> <li>Created in a public subnet</li> <li>Bound to an AZ </li> <li>Cannot be used by EC2 instances in the same subnet (only from other subnets)</li> <li>Cannot be used as a Bastion Host</li> <li>Route Tables for private subnets must be configured to route internet-destined traffic to the NAT gateway</li> </ul> <ul> <li>No Security Groups to manage</li> <li>Pay per hour</li> </ul> <ul> <li>High availability</li> <li>Create NAT gateways in public subnets bound to different AZ all routing outbound connections to the IGW (attached to the VPC)</li> <li>No cross-AZ failover needed because if an AZ goes down, all of the instances in that AZ also go down.</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#network-access-control-list-nacl","title":"Network Access Control List (NACL)","text":"<ul> <li>NACL is a firewall at the subnet level</li> <li>One NACL per subnet but a NACL can be attached to multiple subnets</li> <li>New subnets are assigned the Default NACL</li> <li>Default NACL allows all inbound &amp; outbound requests</li> </ul> <ul> <li>NACL Rules</li> <li>Based only on IP addresses</li> <li>Rules number: 1-32766 (lower number has higher precedence)</li> <li>First rule match will drive the decision</li> <li>The last rule denies the request (only when no previous rule matches)</li> <li>NACL vs Security Group </li> <li>NACL<ul> <li>Firewall for subnets</li> <li>Supports both Allow and Deny rules</li> <li>Stateless (both request and response will be evaluated against the NACL rules)</li> <li>Only the first matched rule is considered</li> </ul> </li> <li>Security Group<ul> <li>Firewall for EC2 instances</li> <li>Supports only Allow rules</li> <li>Stateful (only request will be evaluated against the SG rules)</li> <li>All rules are evaluated</li> </ul> </li> </ul> <ul> <li>NACL with Ephemeral Ports</li> <li> <p>In the example below, the client EC2 instance needs to connect to DB instance. Since the ephemeral port can be randomly assigned from a range of ports, the Web Subnets\u2019s NACL must allow inbound traffic from that range of ports and similarly DB Subnet\u2019s NACL must allow outbound traffic on the same range of ports.</p> <p></p> </li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#vpc-peering","title":"VPC Peering","text":"<ul> <li>Connect two VPCs (could be in different region or account) using the AWS private network</li> <li>Participating VPCs must have non-overlapping CIDR</li> <li>VPC Peering connection is non-transitive (A - B, B - C != A - C) because it works based on route-table rules. </li> </ul> <ul> <li>Must update route tables in each VPC\u2019s subnets to ensure requests destined to the peered VPC can be routed through the peering connection</li> </ul> <ul> <li>You can reference a security group in a peered VPC across account or region. This allows us to use SG instead of CIDR when configuring rules.</li> </ul> <p>VPC Peering does not facilitate centrally-managed VPC like VPC Sharing</p>"},{"location":"solutions-architect-associate/virtual-private-cloud/#vpc-endpoints","title":"VPC Endpoints","text":"<ul> <li>Allows you to connect to AWS service using a private network  instead of using public internet (cheaper)</li> <li>Powered by AWS PrivateLink</li> <li>Route table is updated automatically</li> <li>Bound to a region (do not support inter-region communication)</li> <li>They remove the need of IGW, NATGW,... to access AWS services</li> <li>In case of issues:</li> <li>Check DNS setting Resolution in VPC</li> <li>Check Route tables</li> <li>Two types:</li> <li>Interface Endpoint<ul> <li>Provisions an ENI (private IP) as an entry point per subnet</li> <li>Need to attach a security group to the interface endpoint to control access</li> <li>Supports most AWS services</li> <li>Paid</li> </ul> </li> <li>Gateway Endpoint <ul> <li>Provisions a gateway</li> <li>Must be used as a target in a route table</li> <li>Supports only S3 and DynamoDB</li> <li>Free</li> </ul> </li> <li>Gateway is preferred all the time at exam. Interface Endpoint is preferred when access is required from on-premises (Site to Site VPN or Direct Connect), a different VPC or a different region</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#vpc-flow-logs","title":"VPC Flow Logs","text":"<ul> <li>Captures information about IP traffic going into your interfaces</li> <li>VPC Flow Logs</li> <li>Subnet Flow Logs</li> <li>ENI Flow Logs</li> <li>Helps to monitor and troubleshoot connectivity issues</li> <li>Flow logs data can be sent to S3 (bulk analytics) or CloudWatch Logs (near real-time decision making)</li> <li>Query VPC flow logs using Athena in S3 or CloudWatch Logs Insights</li> <li>Captures network information from AWS managed interfaces too: ELB, RDS, ElastiCache...</li> <li>Syntax</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#ipv6-support","title":"IPv6 Support","text":"<ul> <li>IPv4 cannot be disabled for your VPC</li> <li>Every IPv6 address is public and Internet-routable (no private range)</li> <li>Enable IPv6 to operate in dual-stack mode in which your EC2 instances will get at least a private IPv4 and a public IPv6. They can communicate using either IPv4 or IPv6 to the internet through an Internet Gateway.</li> </ul> <ul> <li>If you cannot launch an EC2 instance in your subnet, It\u2019s not because it cannot acquire an IPv6 (the space is very large). It\u2019s because there are no available IPv4 in your subnet. Solution: Create a larger IPv4 CIDR for the subnet</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#egress-only-internet-gateway","title":"Egress-only Internet Gateway","text":"<ul> <li>Allows instances in your VPC to initiate outbound connections over IPv6 while preventing inbound IPv6 connections to your private instances.</li> <li>Similar to NAT Gateway but for IPv6</li> <li>Must update Route Tables</li> </ul>"},{"location":"solutions-architect-associate/virtual-private-cloud/#networking-costs","title":"Networking Costs","text":"<ul> <li>Inter-AZ and Inter-Region Networking</li> <li>Traffic entering the AWS is free</li> <li>Traffic leaving an AWS region is paid</li> <li>Use Private IP instead of Public IP for good savings and better network performance</li> <li> <p>Use same AZ for maximum savings (at the cost of availability) </p> <p></p> </li> <li> <p>Minimizing Egress Traffic</p> </li> <li>Try to keep as much internet traffic within AWS to minimize costs</li> <li> <p>Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network</p> <p></p> </li> <li> <p>S3 Data Transfer</p> </li> <li>S3 ingress (uploading to S3): free</li> <li>S3 to Internet: $0.09 per GB</li> <li>S3 Transfer Acceleration: <ul> <li>Additional cost on top of Data Transfer (+$0.04 to $0.08 per GB)</li> </ul> </li> <li>S3 to CloudFront: free (internal network)</li> <li>CloudFront to Internet: $0.085 per GB (slightly cheaper than S3)<ul> <li>Caching capability (lower latency)</li> <li>Reduced costs of S3 Requests (cheaper than just S3)</li> </ul> </li> <li> <p>S3 Cross Region Replication: $0.02 per GB</p> <p></p> </li> </ul>"},{"location":"solutions-architect-associate/vmware-cloud/","title":"Vmware cloud","text":""},{"location":"solutions-architect-associate/vmware-cloud/#vmware-cloud-on-aws","title":"VMware Cloud on AWS","text":"<ul> <li>Some customers use VMware Cloud to manage their on-premises data center</li> <li>They want to extend the data center capacity to AWS, but keep using the VMware cloud software</li> <li>Use cases:</li> <li>Migrate VMware vSphere-based workloads to AWS</li> <li>Run your production workloads across VMware vSphere-based private, public, and hybrid cloud environments</li> <li>Have a disaster discover strategy</li> </ul>"},{"location":"solutions-architect-associate/web-application-firewall/","title":"Web Application Firewall (WAF)","text":"<ul> <li>Protects your application from common layer 7(HTTP) web exploits such as SQL Injection and Cross-Site Scripting (XSS)</li> <li>Can only be deployed on</li> <li>Application Load Balancer</li> <li>API Gateway</li> <li>CloudFront</li> <li>AppSync GraphQL API</li> <li>Cognito user pool</li> <li>WAF contains Web ACL (Access Control List) containing rules to filter requests based on:  </li> <li>IP addresses</li> <li>HTTP headers</li> <li>HTTP body</li> <li>URI strings</li> <li>Size constraints (ex. max 5kb)</li> <li>Geo-match (block countries)</li> <li>Rate-based rules (to count occurrences of events per IP) for DDoS protection</li> <li>A rule group is a reusable set of rules that you can add to a web ACL</li> <li>Web ACL are regional except CloudFront</li> <li>Layer 7 has more data about the structure of the incoming request than layer 4 (used by AWS Shield)</li> </ul>"},{"location":"solutions-architect-associate/web-application-firewall/#fixed-ip-while-using-waf-with-a-lb","title":"Fixed IP while using WAF with a LB","text":"<ul> <li>WAF doesn't support NLB (layer 4)</li> <li>We can use Global Accelerator for fixed IP and WAF on ALB</li> </ul>"},{"location":"solutions-architect-associate/well-architected-framework/","title":"Well Architected Framework","text":"<ul> <li>Stop guessing your capacity needs (use auto-scaling)</li> <li>Test systems at production scale (for resiliency)</li> <li>Allow for evolutionary architectures</li> <li>Design based on changing requirements</li> <li>Drive architectures using data</li> <li>Load test your applications</li> <li>Automate to make architectural experimentation easier</li> <li>Improve through game days</li> <li>Simulate applications for flash sale days</li> </ul>"},{"location":"solutions-architect-associate/well-architected-framework/#well-architected-pillars","title":"Well Architected Pillars","text":"<ol> <li>Operational Excellence</li> <li>Security</li> <li>Reliability</li> <li>Performance Efficiency</li> <li>Cost Optimization</li> <li>Sustainability</li> </ol>"},{"location":"solutions-architect-associate/well-architected-framework/#aws-well-architected-tool","title":"AWS Well Architected Tool\u00b6","text":"<ul> <li>Free tool to review your architectures against the 6 pillars framework and adopt architectural best practices</li> <li>How does it work?</li> <li>Select your workload and answer questions</li> <li>Review your answers against the 6 pillars</li> <li>Obtain advice: get videos and documentations, generate a report, see the results in a dashboard</li> </ul>"},{"location":"solutions-architect-associate/whitepapers/","title":"Links to Whitepapers","text":"<ol> <li> <p>Architecting for the cloud: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf (Archived)</p> </li> <li> <p>Whitepapers related to well-architected framework are mentioned here: https://aws.amazon.com/blogs/aws/aws-well-architected-framework-updated-white-papers-tools-and-best-practices/</p> </li> <li> <p>Disaster recovery whitepaper: https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf (Archived)</p> </li> </ol> <p>AWS now recommends a well-architected framework whitepaper: https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf</p>"}]}